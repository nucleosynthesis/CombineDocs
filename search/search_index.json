{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction These pages document the RooStats / RooFit - based software tools used for statistical analysis within the Higgs PAG - combine . Combine provides a command line interface to many different statistical techniques available inside RooFit/RooStats used widely inside CMS. The package exists in GIT under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit For more information about GIT and its usage in CMS, see http://cms-sw.github.io/cmssw/faq.html The code can be checked out from GIT and compiled on top of a CMSSW release that includes a recent RooFit/RooStats","title":"Home"},{"location":"#introduction","text":"These pages document the RooStats / RooFit - based software tools used for statistical analysis within the Higgs PAG - combine . Combine provides a command line interface to many different statistical techniques available inside RooFit/RooStats used widely inside CMS. The package exists in GIT under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit For more information about GIT and its usage in CMS, see http://cms-sw.github.io/cmssw/faq.html The code can be checked out from GIT and compiled on top of a CMSSW release that includes a recent RooFit/RooStats","title":"Introduction"},{"location":"part1/gettingstarted/","text":"Setting up the environment and installation The instructions below are for installation within a CMSSW environment For end users that don't need to commit or do any development You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases ROOT6 SLC6 release CMSSW_8_1_X - recommended version Setting up the environment (once) export SCRAM_ARCH=slc6_64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a reccomended tag - currently the reccomended tag is v7.0.12 cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.12 scramv1 b clean; scramv1 b # always make a clean build You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare. For developers We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites 1) Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html 2) Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit Recommended way to develop a feature (in a branch) # get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created. Committing changes to your repository git add .... git commit -m \"....\" git push You can now make a pull request to the repository. Combine Tool An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package.","title":"Part1"},{"location":"part1/gettingstarted/#setting-up-the-environment-and-installation","text":"The instructions below are for installation within a CMSSW environment","title":"Setting up the environment and installation"},{"location":"part1/gettingstarted/#for-end-users-that-dont-need-to-commit-or-do-any-development","text":"You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases","title":"For end users that don't need to commit or do any development"},{"location":"part1/gettingstarted/#root6-slc6-release-cmssw_8_1_x-recommended-version","text":"","title":"ROOT6 SLC6 release CMSSW_8_1_X - recommended version"},{"location":"part1/gettingstarted/#setting-up-the-environment-once","text":"export SCRAM_ARCH=slc6_64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit","title":"Setting up the environment (once)"},{"location":"part1/gettingstarted/#update-to-a-reccomended-tag-currently-the-reccomended-tag-is-v7012","text":"cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.12 scramv1 b clean; scramv1 b # always make a clean build You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare.","title":"Update to a reccomended tag - currently the reccomended tag is v7.0.12"},{"location":"part1/gettingstarted/#for-developers","text":"We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites 1) Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html 2) Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit","title":"For developers"},{"location":"part1/gettingstarted/#recommended-way-to-develop-a-feature-in-a-branch","text":"# get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created.","title":"Recommended way to develop a feature (in a branch)"},{"location":"part1/gettingstarted/#committing-changes-to-your-repository","text":"git add .... git commit -m \"....\" git push You can now make a pull request to the repository.","title":"Committing changes to your repository"},{"location":"part1/gettingstarted/#combine-tool","text":"An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package.","title":"Combine Tool"},{"location":"part2/bin-wise-stats/","text":"Automatic statistical uncertainties Introduction The text2workspace.py script is now able to produce a new type of workspace in which bin-wise statistical uncertainties are added automatically. This can be built for shape-based datacards where the inputs are in TH1 format. Datacards that use RooDataHists are not supported. The bin errrors (i.e. values returned by TH1::GetBinError) are used to model the uncertainties. By default the script will attempt to assign a single nuisance parameter to scale the sum of the process yields in each bin, constrained by the total uncertainty, instead of requiring separate parameters, one per process. This is sometimes referred to as the Barlow-Beeston -lite approach, and is useful as it minimises the number of parameters required in the maximum-likelihood fit. A useful description of this approach may be found in section 5 of this report . Usage instructions The following line should be added at the bottom of the datacard, underneath the systematics, to produce a new-style workspace and optionally enable the automatic bin-wise uncertainties: [channel] autoMCStats [threshold] [include-signal = 0] [hist-mode = 1] The first string channel should give the name of the channels (bins) in the datacard for which the new histogram classes should be used. The wildcard * is supported for selecting multiple channels in one go. The value of threshold should be set to a value greater than or equal to zero to enable the creation of automatic bin-wise uncertainties, or -1 to use the new histogram classes without these uncertainties. A positive value sets the threshold on the effective number of unweighted events above which the uncertainty will be modeled with the Barlow-Beeston-lite approach described above. Below the threshold an individual uncertainty per-process will be created. The algorithm is described in more detail below. The last two settings are optional. The first of these, include-signal has a default value of 0 but can be set to 1 as an alternative. By default the total nominal yield and uncertainty used to test the threshold excludes signal processes, as typically the initial signal normalisation is arbitrary, and could unduly lead to a bin being considered well-populated despite poorly populated background templates. Setting this flag will include the signal processes in the uncertainty analysis. Note that this option only affects the logic for creating a single Barlow-Beeston-lite parameter vs. separate per-process parameters - the uncertainties on all signal processes are always included in the actual model! The second flag changes the way the normalisation effect of shape-altering uncertainties is handled. In the default mode ( 1 ) the normalisation is handled separately from the shape morphing via a an asymmetric log-normal term. This is identical to how combine has always handled shape morphing. When set to 2 , the normalisation will be adjusted in the shape morphing directly. Unless there is a strong motivation we encourage users to leave this on the default setting. Description of the algorithm When threshold is set to a number of effective unweighted events greater than or equal to zero, denoted n^{\\text{threshold}} n^{\\text{threshold}} , the following algorithm is applied to each bin: Sum the yields n_{i} n_{i} and uncertainities e_{i} e_{i} of each background process i i in the bin. Note that the n_i n_i and e_i e_i include the nominal effect of any scaling parameters that have been set in the datacard, for example rateParams . n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i , e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} If e_{\\text{tot}} = 0 e_{\\text{tot}} = 0 , the bin is skipped and no parameters are created. (Though you might want to check why there is no uncertainty on the background prediction in this bin!) The effective number of unweighted events is defined as n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} , rounded to the nearest integer. If n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} : separate uncertainties will be created for each process. Processes where e_{i} = 0 e_{i} = 0 are skipped. If the number of effective events for a given process is lower than n^{\\text{threshold}} n^{\\text{threshold}} a Poisson-constrained parameter will be created. Otherwise a Gaussian-constrained parameter is used. If n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} : A single Gaussian-constrained Barlow-Beeston-lite parameter is created that will scale the total yield in the bin. Note that the values of e_{i} e_{i} and therefore e_{tot} e_{tot} will be updated automatically in the model whenever the process normalisations change. A Gaussian-constrained parameter x x has a nominal value of zero and scales the yield as n_{\\text{tot}} + x \\cdot e_{\\text{tot}} n_{\\text{tot}} + x \\cdot e_{\\text{tot}} . The Poisson-constrained parameters are expressed as a yield multiplier with nominal value one: n_{\\text{tot}} \\cdot x n_{\\text{tot}} \\cdot x . The output from text2workspace.py will give details on how each bin has been treated by this alogorithm, for example: Show example output ============================================================ Analysing bin errors for: prop_binhtt_et_6_7TeV Poisson cut-off: 10 Processes excluded for sums: ZH qqH WH ggH ============================================================ Bin Contents Error Notes 0 0.000000 0.000000 total sum 0 0.000000 0.000000 excluding marked processes => Error is zero, ignore ------------------------------------------------------------ 1 0.120983 0.035333 total sum 1 0.120983 0.035333 excluding marked processes 1 12.000000 3.464102 Unweighted events, alpha=0.010082 => Total parameter prop_binhtt_et_6_7TeV_bin1[0.00,-7.00,7.00] to be gaussian constrained ------------------------------------------------------------ 2 0.472198 0.232096 total sum 2 0.472198 0.232096 excluding marked processes 2 4.000000 2.000000 Unweighted events, alpha=0.118049 => Number of weighted events is below poisson threshold ZH 0.000000 0.000000 => Error is zero, ignore ---------------------------------------------------------- W 0.050606 0.029220 3.000000 1.732051 Unweighted events, alpha=0.016869 => Product of prop_binhtt_et_6_7TeV_bin2_W[1.00,0.00,12.15] and const [3] to be poisson constrained ---------------------------------------------------------- ZJ 0.142444 0.140865 1.000000 1.000000 Unweighted events, alpha=0.142444 => Product of prop_binhtt_et_6_7TeV_bin2_ZJ[1.00,0.00,30.85] and const [1] to be poisson constrained ---------------------------------------------------------- Analytic minimisation One significant advantage of the Barlow-Beeston-lite approach is that the maximum likelihood estimate of each nuisance parameter has a simple analytic form that depends only on n_{\\text{tot}} n_{\\text{tot}} , e_{\\text{tot}} e_{\\text{tot}} and the observed number of data events in the relevant bin. Therefore when minimising the negative log-likelihood of the whole model it is possible to remove these parameters from the fit and set them to their best-fit values automatically. For models with large numbers of bins this can reduce the fit time and increase the fit stability. To enable the analytic minimisation add the option --X-rtd MINIMIZER_analytic when running combine. Technical details Up until recently text2workspace.py would only construct the PDF for each channel using a RooAddPdf , i.e. each component process is represented by a separate PDF and normalisation coefficient. However in order to model bin-wise statistical uncertainties the alternative RooRealSumPdf can be more useful, as each process is represented by a RooFit function object instead of a PDF, and we can vary the bin yields directly. As such, a new RooFit histogram class CMSHistFunc is introduced, which offers the same vertical template morphing algorithms offered by the current default histogram PDF, FastVerticalInterpHistPdf2 . Accompanying this is the CMSHistErrorPropagator class. This evaluates a sum of CMSHistFunc objects, each multiplied by a coefficient. It is also able to scale the summed yield of each bin to account for bin-wise statistical uncertainty nuisance parameters. [warning] One disadvantage of this new approach comes when evaluating the expectation for individual processes, for example when using the --saveShapes option in the FitDiagnostics mode of combine. The Barlow-Beeston-lite parameters scale the sum of the process yields directly, so extra work is needed in the distribution this total scaling back to each individual process. To achieve this an additional class CMSHistFuncWrapper has been created that, given a particular CMSHistFunc , the CMSHistErrorPropagator will distribute an appropriate fraction of the total yield shift to each bin. As a consequence of the extra computation needed to distribute the yield shifts in this way the evaluation of individual process shapes in --saveShapes can take longer then previously.","title":"Automatic MC statistical uncertainties"},{"location":"part2/bin-wise-stats/#automatic-statistical-uncertainties","text":"","title":"Automatic statistical uncertainties"},{"location":"part2/bin-wise-stats/#introduction","text":"The text2workspace.py script is now able to produce a new type of workspace in which bin-wise statistical uncertainties are added automatically. This can be built for shape-based datacards where the inputs are in TH1 format. Datacards that use RooDataHists are not supported. The bin errrors (i.e. values returned by TH1::GetBinError) are used to model the uncertainties. By default the script will attempt to assign a single nuisance parameter to scale the sum of the process yields in each bin, constrained by the total uncertainty, instead of requiring separate parameters, one per process. This is sometimes referred to as the Barlow-Beeston -lite approach, and is useful as it minimises the number of parameters required in the maximum-likelihood fit. A useful description of this approach may be found in section 5 of this report .","title":"Introduction"},{"location":"part2/bin-wise-stats/#usage-instructions","text":"The following line should be added at the bottom of the datacard, underneath the systematics, to produce a new-style workspace and optionally enable the automatic bin-wise uncertainties: [channel] autoMCStats [threshold] [include-signal = 0] [hist-mode = 1] The first string channel should give the name of the channels (bins) in the datacard for which the new histogram classes should be used. The wildcard * is supported for selecting multiple channels in one go. The value of threshold should be set to a value greater than or equal to zero to enable the creation of automatic bin-wise uncertainties, or -1 to use the new histogram classes without these uncertainties. A positive value sets the threshold on the effective number of unweighted events above which the uncertainty will be modeled with the Barlow-Beeston-lite approach described above. Below the threshold an individual uncertainty per-process will be created. The algorithm is described in more detail below. The last two settings are optional. The first of these, include-signal has a default value of 0 but can be set to 1 as an alternative. By default the total nominal yield and uncertainty used to test the threshold excludes signal processes, as typically the initial signal normalisation is arbitrary, and could unduly lead to a bin being considered well-populated despite poorly populated background templates. Setting this flag will include the signal processes in the uncertainty analysis. Note that this option only affects the logic for creating a single Barlow-Beeston-lite parameter vs. separate per-process parameters - the uncertainties on all signal processes are always included in the actual model! The second flag changes the way the normalisation effect of shape-altering uncertainties is handled. In the default mode ( 1 ) the normalisation is handled separately from the shape morphing via a an asymmetric log-normal term. This is identical to how combine has always handled shape morphing. When set to 2 , the normalisation will be adjusted in the shape morphing directly. Unless there is a strong motivation we encourage users to leave this on the default setting.","title":"Usage instructions"},{"location":"part2/bin-wise-stats/#description-of-the-algorithm","text":"When threshold is set to a number of effective unweighted events greater than or equal to zero, denoted n^{\\text{threshold}} n^{\\text{threshold}} , the following algorithm is applied to each bin: Sum the yields n_{i} n_{i} and uncertainities e_{i} e_{i} of each background process i i in the bin. Note that the n_i n_i and e_i e_i include the nominal effect of any scaling parameters that have been set in the datacard, for example rateParams . n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i , e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} If e_{\\text{tot}} = 0 e_{\\text{tot}} = 0 , the bin is skipped and no parameters are created. (Though you might want to check why there is no uncertainty on the background prediction in this bin!) The effective number of unweighted events is defined as n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} , rounded to the nearest integer. If n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} : separate uncertainties will be created for each process. Processes where e_{i} = 0 e_{i} = 0 are skipped. If the number of effective events for a given process is lower than n^{\\text{threshold}} n^{\\text{threshold}} a Poisson-constrained parameter will be created. Otherwise a Gaussian-constrained parameter is used. If n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} : A single Gaussian-constrained Barlow-Beeston-lite parameter is created that will scale the total yield in the bin. Note that the values of e_{i} e_{i} and therefore e_{tot} e_{tot} will be updated automatically in the model whenever the process normalisations change. A Gaussian-constrained parameter x x has a nominal value of zero and scales the yield as n_{\\text{tot}} + x \\cdot e_{\\text{tot}} n_{\\text{tot}} + x \\cdot e_{\\text{tot}} . The Poisson-constrained parameters are expressed as a yield multiplier with nominal value one: n_{\\text{tot}} \\cdot x n_{\\text{tot}} \\cdot x . The output from text2workspace.py will give details on how each bin has been treated by this alogorithm, for example: Show example output ============================================================ Analysing bin errors for: prop_binhtt_et_6_7TeV Poisson cut-off: 10 Processes excluded for sums: ZH qqH WH ggH ============================================================ Bin Contents Error Notes 0 0.000000 0.000000 total sum 0 0.000000 0.000000 excluding marked processes => Error is zero, ignore ------------------------------------------------------------ 1 0.120983 0.035333 total sum 1 0.120983 0.035333 excluding marked processes 1 12.000000 3.464102 Unweighted events, alpha=0.010082 => Total parameter prop_binhtt_et_6_7TeV_bin1[0.00,-7.00,7.00] to be gaussian constrained ------------------------------------------------------------ 2 0.472198 0.232096 total sum 2 0.472198 0.232096 excluding marked processes 2 4.000000 2.000000 Unweighted events, alpha=0.118049 => Number of weighted events is below poisson threshold ZH 0.000000 0.000000 => Error is zero, ignore ---------------------------------------------------------- W 0.050606 0.029220 3.000000 1.732051 Unweighted events, alpha=0.016869 => Product of prop_binhtt_et_6_7TeV_bin2_W[1.00,0.00,12.15] and const [3] to be poisson constrained ---------------------------------------------------------- ZJ 0.142444 0.140865 1.000000 1.000000 Unweighted events, alpha=0.142444 => Product of prop_binhtt_et_6_7TeV_bin2_ZJ[1.00,0.00,30.85] and const [1] to be poisson constrained ----------------------------------------------------------","title":"Description of the algorithm"},{"location":"part2/bin-wise-stats/#analytic-minimisation","text":"One significant advantage of the Barlow-Beeston-lite approach is that the maximum likelihood estimate of each nuisance parameter has a simple analytic form that depends only on n_{\\text{tot}} n_{\\text{tot}} , e_{\\text{tot}} e_{\\text{tot}} and the observed number of data events in the relevant bin. Therefore when minimising the negative log-likelihood of the whole model it is possible to remove these parameters from the fit and set them to their best-fit values automatically. For models with large numbers of bins this can reduce the fit time and increase the fit stability. To enable the analytic minimisation add the option --X-rtd MINIMIZER_analytic when running combine.","title":"Analytic minimisation"},{"location":"part2/bin-wise-stats/#technical-details","text":"Up until recently text2workspace.py would only construct the PDF for each channel using a RooAddPdf , i.e. each component process is represented by a separate PDF and normalisation coefficient. However in order to model bin-wise statistical uncertainties the alternative RooRealSumPdf can be more useful, as each process is represented by a RooFit function object instead of a PDF, and we can vary the bin yields directly. As such, a new RooFit histogram class CMSHistFunc is introduced, which offers the same vertical template morphing algorithms offered by the current default histogram PDF, FastVerticalInterpHistPdf2 . Accompanying this is the CMSHistErrorPropagator class. This evaluates a sum of CMSHistFunc objects, each multiplied by a coefficient. It is also able to scale the summed yield of each bin to account for bin-wise statistical uncertainty nuisance parameters. [warning] One disadvantage of this new approach comes when evaluating the expectation for individual processes, for example when using the --saveShapes option in the FitDiagnostics mode of combine. The Barlow-Beeston-lite parameters scale the sum of the process yields directly, so extra work is needed in the distribution this total scaling back to each individual process. To achieve this an additional class CMSHistFuncWrapper has been created that, given a particular CMSHistFunc , the CMSHistErrorPropagator will distribute an appropriate fraction of the total yield shift to each bin. As a consequence of the extra computation needed to distribute the yield shifts in this way the evaluation of individual process shapes in --saveShapes can take longer then previously.","title":"Technical details"},{"location":"part2/physicsmodels/","text":"Physics Models Combine can be run directly on the text based datacard. However, for more advanced physics models, the internal step to convert the datacard to a binary workspace can be performed by the user. To create a binary workspace starting from a datacard.txt , just do text2workspace.py datacard.txt -o workspace.root By default (without the -o option), the binary workspace will be named datacard.root - i.e the .txt suffix will be replaced by .root . A full set of options for text2workspace can be found by using --help . The default model which will be produced when running text2workspace is one in which all processes identified as signal are multiplied by a common multiplier r . This is all that is needed for simply setting limits or calculating significances. text2workspace will convert the datacard into a pdf which summaries the analysis. For example, lets take a look at the data/tutorials/counting/simple-counting-experiment.txt datacard. # Simple counting experiment, with one signal and one background process # Extremely simplified version of the 35/pb H->WW analysis for mH = 200 GeV, # for 4th generation exclusion (EWK-10-009, arxiv:1102.5429v1) imax 1 number of channels jmax 1 number of backgrounds kmax 2 number of nuisance parameters (sources of systematical uncertainties) ------------ # we have just one channel, in which we observe 0 events bin 1 observation 0 ------------ # now we list the expected events for signal and all backgrounds in that bin # the second 'process' line must have a positive number for backgrounds, and 0 for signal # then we list the independent sources of uncertainties, and give their effect (syst. error) # on each process and bin bin 1 1 process ggh4G Bckg process 0 1 rate 4.76 1.47 ------------ deltaS lnN 1.20 - 20% uncertainty on signal deltaB lnN - 1.50 50% uncertainty on background If we run text2workspace.py on this datacard and take a look at the workspace ( w ) inside the .root file produced, we will find a number of different objects representing the signal, background and observed event rates as well as the nuisance parameters and signal strength r . From these objects, the necessary pdf has been constructed (named model_s ). For this counting experiment we will expect a simple pdf of the form p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} where the expected signal and background rates are expressed as functions of the nuisance parameters, n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ and ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} The first term represents the usual Poisson expression for observing n_{\\mathrm{obs}} n_{\\mathrm{obs}} events while the second two are the Gaussian constraint terms for the nuisance parameters. In this case {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 , and the widths of both Gaussians are 1. A combination of counting experiments (or a binned shape datacard) will look like a product of pdfs of this kind. For a parametric/unbinned analyses, the pdf for each process in each channel is provided instead of the using the Poisson terms and a product is over the bin counts/events. Model building For more complex models, PhysicsModels can be produced. To use a different physics model instead of the default one, use the option -P as in text2workspace.py datacard -P HiggsAnalysis.CombinedLimit.PythonFile:modelName Generic models can be implemented by writing a python class that: defines the model parameters (by default it's just the signal strength modifier r ) defines how signal and background yields depend on the parameters (by default, signal scale linearly with r , backgrounds are constant) potentially also modifies the systematics (e.g. switch off theory uncertainties on cross section when measuring the cross section itself) In the case of SM-like Higgs searches the class should inherit from SMLikeHiggsModel (redefining getHiggsSignalYieldScale ), while beyond that one can inherit from PhysicsModel . You can find some examples in PhysicsModel.py . In the 4-process model ( PhysicsModel:floatingXSHiggs , you will see that each of the 4 dominant Higgs production modes get separate scaling parameters, r_ggH , r_qqH , r_ttH and r_VH (or r_ZH and r_WH ) as defined in, def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" # --- Signal Strength as only POI --- if \"ggH\" in self.modes: self.modelBuilder.doVar(\"r_ggH[1,%s,%s]\" % (self.ggHRange[0], self.ggHRange[1])) if \"qqH\" in self.modes: self.modelBuilder.doVar(\"r_qqH[1,%s,%s]\" % (self.qqHRange[0], self.qqHRange[1])) if \"VH\" in self.modes: self.modelBuilder.doVar(\"r_VH[1,%s,%s]\" % (self.VHRange [0], self.VHRange [1])) if \"WH\" in self.modes: self.modelBuilder.doVar(\"r_WH[1,%s,%s]\" % (self.WHRange [0], self.WHRange [1])) if \"ZH\" in self.modes: self.modelBuilder.doVar(\"r_ZH[1,%s,%s]\" % (self.ZHRange [0], self.ZHRange [1])) if \"ttH\" in self.modes: self.modelBuilder.doVar(\"r_ttH[1,%s,%s]\" % (self.ttHRange[0], self.ttHRange[1])) poi = \",\".join([\"r_\"+m for m in self.modes]) if self.pois: poi = self.pois ... The mapping of which POI scales which process is handled via the following function, def getHiggsSignalYieldScale(self,production,decay, energy): if production == \"ggH\": return (\"r_ggH\" if \"ggH\" in self.modes else 1) if production == \"qqH\": return (\"r_qqH\" if \"qqH\" in self.modes else 1) if production == \"ttH\": return (\"r_ttH\" if \"ttH\" in self.modes else (\"r_ggH\" if self.ttHasggH else 1)) if production in [ \"WH\", \"ZH\", \"VH\" ]: return (\"r_VH\" if \"VH\" in self.modes else 1) raise RuntimeError, \"Unknown production mode '%s'\" % production You should note that text2workspace will look for the python module in PYTHONPATH . If you want to keep your model local, you'll need to add the location of the python file to PYTHONPATH . A number of models used in the LHC Higgs combination paper can be found in LHCHCGModels.py . These can be easily accessed by providing for example -P HiggsAnalysis.CombinedLimit.HiggsCouplings:c7 and others defined un HiggsCouplings.py . Below are some (more generic) example models which also exist in gitHub. MultiSignalModel ready made model for multiple signal processes Combine already contains a model HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel that can be used to assign different signal strengths to multiple processes in a datacard, configurable from the command line. The model is configured passing to text2workspace one or more mappings in the form --PO 'map=bin/process:parameter' bin and process can be arbitrary regular expressions matching the bin names and process names in the datacard Note that mappings are applied both to signals and to background processes; if a line matches multiple mappings, precedence is given to the last one in the order they are in the command line. it is suggested to put quotes around the argument of --PO so that the shell does not try to expand any * signs in the patterns. parameter is the POI to use to scale that process ( name[starting_value,min,max] the first time a parameter is defined, then just name if used more than once) Special values are 1 and 0==; ==0 means to drop the process completely from the card, while 1 means to keep the yield as is in the card with no scaling (as normally done for backgrounds); 1 is the default that is applied to processes that have no mappings, so it's normally not needed, but it may be used either to make the thing explicit, or to override a previous more generic match on the same command line (e.g. --PO 'map .*/ggH:r[1,0,5]' --PO 'map bin37/ggH:1' would treat ggH as signal in general, but count it as background in the channel bin37 ) Passing the additional option --PO verbose will set the code to verbose mode, printing out the scaling factors for each process; people are encouraged to use this option to make sure that the processes are being scaled correctly. The MultiSignalModel will define all parameters as parameters of interest, but that can be then changed from the command line of combine, as described in the following sub-section. Some examples, taking as reference the toy datacard test/multiDim/toy-hgg-125.txt : Scale both ggH and qqH with the same signal strength r (that's what the default physics model of combine does for all signals; if they all have the same systematic uncertainties, it is also equivalent to adding up their yields and writing them as a single column in the card) $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r[1,0,10]' --PO 'map=.*/qqH:r' toy-hgg-125.txt -o toy-1d.root [...] Will create a POI r with factory r[1,0,10] Mapping r to ['.*/ggH'] patterns Mapping r to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r Will scale incl/qqH by r Will scale dijet/bkg by 1 Will scale dijet/ggH by r Will scale dijet/qqH by r Define two independent parameters of interest r_ggH and r_qqH $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r_ggH[1,0,10]' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-2d.root [...] Will create a POI r_ggH with factory r_ggH[1,0,10] Mapping r_ggH to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r_ggH Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by r_ggH Will scale dijet/qqH by r_qqH Fix ggH to SM, define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:1' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH.root [...] Mapping 1 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 1 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 1 Will scale dijet/qqH by r_qqH Drop ggH , and define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:0' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH0-only.root [...] Mapping 0 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 0 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 0 Will scale dijet/qqH by r_qqH Two Hypothesis testing The PhysicsModel that encodes the signal model above is the twoHypothesisHiggs , which assumes that there will exist signal processes with suffix _ALT in the datacard. An example of such a datacard can be found under data/benchmarks/simple-counting/twoSignals-3bin-bigBSyst.txt $ text2workspace.py twoSignals-3bin-bigBSyst.txt -P HiggsAnalysis.CombinedLimit.HiggsJPC:twoHypothesisHiggs -m 125.7 --PO verbose -o jcp_hww.root MH (not there before) will be assumed to be 125.7 Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x The two processes (S and S_ALT) will get different scaling parameters. The LEP-style likelihood for hypothesis testing can now be performed by setting x or not_x to 1 and 0 and comparing two likelihood evaluations. Interference Since there are no such things as negative probability distribution functions, the recommended way to implement this is to start from the expression for the individual amplitudes and the parameter of interest k k , \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B where \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 and $ S+B+I = (As + Ab)^2$. With some algebra you can work out that, \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] where square brackets represent the input (histograms as TH1 or RooDataHists ) that one needs to provide. An example of this scheme is implemented in a HiggsWidth and is completely general, since all of the three components above are strictly positive. In this example, the POI is CMS_zz4l_mu and the equations for the three components are scaled (separately for the qqH and ggH processes) as, self.modelBuilder.factory_( \"expr::ggH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\")","title":"Physics models"},{"location":"part2/physicsmodels/#physics-models","text":"Combine can be run directly on the text based datacard. However, for more advanced physics models, the internal step to convert the datacard to a binary workspace can be performed by the user. To create a binary workspace starting from a datacard.txt , just do text2workspace.py datacard.txt -o workspace.root By default (without the -o option), the binary workspace will be named datacard.root - i.e the .txt suffix will be replaced by .root . A full set of options for text2workspace can be found by using --help . The default model which will be produced when running text2workspace is one in which all processes identified as signal are multiplied by a common multiplier r . This is all that is needed for simply setting limits or calculating significances. text2workspace will convert the datacard into a pdf which summaries the analysis. For example, lets take a look at the data/tutorials/counting/simple-counting-experiment.txt datacard. # Simple counting experiment, with one signal and one background process # Extremely simplified version of the 35/pb H->WW analysis for mH = 200 GeV, # for 4th generation exclusion (EWK-10-009, arxiv:1102.5429v1) imax 1 number of channels jmax 1 number of backgrounds kmax 2 number of nuisance parameters (sources of systematical uncertainties) ------------ # we have just one channel, in which we observe 0 events bin 1 observation 0 ------------ # now we list the expected events for signal and all backgrounds in that bin # the second 'process' line must have a positive number for backgrounds, and 0 for signal # then we list the independent sources of uncertainties, and give their effect (syst. error) # on each process and bin bin 1 1 process ggh4G Bckg process 0 1 rate 4.76 1.47 ------------ deltaS lnN 1.20 - 20% uncertainty on signal deltaB lnN - 1.50 50% uncertainty on background If we run text2workspace.py on this datacard and take a look at the workspace ( w ) inside the .root file produced, we will find a number of different objects representing the signal, background and observed event rates as well as the nuisance parameters and signal strength r . From these objects, the necessary pdf has been constructed (named model_s ). For this counting experiment we will expect a simple pdf of the form p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} where the expected signal and background rates are expressed as functions of the nuisance parameters, n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ and ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} The first term represents the usual Poisson expression for observing n_{\\mathrm{obs}} n_{\\mathrm{obs}} events while the second two are the Gaussian constraint terms for the nuisance parameters. In this case {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 , and the widths of both Gaussians are 1. A combination of counting experiments (or a binned shape datacard) will look like a product of pdfs of this kind. For a parametric/unbinned analyses, the pdf for each process in each channel is provided instead of the using the Poisson terms and a product is over the bin counts/events.","title":"Physics Models"},{"location":"part2/physicsmodels/#model-building","text":"For more complex models, PhysicsModels can be produced. To use a different physics model instead of the default one, use the option -P as in text2workspace.py datacard -P HiggsAnalysis.CombinedLimit.PythonFile:modelName Generic models can be implemented by writing a python class that: defines the model parameters (by default it's just the signal strength modifier r ) defines how signal and background yields depend on the parameters (by default, signal scale linearly with r , backgrounds are constant) potentially also modifies the systematics (e.g. switch off theory uncertainties on cross section when measuring the cross section itself) In the case of SM-like Higgs searches the class should inherit from SMLikeHiggsModel (redefining getHiggsSignalYieldScale ), while beyond that one can inherit from PhysicsModel . You can find some examples in PhysicsModel.py . In the 4-process model ( PhysicsModel:floatingXSHiggs , you will see that each of the 4 dominant Higgs production modes get separate scaling parameters, r_ggH , r_qqH , r_ttH and r_VH (or r_ZH and r_WH ) as defined in, def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" # --- Signal Strength as only POI --- if \"ggH\" in self.modes: self.modelBuilder.doVar(\"r_ggH[1,%s,%s]\" % (self.ggHRange[0], self.ggHRange[1])) if \"qqH\" in self.modes: self.modelBuilder.doVar(\"r_qqH[1,%s,%s]\" % (self.qqHRange[0], self.qqHRange[1])) if \"VH\" in self.modes: self.modelBuilder.doVar(\"r_VH[1,%s,%s]\" % (self.VHRange [0], self.VHRange [1])) if \"WH\" in self.modes: self.modelBuilder.doVar(\"r_WH[1,%s,%s]\" % (self.WHRange [0], self.WHRange [1])) if \"ZH\" in self.modes: self.modelBuilder.doVar(\"r_ZH[1,%s,%s]\" % (self.ZHRange [0], self.ZHRange [1])) if \"ttH\" in self.modes: self.modelBuilder.doVar(\"r_ttH[1,%s,%s]\" % (self.ttHRange[0], self.ttHRange[1])) poi = \",\".join([\"r_\"+m for m in self.modes]) if self.pois: poi = self.pois ... The mapping of which POI scales which process is handled via the following function, def getHiggsSignalYieldScale(self,production,decay, energy): if production == \"ggH\": return (\"r_ggH\" if \"ggH\" in self.modes else 1) if production == \"qqH\": return (\"r_qqH\" if \"qqH\" in self.modes else 1) if production == \"ttH\": return (\"r_ttH\" if \"ttH\" in self.modes else (\"r_ggH\" if self.ttHasggH else 1)) if production in [ \"WH\", \"ZH\", \"VH\" ]: return (\"r_VH\" if \"VH\" in self.modes else 1) raise RuntimeError, \"Unknown production mode '%s'\" % production You should note that text2workspace will look for the python module in PYTHONPATH . If you want to keep your model local, you'll need to add the location of the python file to PYTHONPATH . A number of models used in the LHC Higgs combination paper can be found in LHCHCGModels.py . These can be easily accessed by providing for example -P HiggsAnalysis.CombinedLimit.HiggsCouplings:c7 and others defined un HiggsCouplings.py . Below are some (more generic) example models which also exist in gitHub.","title":"Model building"},{"location":"part2/physicsmodels/#multisignalmodel-ready-made-model-for-multiple-signal-processes","text":"Combine already contains a model HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel that can be used to assign different signal strengths to multiple processes in a datacard, configurable from the command line. The model is configured passing to text2workspace one or more mappings in the form --PO 'map=bin/process:parameter' bin and process can be arbitrary regular expressions matching the bin names and process names in the datacard Note that mappings are applied both to signals and to background processes; if a line matches multiple mappings, precedence is given to the last one in the order they are in the command line. it is suggested to put quotes around the argument of --PO so that the shell does not try to expand any * signs in the patterns. parameter is the POI to use to scale that process ( name[starting_value,min,max] the first time a parameter is defined, then just name if used more than once) Special values are 1 and 0==; ==0 means to drop the process completely from the card, while 1 means to keep the yield as is in the card with no scaling (as normally done for backgrounds); 1 is the default that is applied to processes that have no mappings, so it's normally not needed, but it may be used either to make the thing explicit, or to override a previous more generic match on the same command line (e.g. --PO 'map .*/ggH:r[1,0,5]' --PO 'map bin37/ggH:1' would treat ggH as signal in general, but count it as background in the channel bin37 ) Passing the additional option --PO verbose will set the code to verbose mode, printing out the scaling factors for each process; people are encouraged to use this option to make sure that the processes are being scaled correctly. The MultiSignalModel will define all parameters as parameters of interest, but that can be then changed from the command line of combine, as described in the following sub-section. Some examples, taking as reference the toy datacard test/multiDim/toy-hgg-125.txt : Scale both ggH and qqH with the same signal strength r (that's what the default physics model of combine does for all signals; if they all have the same systematic uncertainties, it is also equivalent to adding up their yields and writing them as a single column in the card) $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r[1,0,10]' --PO 'map=.*/qqH:r' toy-hgg-125.txt -o toy-1d.root [...] Will create a POI r with factory r[1,0,10] Mapping r to ['.*/ggH'] patterns Mapping r to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r Will scale incl/qqH by r Will scale dijet/bkg by 1 Will scale dijet/ggH by r Will scale dijet/qqH by r Define two independent parameters of interest r_ggH and r_qqH $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r_ggH[1,0,10]' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-2d.root [...] Will create a POI r_ggH with factory r_ggH[1,0,10] Mapping r_ggH to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r_ggH Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by r_ggH Will scale dijet/qqH by r_qqH Fix ggH to SM, define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:1' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH.root [...] Mapping 1 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 1 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 1 Will scale dijet/qqH by r_qqH Drop ggH , and define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:0' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH0-only.root [...] Mapping 0 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 0 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 0 Will scale dijet/qqH by r_qqH","title":"MultiSignalModel ready made model for multiple signal processes"},{"location":"part2/physicsmodels/#two-hypothesis-testing","text":"The PhysicsModel that encodes the signal model above is the twoHypothesisHiggs , which assumes that there will exist signal processes with suffix _ALT in the datacard. An example of such a datacard can be found under data/benchmarks/simple-counting/twoSignals-3bin-bigBSyst.txt $ text2workspace.py twoSignals-3bin-bigBSyst.txt -P HiggsAnalysis.CombinedLimit.HiggsJPC:twoHypothesisHiggs -m 125.7 --PO verbose -o jcp_hww.root MH (not there before) will be assumed to be 125.7 Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x The two processes (S and S_ALT) will get different scaling parameters. The LEP-style likelihood for hypothesis testing can now be performed by setting x or not_x to 1 and 0 and comparing two likelihood evaluations.","title":"Two Hypothesis testing"},{"location":"part2/physicsmodels/#interference","text":"Since there are no such things as negative probability distribution functions, the recommended way to implement this is to start from the expression for the individual amplitudes and the parameter of interest k k , \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B where \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 and $ S+B+I = (As + Ab)^2$. With some algebra you can work out that, \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] where square brackets represent the input (histograms as TH1 or RooDataHists ) that one needs to provide. An example of this scheme is implemented in a HiggsWidth and is completely general, since all of the three components above are strictly positive. In this example, the POI is CMS_zz4l_mu and the equations for the three components are scaled (separately for the qqH and ggH processes) as, self.modelBuilder.factory_( \"expr::ggH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\")","title":"Interference"},{"location":"part2/settinguptheanalysis/","text":"Preparing the datacard The input to combine, which defines the details of the experiment, is a datacard file is a plain ASCII file. This is true whether the experiment is a simple counting experiment or a shape analysis. A simple counting experiment The file data/tutorials/counting/realistic-counting-experiment.txt shows an example of a counting experiment. The first lines can be used as a description and are not parsed by the program. They have to begin with a \"#\": # Simple counting experiment, with one signal and a few background processes # Simplified version of the 35/pb H->WW analysis for mH = 160 GeV Then one declares the number of observables , imax , that are present in the model used to calculate limits/significances. The number of observables will typically be the number of channels in a counting experiment or the number of bins in a binned shape fit. (If one specifies for imax the value * it means \"figure it out from the rest of the datacard\", but in order to better catch mistakes it's recommended to specify it explicitly) imax 1 number of channels Then one declares the number of background sources to be considered, jmax , and the number of independent sources of systematic uncertainties , kmax : jmax 3 number of backgrounds kmax 5 number of nuisance parameters (sources of systematic uncertainties) In the example there is 1 channel, there are 3 background sources, and there are 5 independent sources of systematic uncertainty. Then there are the lines describing what is actually observed: the number of events observed in each channel. The first line, starting with bin defines the label used for each channel. In the example we have 1 channel, labelled 1 , and in the following line, observation , are listed the events observed, 0 in this example: # we have just one channel, in which we observe 0 events bin bin1 observation 0 Following is the part related to the number of events expected, for each bin and process, arranged in (#channels)*(#processes) columns. bin bin1 bin1 bin1 bin1 process ggH qqWW ggWW others process 0 1 2 3 rate 1.47 0.63 0.06 0.22 The bin line identifies the channel the column is referring to. It goes from 1 to the imax declared above. The first process line contains the labels of the various sources The second process line must have a positive number for backgrounds, and 0 or a negative number for the signals. You should use different process ids for different processes. The last line, rate , tells the expected yield of events in the specified bin and process All bins should be declared in increasing order, and within each bin one should include all processes in increasing order, specifying a 0 for processes that do not contribute. The last section contains the description of the systematic uncertainties: lumi lnN 1.11 - 1.11 - lumi affects both signal and gg->WW (mc-driven). lnN = lognormal xs_ggH lnN 1.16 - - - gg->H cross section + signal efficiency + other minor ones. WW_norm gmN 4 - 0.16 - - WW estimate of 0.64 comes from sidebands: 4 events in sideband times 0.16 (=> ~50% statistical uncertainty) xs_ggWW lnN - - 1.50 - 50% uncertainty on gg->WW cross section bg_others lnN - - - 1.30 30% uncertainty on the rest of the backgrounds the first columns is a label identifying the uncertainty the second column identifies the type of distribution used lnN stands for Log-normal , which is the recommended choice for multiplicative corrections (efficiencies, cross sections, ...). If \u0394x/x is the relative uncertainty on the multiplicative correction, one should put 1+\u0394x/x in the column corresponding to the process and channel. Asymetric log-normals are supported by providing \u03ba down /\u03ba up where \u03ba down is the ratio of the the yield to the nominal value for a -1\u03c3 deviation of the nuisance and \u03ba up is the ratio of the the yield to the nominal value for a +1\u03c3 deviation. gmN stands for Gamma , and is the recommended choice for the statistical uncertainty on a background coming from the number of events in a control region (or in a MC sample with limited statistics). If the control region or MC contains N events, and the extrapolation factor from the control region to the signal region is \u03b1 then one shoud put N just after the gmN keyword, and then the value of \u03b1 in the proper column. Also, the yield in the rate row should match with N\u03b1 lnU stands for log-uniform distribution. A value of 1+\u03b5 in the column will imply that the yield of this background is allowed to float freely between x(1+\u03b5) and x/(1+\u03b5) (in particular, if \u03b5 is small, then this is approximately (x-\u0394x,x+\u0394x) with \u03b5=\u0394x/x ) This is normally useful when you want to set a large a-priori uncertainty on a given background and then rely on the correlation between channels to constrain it. Beware that while Gaussian-like uncertainties behave in a similar way under profiling and marginalization, uniform uncertainties do not, so the impact of the uncertainty on the result will depend on how the nuisances are treated. then there are (#channels)*(#processes) columns reporting the relative effect of the systematic uncertainty on the rate of each process in each channel. The columns are aligned with the ones in the previous lines declaring bins, processes and rates. In the example, there are 5 uncertainties: the first uncertainty affects the signal by 11%, and affects the ggWW process by 11% the second uncertainty affects the signal by 16% leaving the backgrounds unaffected the third line specifies that the qqWW background comes from a sideband with 4 observed events and an extrapolation factor of 0.16; the resulting uncertainty on the expected yield is 1/\\sqrt{4+1} 1/\\sqrt{4+1} = 45% the fourth uncertainty does not affect the signal, affects the ggWW background by 50%, leaving the other backgrounds unaffected the last uncertainty does not affect the signal, affects by 30% the others backgrounds, leaving the rest of the backgrounds unaffected Shape analysis The datacard has to be supplemented with two extensions: A new block of lines defining how channels and processes are mapped into shapes The block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a RooDataSet to perform an unbinned shape analysis. [danger] If using RooFit based inputs (RooDataHists/RooDataSets/RooAbsPdfs) then you should be careful to use different RooRealVars as the observable in each category being combined. It is possible to use the same RooRealVar if the observable has the same range (and binning if using binned data) in each category though in most cases it is simpler to avoid doing so. Rates for shape analysis As with the counting experiment, the total nominal rate of a given process must be identified in the rate line of the datacard. However, there are special options for shape based analyses as follows A value of -1 in the rate line indicates to combine to calculate the rate from the input TH1 (via TH1::Integral) or RooDataSet/RooDataHist (via RooAbsData::sumEntries) For parametric shapes (RooAbsPdf), if a parameter is found in the input workspace with the name pdfname _norm the rate will be multiplied by the value of that parameter. Note that since this parameter can be freely floating, the normalization of a shape can be made to freely float this way. This can also be achieved through the use of rateParams Binned shape analysis For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [histogram_with_systematics] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels file , histogram and histogram_with_systematics identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool In addition, user defined keywords can be included to be replaced. Any word in the datacard $WORD will be replaced by VALUE when including the option --keyword-value WORD=VALUE . The option can be repeated multiple times for multiple keywords. Template shape uncertainties Shape uncertainties can be taken into account by vertical interpolation of the histograms. The shapes are interpolated quadratically for shifts below 1\u03c3 and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row name shape effect_for_each_process_and_channel The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) The datacard in data/tutorials/shapes/simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a quadratic interpolation (within +/-1 sigma) and a linear extrapolation (beyond +/-1 sigma) of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. For both shape and shapeN , the total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: background_alphaUp and background_alphaDown , signal_sigmaUp and signal_sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard data/tutorials/shapes/simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC For a detailed example of a template based binned analysis see the H\u2192\u03c4\u03c4 2014 DAS tutorial Unbinned or parametric shape analysis In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. In the datacard using templates, the column after the file name would have been the name of the histogram. For the parametric analysis we need two names to identify the mapping, separated by a colon ( : ). shapes process channel shapes.root *workspace_name:pdf_name * The first part identifies the name of the input RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). There can be multiple input workspaces, just as there can be multiple input root files. You can use any of the usual RooFit pre-defined pdfs for your signal and background models. [danger] If you are using RooAddPdfs in your model in which the coefficients are not defined recursively , combine will not interpret them properly. You can add the option --X-rtd ADDNLL_RECURSIVE=0 to any combine command in order to recover the correct interpretation, however we recommend that you instead redefine your pdf so that the coefficients are recursive (as described on the RooAddPdf documentation ) and keep the total normalisation (i.e extended term) as a separate object as in the case of the tutorial datacard. For example, take a look at the data/tutorials/shapes/simple-shapes-parametric.txt . We see the following line. shapes * * simple-shapes-parametric_input.root w:$PROCESS [...] bin 1 1 process sig bkg which indicates that the input file simple-shapes-parametric_input.root should contain an input workspace ( w ) with pdfs named sig and bkg since these are the names of the two processes in the datacard. Additionally, we expect there to be a dataset named data_obs . If we look at the contents of the workspace inside data/tutorials/shapes/simple-shapes-parametric_input.root , this is indeed what we see... root [1] w->Print() RooWorkspace(w) w contents variables --------- (MH,bkg_norm,cc_a0,cc_a1,cc_a2,j,vogian_sigma,vogian_width) p.d.f.s ------- RooChebychev::bkg[ x=j coefList=(cc_a0,cc_a1,cc_a2) ] = 2.6243 RooVoigtian::sig[ x=j mean=MH width=vogian_width sigma=vogian_sigma ] = 0.000639771 datasets -------- RooDataSet::data_obs(j) In this datacard, the signal is parameterised in terms of the hypothesised mass ( MH ). Combine will use this variable, instead of creating its own, which will be interpreted as the value for -m . For this reason, we should add the option -m 30 (or something else within the observable range) when running combine. You will also see there is a variable named bkg_norm . This is used to normalize the background rate (see the section on Rate parameters below for details). [warning] Combine will not accept RooExtendedPdfs as an input. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. You should instead use RooAbsPdfs and provide the rate as a separate object (see the Rate parameters section). The part of the datacard related to the systematics can include lines with the syntax name param X Y These lines encode uncertainties on the parameters of the signal and background pdfs. The parameter is to be assigned a Gaussian uncertainty of Y around its mean value of X . One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. In the data/tutorials/shapes/simple-shapes-parametric.txt datacard, there are lines for one such parametric uncertainty, sigma param 1.0 0.1 meaning there is a parameter already contained in the input workspace called sigma which should be constrained with a Gaussian centered at 1.0 with a width of 0.1. Note that, the exact interpretation (i.e all combine knows is that 1.0 should be the most likely value and 0.1 is its 1\u03c3 uncertainy) of these parameters is left to the user since the signal pdf is constructed externally by you. Asymmetric uncertainties are written as with lnN using the syntax -1\u03c3/+1\u03c3 in the datacard. If one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: name flatParam Though this is not strictly necessary in frequentist methods using profiled likelihoods as combine will still profile these nuisances when performing fits (as is the case for the simple-shapes-parametric.txt datacard). [danger] All parameters which are floating or constant in the user's input workspaces will remain floating or constant. Combine will not modify those for you! A full example of a parametric analysis can be found in this H\u2192\u03b3\u03b3 2014 DAS tutorial Caveat on using parametric pdfs with binned datasets Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). There are two reccomended ways to work around this ... 1. Use narrow bins It is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets. 2. Use a RooParametricShapeBinPdf Another solution (currently implemented for 1-dimensional histograms only) is to use a custom pdf which performs the correct integrals internally as in RooParametricShapeBinPdf Note that this pdf class now allows parameters that are themselves RooAbsReal objects (i.e. functions of other variables). The integrals are handled internally by calling the underlying pdf\u2019s createIntegral() method with named ranges created for each of the bins. This means that if the analytical integrals for the underlying pdf are available, they will be used. The constructor for this class requires a RooAbsReal (eg any RooAbsPdf )along with a list of RooRealVars (the parameters, excluding the observable x x ), RooParametricShapeBinPdf(const char *name, const char *title, RooAbsReal& _pdf, RooAbsReal& _x, RooArgList& _pars, const TH1 &_shape ) Below is a comparison of a fit to a binned dataset containing 1000 events with one observable $x\\varepsilon \\left[0,100\\right] $. The fit function is a RooExponential of the form e^{xp} e^{xp} . In the upper plot, the data are binned in 100 evenly spaced bins, while in the lower plot, there are 3 irregular bins. The blue lines show the result of the fit when using the RooExponential directly while the red shows the result when wrapping the pdf inside a RooParametricShapeBinPdf . In the narrow binned case, the two agree well while for wide bins, accounting for the integral over the bin yields a better fit. You should note that using this class will result in slower fits so you should first decide if the added accuracy is enough to justify the reduced efficiency. Beyond simple datacards Datacards can be extended in order to provide additional functionality and flexibility during runtime. These can also allow for the production of more complicated models and performing advanced computation of results beyond limits and significances. Rate parameters The overall rate \"expected\" of a particular process in a particular bin does not necessarily need to be a fixed quantity. Scale factors can be introduced to modify the rate directly in the datacards for ANY type of analysis. This can be achieved using the directive rateParam in the datacard with the following syntax, name rateParam bin process initial_value [min,max] The [min,max] argument is optional and if not included, combine will remove the range of this parameter. This will produce a new parameter in the model (unless it already exists) which multiplies the rate of that particular process in the given bin by its value. You can attach the same rateParam to multiple processes/bins by either using a wild card (eg * will match everything, QCD_* will match everything starting with QCD_ etc.) in the name of the bin and/or process or by repeating the rateParam line in the datacard for different bins/processes with the same name. [warning] rateParam is not a shortcut to evaluate the post-fit yield of a process since other nuisances can also change the normalisation . E.g., finding that the rateParam best-fit value is 0.9 does not necessarily imply that the process yield is 0.9 times the initial one. The best is to evaluate the yield taking into account the values of all nuisance parameters using --saveNormalizations . This parameter is by default, freely floating. It is possible to include a Gaussian constraint on any rateParam which is floating (i.e not a formula or spline) by adding a param nuisance line in the datacard with the same name. In addition to rate modifiers which are freely floating, modifiers which are functions of other parameters can be included using the following syntax, name rateParam bin process formula args where args is a comma separated list of the arguments for the string formula . You can include other nuisance parameters in the formula , including ones which are Gaussian constrained (i,e via the param directive.) Below is an example datacard which uses the rateParam directive to implement an ABCD like method in combine. For a more realistic description of it's use for ABCD, see the single-lepton SUSY search implementation described here imax 4 number of channels jmax 0 number of processes -1 kmax * number of nuisance parameters (sources of systematical uncertainties) ------- bin B C D A observation 50 100 500 10 ------- bin B C D A process bkg bkg bkg bkg process 1 1 1 1 rate 1 1 1 1 ------- alpha rateParam A bkg (@0*@1/@2) beta,gamma,delta beta rateParam B bkg 50 gamma rateParam C bkg 100 delta rateParam D bkg 500 For more examples of using rateParam (eg for fitting process normalisations in control regions and signal regions simultaneously) see this 2016 CMS tutorial Finally, any pre-existing RooAbsReal inside some rootfile with a workspace can be imported using the following name rateParam bin process rootfile:workspacename The name should correspond to the name of the object which is being picked up inside the RooWorkspace. A simple example using the SM XS and BR splines available in HiggsAnalysis/CombinedLimit can be found under data/tutorials/rate_params/simple_sm_datacard.txt After running text2workspace.py on your datacard, you can check the normalisation objects using the tool test/printWorkspaceNormalisations.py . See the example below for the data/tutorials/shapes/simple-shapes-parametric.txt datacard. text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt python test/printWorkspaceNormalisations.py data/tutorials/shapes/simple-shapes-parametric.root ... --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - bin1 --------------------------------------------------------------------------- Top-level normalisation for process bkg -> n_exp_final_binbin1_proc_bkg ------------------------------------------------------------------------- RooProduct::n_exp_final_binbin1_proc_bkg[ n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm ] = 521.163 ... is a product, which contains n_exp_binbin1_proc_bkg RooRealVar::n_exp_binbin1_proc_bkg = 1 C L(-INF - +INF) ------------------------------------------------------------------------- default value = 521.163204829 --------------------------------------------------------------------------- Top-level normalisation for process sig -> n_exp_binbin1_proc_sig ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binbin1_proc_sig @ 0x464f700 nominal value: 1 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = lumi = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 1.0 This tells us that the normalisation for the background process, named n_exp_final_binbin1_proc_bkg is a product of two objects n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm . The first object is just from the rate line in the datacard (equal to 1) and the second is a floating parameter. For the signal, the normalisation is called n_exp_binbin1_proc_sig and is a ProcessNormalization object which contains the rate modifications due to the systematic uncertainties. You can see that it also has a \" nominal value \" which again is just from the value given in the rate line of the datacard (again=1). Extra arguments If a parameter is intended to be used and it is not a user defined param or rateParam , it can be picked up by first issuing an extArgs directive before this line in the datacard. The syntax for extArgs is name extArg rootfile:workspacename The string \":RecycleConflictNodes\" can be added at the end of the final argument (i.e. rootfile:workspacename:RecycleConflictNodes) to apply the corresponding RooFit option when the object is imported into the workspace. It is also possible to simply add a RooRealVar using extArg for use in function rateParams with the following name extArg init [min,max] Note that the [min,max] argument is optional and if not included, the code will remove the range of this parameter. Manipulation of Nuisance parameters It can often be useful to modify datacards, or the runtime behavior, without having to modify individual systematics lines. This can be acheived through the following. Nuisance modifiers If a nuisance parameter needs to be renamed for certain processes/channels, it can be done so using a single nuisance edit directive at the end of a datacard nuisance edit rename process channel oldname newname This will have the effect that nuisance parameter effecting a given process/channel will be renamed, thereby de-correlating it from other processes/channels. Use options ifexists to skip/avoid error if nuisance not found. Other edits are also supported as follows, nuisance edit add process channel name pdf value [options] -> add a new or add to a nuisance. If options is addq , value will be added in quadrature to this nuisance for this process/channel. If options is overwrite , the nuisance value will be replaced with this value nuisance edit drop process channel name [options] -> remove this nuisance from the process/channel. Use options ifexists to skip/avoid error if nuisance not found. nuisance edit changepdf name newpdf -> change the pdf type of a given nuisance to newpdf . nuisance edit split process channel oldname newname1 newname2 value1 value2 -> split a nuisance line into two separate nuisances called newname1 and newname2 with values value1 and value2 . Will produce two separate lines to that the original nuisance oldname becomes two uncorrelated nuisances. nuisance edit freeze name [options] -> set nuisance to frozen by default. Can be over-ridden in combine command line using --floatNuisances option Use options ifexists to skip/avoid error if nuisance not found. nuisance edit merge process channel name1 name2 -> merge systematic name2 into name1 by adding their values in quadrature and removing name2 . This only works if, for each process and channel included, they go in the same direction. For example, you can add 1.1 to 1.2, but not to 0.9. Note that the wildcard ( * ) can be used for either/both of process and channel. The above edits support nuisances which are any of shape[N] , lnN , lnU , gmN , param , flatParam , rateParam or discrete types. Groups of nuisances Often it is desirable to freeze one or more nuisances to check the impact they have on limits, likelihood scans, significances etc. However, for large groups of nuisances (eg everything associated to theory) it is easier to define nuisance groups in the datacard. The following line in a datacard will, for example, produce a group of nuisances with the group name theory which contains two parameters, QCDscale and pdf . theory group = QCDscale pdf Multiple groups can be defined in this way. It is also possible to extend nuisance groups in datacards using += in place of = . These groups can be manipulated at runtime (eg for freezing all nuisances associated to a group at runtime, see Running the tool ). You can find more info on groups of nuisances here Note that when using the automatic addition of statistical uncertainties (autoMCStats), the corresponding nuisance parameters are created by text2workspace.py and so do not exist in the datacards. It is therefore not possible to add autoMCStats parameters to groups of nuisances in the way described above. However, text2workspace.py will automatically create a group labelled autoMCStats which contains all autoMCStats parameters. This group is useful for freezing all parameters created by autoMCStats. For freezing subsets of the parameters, for example if the datacard contains two categories, cat_label_1 and cat_label_2 , to only freeze the autoMCStat parameters created for category cat_label_1 the regular expression features can be used. In this example this can be achieved by using --freezeParameters 'rgx{prop_bincat_label_1_bin.*}' . Combination of multiple datacards If you have separate channels each with it's own datacard, it is possible to produce a combined datacard using the script combineCards.py The syntax is simple: combineCards.py Name1=card1.txt Name2=card2.txt .... > card.txt If the input datacards had just one bin each, then the output channels will be called Name1 , Name2 , and so on. Otherwise, a prefix Name1_ ... Name2_ will be added to the bin labels in each datacard. The supplied bin names Name1 , Name2 , etc. must themselves conform to valid C++/python identifier syntax. [warning] Systematics which have different names will be assumed to be uncorrelated, and the ones with the same name will be assumed 100% correlated. A systematic correlated across channels must have the same p.d.f. in all cards (i.e. always lnN , or all gmN with same N ) The combineCards.py script will complain if you are trying to combine a shape datacard with a counting datacard. You can however convert a counting datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax and kmax section. Alternatively, you can add the option -S in combineCards.py which will do this for you while making the combination. Automatic production of datacards and workspaces For complicated analyses or cases in which multiple datacards are needed (e.g. optimisation studies), you can avoid writing these by hand. The object Datacard defines the analysis and can be created as a python object. The template python script below will produce the same workspace as running textToWorkspace.py (see the section on Physics Models ) on the realistic-counting-experiment.txt datacard. from HiggsAnalysis.CombinedLimit.DatacardParser import * from HiggsAnalysis.CombinedLimit.ModelTools import * from HiggsAnalysis.CombinedLimit.ShapeTools import * from HiggsAnalysis.CombinedLimit.PhysicsModel import * from sys import exit from optparse import OptionParser parser = OptionParser() addDatacardParserOptions(parser) options,args = parser.parse_args() options.bin = True # make a binary workspace DC = Datacard() MB = None ############## Setup the datacard (must be filled in) ########################### DC.bins = ['bin1'] # <type 'list'> DC.obs = {'bin1': 0.0} # <type 'dict'> DC.processes = ['ggH', 'qqWW', 'ggWW', 'others'] # <type 'list'> DC.signals = ['ggH'] # <type 'list'> DC.isSignal = {'qqWW': False, 'ggWW': False, 'ggH': True, 'others': False} # <type 'dict'> DC.keyline = [('bin1', 'ggH', True), ('bin1', 'qqWW', False), ('bin1', 'ggWW', False), ('bin1', 'others', False)] # <type 'list'> DC.exp = {'bin1': {'qqWW': 0.63, 'ggWW': 0.06, 'ggH': 1.47, 'others': 0.22}} # <type 'dict'> DC.systs = [('lumi', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.11, 'ggH': 1.11, 'others': 0.0}}), ('xs_ggH', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 1.16, 'others': 0.0}}), ('WW_norm', False, 'gmN', [4], {'bin1': {'qqWW': 0.16, 'ggWW': 0.0, 'ggH': 0.0, 'others': 0.0}}), ('xs_ggWW', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.5, 'ggH': 0.0, 'others': 0.0}}), ('bg_others', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 0.0, 'others': 1.3}})] # <type 'list'> DC.shapeMap = {} # <type 'dict'> DC.hasShapes = False # <type 'bool'> DC.flatParamNuisances = {} # <type 'dict'> DC.rateParams = {} # <type 'dict'> DC.extArgs = {} # <type 'dict'> DC.rateParamsOrder = set([]) # <type 'set'> DC.frozenNuisances = set([]) # <type 'set'> DC.systematicsShapeMap = {} # <type 'dict'> DC.nuisanceEditLines = [] # <type 'list'> DC.groups = {} # <type 'dict'> DC.discretes = [] # <type 'list'> ###### User defined options ############################################# options.out = \"combine_workspace.root\" # Output workspace name options.fileName = \"./\" # Path to input ROOT files options.verbose = \"1\" # Verbosity ########################################################################## if DC.hasShapes: MB = ShapeBuilder(DC, options) else: MB = CountingModelBuilder(DC, options) # Set physics models MB.setPhysics(defaultModel) MB.doModel() Any existing datacard can be converted into such a template python script by using the --dump-datacard option in text2workspace.py in case a more complicated template is needed. [warning] The above is not advised for final results as this script is not easily combined with other analyses so should only be used for internal studies. For the automatic generation of datacards (which are combinable), you should instead use the CombineHarvester package which includes many features for producing complex datacards in a reliable, automated way.","title":"Setting up the analysis"},{"location":"part2/settinguptheanalysis/#preparing-the-datacard","text":"The input to combine, which defines the details of the experiment, is a datacard file is a plain ASCII file. This is true whether the experiment is a simple counting experiment or a shape analysis.","title":"Preparing the datacard"},{"location":"part2/settinguptheanalysis/#a-simple-counting-experiment","text":"The file data/tutorials/counting/realistic-counting-experiment.txt shows an example of a counting experiment. The first lines can be used as a description and are not parsed by the program. They have to begin with a \"#\": # Simple counting experiment, with one signal and a few background processes # Simplified version of the 35/pb H->WW analysis for mH = 160 GeV Then one declares the number of observables , imax , that are present in the model used to calculate limits/significances. The number of observables will typically be the number of channels in a counting experiment or the number of bins in a binned shape fit. (If one specifies for imax the value * it means \"figure it out from the rest of the datacard\", but in order to better catch mistakes it's recommended to specify it explicitly) imax 1 number of channels Then one declares the number of background sources to be considered, jmax , and the number of independent sources of systematic uncertainties , kmax : jmax 3 number of backgrounds kmax 5 number of nuisance parameters (sources of systematic uncertainties) In the example there is 1 channel, there are 3 background sources, and there are 5 independent sources of systematic uncertainty. Then there are the lines describing what is actually observed: the number of events observed in each channel. The first line, starting with bin defines the label used for each channel. In the example we have 1 channel, labelled 1 , and in the following line, observation , are listed the events observed, 0 in this example: # we have just one channel, in which we observe 0 events bin bin1 observation 0 Following is the part related to the number of events expected, for each bin and process, arranged in (#channels)*(#processes) columns. bin bin1 bin1 bin1 bin1 process ggH qqWW ggWW others process 0 1 2 3 rate 1.47 0.63 0.06 0.22 The bin line identifies the channel the column is referring to. It goes from 1 to the imax declared above. The first process line contains the labels of the various sources The second process line must have a positive number for backgrounds, and 0 or a negative number for the signals. You should use different process ids for different processes. The last line, rate , tells the expected yield of events in the specified bin and process All bins should be declared in increasing order, and within each bin one should include all processes in increasing order, specifying a 0 for processes that do not contribute. The last section contains the description of the systematic uncertainties: lumi lnN 1.11 - 1.11 - lumi affects both signal and gg->WW (mc-driven). lnN = lognormal xs_ggH lnN 1.16 - - - gg->H cross section + signal efficiency + other minor ones. WW_norm gmN 4 - 0.16 - - WW estimate of 0.64 comes from sidebands: 4 events in sideband times 0.16 (=> ~50% statistical uncertainty) xs_ggWW lnN - - 1.50 - 50% uncertainty on gg->WW cross section bg_others lnN - - - 1.30 30% uncertainty on the rest of the backgrounds the first columns is a label identifying the uncertainty the second column identifies the type of distribution used lnN stands for Log-normal , which is the recommended choice for multiplicative corrections (efficiencies, cross sections, ...). If \u0394x/x is the relative uncertainty on the multiplicative correction, one should put 1+\u0394x/x in the column corresponding to the process and channel. Asymetric log-normals are supported by providing \u03ba down /\u03ba up where \u03ba down is the ratio of the the yield to the nominal value for a -1\u03c3 deviation of the nuisance and \u03ba up is the ratio of the the yield to the nominal value for a +1\u03c3 deviation. gmN stands for Gamma , and is the recommended choice for the statistical uncertainty on a background coming from the number of events in a control region (or in a MC sample with limited statistics). If the control region or MC contains N events, and the extrapolation factor from the control region to the signal region is \u03b1 then one shoud put N just after the gmN keyword, and then the value of \u03b1 in the proper column. Also, the yield in the rate row should match with N\u03b1 lnU stands for log-uniform distribution. A value of 1+\u03b5 in the column will imply that the yield of this background is allowed to float freely between x(1+\u03b5) and x/(1+\u03b5) (in particular, if \u03b5 is small, then this is approximately (x-\u0394x,x+\u0394x) with \u03b5=\u0394x/x ) This is normally useful when you want to set a large a-priori uncertainty on a given background and then rely on the correlation between channels to constrain it. Beware that while Gaussian-like uncertainties behave in a similar way under profiling and marginalization, uniform uncertainties do not, so the impact of the uncertainty on the result will depend on how the nuisances are treated. then there are (#channels)*(#processes) columns reporting the relative effect of the systematic uncertainty on the rate of each process in each channel. The columns are aligned with the ones in the previous lines declaring bins, processes and rates. In the example, there are 5 uncertainties: the first uncertainty affects the signal by 11%, and affects the ggWW process by 11% the second uncertainty affects the signal by 16% leaving the backgrounds unaffected the third line specifies that the qqWW background comes from a sideband with 4 observed events and an extrapolation factor of 0.16; the resulting uncertainty on the expected yield is 1/\\sqrt{4+1} 1/\\sqrt{4+1} = 45% the fourth uncertainty does not affect the signal, affects the ggWW background by 50%, leaving the other backgrounds unaffected the last uncertainty does not affect the signal, affects by 30% the others backgrounds, leaving the rest of the backgrounds unaffected","title":"A simple counting experiment"},{"location":"part2/settinguptheanalysis/#shape-analysis","text":"The datacard has to be supplemented with two extensions: A new block of lines defining how channels and processes are mapped into shapes The block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a RooDataSet to perform an unbinned shape analysis. [danger] If using RooFit based inputs (RooDataHists/RooDataSets/RooAbsPdfs) then you should be careful to use different RooRealVars as the observable in each category being combined. It is possible to use the same RooRealVar if the observable has the same range (and binning if using binned data) in each category though in most cases it is simpler to avoid doing so.","title":"Shape analysis"},{"location":"part2/settinguptheanalysis/#rates-for-shape-analysis","text":"As with the counting experiment, the total nominal rate of a given process must be identified in the rate line of the datacard. However, there are special options for shape based analyses as follows A value of -1 in the rate line indicates to combine to calculate the rate from the input TH1 (via TH1::Integral) or RooDataSet/RooDataHist (via RooAbsData::sumEntries) For parametric shapes (RooAbsPdf), if a parameter is found in the input workspace with the name pdfname _norm the rate will be multiplied by the value of that parameter. Note that since this parameter can be freely floating, the normalization of a shape can be made to freely float this way. This can also be achieved through the use of rateParams","title":"Rates for shape analysis"},{"location":"part2/settinguptheanalysis/#binned-shape-analysis","text":"For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [histogram_with_systematics] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels file , histogram and histogram_with_systematics identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool In addition, user defined keywords can be included to be replaced. Any word in the datacard $WORD will be replaced by VALUE when including the option --keyword-value WORD=VALUE . The option can be repeated multiple times for multiple keywords.","title":"Binned shape analysis"},{"location":"part2/settinguptheanalysis/#template-shape-uncertainties","text":"Shape uncertainties can be taken into account by vertical interpolation of the histograms. The shapes are interpolated quadratically for shifts below 1\u03c3 and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row name shape effect_for_each_process_and_channel The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) The datacard in data/tutorials/shapes/simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a quadratic interpolation (within +/-1 sigma) and a linear extrapolation (beyond +/-1 sigma) of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. For both shape and shapeN , the total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: background_alphaUp and background_alphaDown , signal_sigmaUp and signal_sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard data/tutorials/shapes/simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC For a detailed example of a template based binned analysis see the H\u2192\u03c4\u03c4 2014 DAS tutorial","title":"Template shape uncertainties"},{"location":"part2/settinguptheanalysis/#unbinned-or-parametric-shape-analysis","text":"In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. In the datacard using templates, the column after the file name would have been the name of the histogram. For the parametric analysis we need two names to identify the mapping, separated by a colon ( : ). shapes process channel shapes.root *workspace_name:pdf_name * The first part identifies the name of the input RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). There can be multiple input workspaces, just as there can be multiple input root files. You can use any of the usual RooFit pre-defined pdfs for your signal and background models. [danger] If you are using RooAddPdfs in your model in which the coefficients are not defined recursively , combine will not interpret them properly. You can add the option --X-rtd ADDNLL_RECURSIVE=0 to any combine command in order to recover the correct interpretation, however we recommend that you instead redefine your pdf so that the coefficients are recursive (as described on the RooAddPdf documentation ) and keep the total normalisation (i.e extended term) as a separate object as in the case of the tutorial datacard. For example, take a look at the data/tutorials/shapes/simple-shapes-parametric.txt . We see the following line. shapes * * simple-shapes-parametric_input.root w:$PROCESS [...] bin 1 1 process sig bkg which indicates that the input file simple-shapes-parametric_input.root should contain an input workspace ( w ) with pdfs named sig and bkg since these are the names of the two processes in the datacard. Additionally, we expect there to be a dataset named data_obs . If we look at the contents of the workspace inside data/tutorials/shapes/simple-shapes-parametric_input.root , this is indeed what we see... root [1] w->Print() RooWorkspace(w) w contents variables --------- (MH,bkg_norm,cc_a0,cc_a1,cc_a2,j,vogian_sigma,vogian_width) p.d.f.s ------- RooChebychev::bkg[ x=j coefList=(cc_a0,cc_a1,cc_a2) ] = 2.6243 RooVoigtian::sig[ x=j mean=MH width=vogian_width sigma=vogian_sigma ] = 0.000639771 datasets -------- RooDataSet::data_obs(j) In this datacard, the signal is parameterised in terms of the hypothesised mass ( MH ). Combine will use this variable, instead of creating its own, which will be interpreted as the value for -m . For this reason, we should add the option -m 30 (or something else within the observable range) when running combine. You will also see there is a variable named bkg_norm . This is used to normalize the background rate (see the section on Rate parameters below for details). [warning] Combine will not accept RooExtendedPdfs as an input. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. You should instead use RooAbsPdfs and provide the rate as a separate object (see the Rate parameters section). The part of the datacard related to the systematics can include lines with the syntax name param X Y These lines encode uncertainties on the parameters of the signal and background pdfs. The parameter is to be assigned a Gaussian uncertainty of Y around its mean value of X . One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. In the data/tutorials/shapes/simple-shapes-parametric.txt datacard, there are lines for one such parametric uncertainty, sigma param 1.0 0.1 meaning there is a parameter already contained in the input workspace called sigma which should be constrained with a Gaussian centered at 1.0 with a width of 0.1. Note that, the exact interpretation (i.e all combine knows is that 1.0 should be the most likely value and 0.1 is its 1\u03c3 uncertainy) of these parameters is left to the user since the signal pdf is constructed externally by you. Asymmetric uncertainties are written as with lnN using the syntax -1\u03c3/+1\u03c3 in the datacard. If one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: name flatParam Though this is not strictly necessary in frequentist methods using profiled likelihoods as combine will still profile these nuisances when performing fits (as is the case for the simple-shapes-parametric.txt datacard). [danger] All parameters which are floating or constant in the user's input workspaces will remain floating or constant. Combine will not modify those for you! A full example of a parametric analysis can be found in this H\u2192\u03b3\u03b3 2014 DAS tutorial","title":"Unbinned or parametric shape analysis"},{"location":"part2/settinguptheanalysis/#caveat-on-using-parametric-pdfs-with-binned-datasets","text":"Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). There are two reccomended ways to work around this ... 1. Use narrow bins It is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets. 2. Use a RooParametricShapeBinPdf Another solution (currently implemented for 1-dimensional histograms only) is to use a custom pdf which performs the correct integrals internally as in RooParametricShapeBinPdf Note that this pdf class now allows parameters that are themselves RooAbsReal objects (i.e. functions of other variables). The integrals are handled internally by calling the underlying pdf\u2019s createIntegral() method with named ranges created for each of the bins. This means that if the analytical integrals for the underlying pdf are available, they will be used. The constructor for this class requires a RooAbsReal (eg any RooAbsPdf )along with a list of RooRealVars (the parameters, excluding the observable x x ), RooParametricShapeBinPdf(const char *name, const char *title, RooAbsReal& _pdf, RooAbsReal& _x, RooArgList& _pars, const TH1 &_shape ) Below is a comparison of a fit to a binned dataset containing 1000 events with one observable $x\\varepsilon \\left[0,100\\right] $. The fit function is a RooExponential of the form e^{xp} e^{xp} . In the upper plot, the data are binned in 100 evenly spaced bins, while in the lower plot, there are 3 irregular bins. The blue lines show the result of the fit when using the RooExponential directly while the red shows the result when wrapping the pdf inside a RooParametricShapeBinPdf . In the narrow binned case, the two agree well while for wide bins, accounting for the integral over the bin yields a better fit. You should note that using this class will result in slower fits so you should first decide if the added accuracy is enough to justify the reduced efficiency.","title":"Caveat on using parametric pdfs with binned datasets"},{"location":"part2/settinguptheanalysis/#beyond-simple-datacards","text":"Datacards can be extended in order to provide additional functionality and flexibility during runtime. These can also allow for the production of more complicated models and performing advanced computation of results beyond limits and significances.","title":"Beyond simple datacards"},{"location":"part2/settinguptheanalysis/#rate-parameters","text":"The overall rate \"expected\" of a particular process in a particular bin does not necessarily need to be a fixed quantity. Scale factors can be introduced to modify the rate directly in the datacards for ANY type of analysis. This can be achieved using the directive rateParam in the datacard with the following syntax, name rateParam bin process initial_value [min,max] The [min,max] argument is optional and if not included, combine will remove the range of this parameter. This will produce a new parameter in the model (unless it already exists) which multiplies the rate of that particular process in the given bin by its value. You can attach the same rateParam to multiple processes/bins by either using a wild card (eg * will match everything, QCD_* will match everything starting with QCD_ etc.) in the name of the bin and/or process or by repeating the rateParam line in the datacard for different bins/processes with the same name. [warning] rateParam is not a shortcut to evaluate the post-fit yield of a process since other nuisances can also change the normalisation . E.g., finding that the rateParam best-fit value is 0.9 does not necessarily imply that the process yield is 0.9 times the initial one. The best is to evaluate the yield taking into account the values of all nuisance parameters using --saveNormalizations . This parameter is by default, freely floating. It is possible to include a Gaussian constraint on any rateParam which is floating (i.e not a formula or spline) by adding a param nuisance line in the datacard with the same name. In addition to rate modifiers which are freely floating, modifiers which are functions of other parameters can be included using the following syntax, name rateParam bin process formula args where args is a comma separated list of the arguments for the string formula . You can include other nuisance parameters in the formula , including ones which are Gaussian constrained (i,e via the param directive.) Below is an example datacard which uses the rateParam directive to implement an ABCD like method in combine. For a more realistic description of it's use for ABCD, see the single-lepton SUSY search implementation described here imax 4 number of channels jmax 0 number of processes -1 kmax * number of nuisance parameters (sources of systematical uncertainties) ------- bin B C D A observation 50 100 500 10 ------- bin B C D A process bkg bkg bkg bkg process 1 1 1 1 rate 1 1 1 1 ------- alpha rateParam A bkg (@0*@1/@2) beta,gamma,delta beta rateParam B bkg 50 gamma rateParam C bkg 100 delta rateParam D bkg 500 For more examples of using rateParam (eg for fitting process normalisations in control regions and signal regions simultaneously) see this 2016 CMS tutorial Finally, any pre-existing RooAbsReal inside some rootfile with a workspace can be imported using the following name rateParam bin process rootfile:workspacename The name should correspond to the name of the object which is being picked up inside the RooWorkspace. A simple example using the SM XS and BR splines available in HiggsAnalysis/CombinedLimit can be found under data/tutorials/rate_params/simple_sm_datacard.txt After running text2workspace.py on your datacard, you can check the normalisation objects using the tool test/printWorkspaceNormalisations.py . See the example below for the data/tutorials/shapes/simple-shapes-parametric.txt datacard. text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt python test/printWorkspaceNormalisations.py data/tutorials/shapes/simple-shapes-parametric.root ... --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - bin1 --------------------------------------------------------------------------- Top-level normalisation for process bkg -> n_exp_final_binbin1_proc_bkg ------------------------------------------------------------------------- RooProduct::n_exp_final_binbin1_proc_bkg[ n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm ] = 521.163 ... is a product, which contains n_exp_binbin1_proc_bkg RooRealVar::n_exp_binbin1_proc_bkg = 1 C L(-INF - +INF) ------------------------------------------------------------------------- default value = 521.163204829 --------------------------------------------------------------------------- Top-level normalisation for process sig -> n_exp_binbin1_proc_sig ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binbin1_proc_sig @ 0x464f700 nominal value: 1 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = lumi = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 1.0 This tells us that the normalisation for the background process, named n_exp_final_binbin1_proc_bkg is a product of two objects n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm . The first object is just from the rate line in the datacard (equal to 1) and the second is a floating parameter. For the signal, the normalisation is called n_exp_binbin1_proc_sig and is a ProcessNormalization object which contains the rate modifications due to the systematic uncertainties. You can see that it also has a \" nominal value \" which again is just from the value given in the rate line of the datacard (again=1).","title":"Rate parameters"},{"location":"part2/settinguptheanalysis/#extra-arguments","text":"If a parameter is intended to be used and it is not a user defined param or rateParam , it can be picked up by first issuing an extArgs directive before this line in the datacard. The syntax for extArgs is name extArg rootfile:workspacename The string \":RecycleConflictNodes\" can be added at the end of the final argument (i.e. rootfile:workspacename:RecycleConflictNodes) to apply the corresponding RooFit option when the object is imported into the workspace. It is also possible to simply add a RooRealVar using extArg for use in function rateParams with the following name extArg init [min,max] Note that the [min,max] argument is optional and if not included, the code will remove the range of this parameter.","title":"Extra arguments"},{"location":"part2/settinguptheanalysis/#manipulation-of-nuisance-parameters","text":"It can often be useful to modify datacards, or the runtime behavior, without having to modify individual systematics lines. This can be acheived through the following.","title":"Manipulation of Nuisance parameters"},{"location":"part2/settinguptheanalysis/#nuisance-modifiers","text":"If a nuisance parameter needs to be renamed for certain processes/channels, it can be done so using a single nuisance edit directive at the end of a datacard nuisance edit rename process channel oldname newname This will have the effect that nuisance parameter effecting a given process/channel will be renamed, thereby de-correlating it from other processes/channels. Use options ifexists to skip/avoid error if nuisance not found. Other edits are also supported as follows, nuisance edit add process channel name pdf value [options] -> add a new or add to a nuisance. If options is addq , value will be added in quadrature to this nuisance for this process/channel. If options is overwrite , the nuisance value will be replaced with this value nuisance edit drop process channel name [options] -> remove this nuisance from the process/channel. Use options ifexists to skip/avoid error if nuisance not found. nuisance edit changepdf name newpdf -> change the pdf type of a given nuisance to newpdf . nuisance edit split process channel oldname newname1 newname2 value1 value2 -> split a nuisance line into two separate nuisances called newname1 and newname2 with values value1 and value2 . Will produce two separate lines to that the original nuisance oldname becomes two uncorrelated nuisances. nuisance edit freeze name [options] -> set nuisance to frozen by default. Can be over-ridden in combine command line using --floatNuisances option Use options ifexists to skip/avoid error if nuisance not found. nuisance edit merge process channel name1 name2 -> merge systematic name2 into name1 by adding their values in quadrature and removing name2 . This only works if, for each process and channel included, they go in the same direction. For example, you can add 1.1 to 1.2, but not to 0.9. Note that the wildcard ( * ) can be used for either/both of process and channel. The above edits support nuisances which are any of shape[N] , lnN , lnU , gmN , param , flatParam , rateParam or discrete types.","title":"Nuisance modifiers"},{"location":"part2/settinguptheanalysis/#groups-of-nuisances","text":"Often it is desirable to freeze one or more nuisances to check the impact they have on limits, likelihood scans, significances etc. However, for large groups of nuisances (eg everything associated to theory) it is easier to define nuisance groups in the datacard. The following line in a datacard will, for example, produce a group of nuisances with the group name theory which contains two parameters, QCDscale and pdf . theory group = QCDscale pdf Multiple groups can be defined in this way. It is also possible to extend nuisance groups in datacards using += in place of = . These groups can be manipulated at runtime (eg for freezing all nuisances associated to a group at runtime, see Running the tool ). You can find more info on groups of nuisances here Note that when using the automatic addition of statistical uncertainties (autoMCStats), the corresponding nuisance parameters are created by text2workspace.py and so do not exist in the datacards. It is therefore not possible to add autoMCStats parameters to groups of nuisances in the way described above. However, text2workspace.py will automatically create a group labelled autoMCStats which contains all autoMCStats parameters. This group is useful for freezing all parameters created by autoMCStats. For freezing subsets of the parameters, for example if the datacard contains two categories, cat_label_1 and cat_label_2 , to only freeze the autoMCStat parameters created for category cat_label_1 the regular expression features can be used. In this example this can be achieved by using --freezeParameters 'rgx{prop_bincat_label_1_bin.*}' .","title":"Groups of nuisances"},{"location":"part2/settinguptheanalysis/#combination-of-multiple-datacards","text":"If you have separate channels each with it's own datacard, it is possible to produce a combined datacard using the script combineCards.py The syntax is simple: combineCards.py Name1=card1.txt Name2=card2.txt .... > card.txt If the input datacards had just one bin each, then the output channels will be called Name1 , Name2 , and so on. Otherwise, a prefix Name1_ ... Name2_ will be added to the bin labels in each datacard. The supplied bin names Name1 , Name2 , etc. must themselves conform to valid C++/python identifier syntax. [warning] Systematics which have different names will be assumed to be uncorrelated, and the ones with the same name will be assumed 100% correlated. A systematic correlated across channels must have the same p.d.f. in all cards (i.e. always lnN , or all gmN with same N ) The combineCards.py script will complain if you are trying to combine a shape datacard with a counting datacard. You can however convert a counting datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax and kmax section. Alternatively, you can add the option -S in combineCards.py which will do this for you while making the combination.","title":"Combination of multiple datacards"},{"location":"part2/settinguptheanalysis/#automatic-production-of-datacards-and-workspaces","text":"For complicated analyses or cases in which multiple datacards are needed (e.g. optimisation studies), you can avoid writing these by hand. The object Datacard defines the analysis and can be created as a python object. The template python script below will produce the same workspace as running textToWorkspace.py (see the section on Physics Models ) on the realistic-counting-experiment.txt datacard. from HiggsAnalysis.CombinedLimit.DatacardParser import * from HiggsAnalysis.CombinedLimit.ModelTools import * from HiggsAnalysis.CombinedLimit.ShapeTools import * from HiggsAnalysis.CombinedLimit.PhysicsModel import * from sys import exit from optparse import OptionParser parser = OptionParser() addDatacardParserOptions(parser) options,args = parser.parse_args() options.bin = True # make a binary workspace DC = Datacard() MB = None ############## Setup the datacard (must be filled in) ########################### DC.bins = ['bin1'] # <type 'list'> DC.obs = {'bin1': 0.0} # <type 'dict'> DC.processes = ['ggH', 'qqWW', 'ggWW', 'others'] # <type 'list'> DC.signals = ['ggH'] # <type 'list'> DC.isSignal = {'qqWW': False, 'ggWW': False, 'ggH': True, 'others': False} # <type 'dict'> DC.keyline = [('bin1', 'ggH', True), ('bin1', 'qqWW', False), ('bin1', 'ggWW', False), ('bin1', 'others', False)] # <type 'list'> DC.exp = {'bin1': {'qqWW': 0.63, 'ggWW': 0.06, 'ggH': 1.47, 'others': 0.22}} # <type 'dict'> DC.systs = [('lumi', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.11, 'ggH': 1.11, 'others': 0.0}}), ('xs_ggH', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 1.16, 'others': 0.0}}), ('WW_norm', False, 'gmN', [4], {'bin1': {'qqWW': 0.16, 'ggWW': 0.0, 'ggH': 0.0, 'others': 0.0}}), ('xs_ggWW', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.5, 'ggH': 0.0, 'others': 0.0}}), ('bg_others', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 0.0, 'others': 1.3}})] # <type 'list'> DC.shapeMap = {} # <type 'dict'> DC.hasShapes = False # <type 'bool'> DC.flatParamNuisances = {} # <type 'dict'> DC.rateParams = {} # <type 'dict'> DC.extArgs = {} # <type 'dict'> DC.rateParamsOrder = set([]) # <type 'set'> DC.frozenNuisances = set([]) # <type 'set'> DC.systematicsShapeMap = {} # <type 'dict'> DC.nuisanceEditLines = [] # <type 'list'> DC.groups = {} # <type 'dict'> DC.discretes = [] # <type 'list'> ###### User defined options ############################################# options.out = \"combine_workspace.root\" # Output workspace name options.fileName = \"./\" # Path to input ROOT files options.verbose = \"1\" # Verbosity ########################################################################## if DC.hasShapes: MB = ShapeBuilder(DC, options) else: MB = CountingModelBuilder(DC, options) # Set physics models MB.setPhysics(defaultModel) MB.doModel() Any existing datacard can be converted into such a template python script by using the --dump-datacard option in text2workspace.py in case a more complicated template is needed. [warning] The above is not advised for final results as this script is not easily combined with other analyses so should only be used for internal studies. For the automatic generation of datacards (which are combinable), you should instead use the CombineHarvester package which includes many features for producing complex datacards in a reliable, automated way.","title":"Automatic production of datacards and workspaces"},{"location":"part3/commonstatsmethods/","text":"Common Statistical Methods In this section, the most commonly used statistical methods from combine will be covered including specific instructions on how to obtain limits, significances and likelihood scans. For all of these methods, the assumed parameters of interest (POI) is the overall signal strength r (i.e the default PhysicsModel). In general however, the first POI in the list of POIs (as defined by the PhysicsModel) will be taken instead of r which may or may not make sense for a given method ... use your judgment! This section will assume that you are using the default model unless otherwise specified. Asymptotic Frequentist Limits The AsymptoticLimits method allows to compute quickly an estimate of the observed and expected limits, which is fairly accurate when the event yields are not too small and the systematic uncertainties don't play a major role in the result. The limit calculation relies on an asymptotic approximation of the distributions of the LHC test-statistic, which is based on a profile likelihood ratio, under signal and background hypotheses to compute CL s+b , CL b and therefore CL s =CL s+b /CL b - i.e it is the asymptotic approximation of computing limits with frequentist toys. This method is so commonly used that it is the default method (i.e not specifying -M will run AsymptoticLimits ) A realistic example of datacard for a counting experiment can be found in the HiggsCombination package: data/tutorials/counting/realistic-counting-experiment.txt The method can be run using combine -M AsymptoticLimits realistic-counting-experiment.txt The program will print out the limit on the signal strength r (number of signal events / number of expected signal events) e .g. Observed Limit: r < 1.6297 @ 95% CL , the median expected limit Expected 50.0%: r < 2.3111 and edges of the 68% and 95% ranges for the expected limits. <<< Combine >>> >>> including systematics >>> method used to compute upper limit is AsymptoticLimits [...] -- AsymptoticLimits ( CLs ) -- Observed Limit: r < 1.6281 Expected 2.5%: r < 0.9640 Expected 16.0%: r < 1.4329 Expected 50.0%: r < 2.3281 Expected 84.0%: r < 3.9800 Expected 97.5%: r < 6.6194 Done in 0.01 min (cpu), 0.01 min (real) By default, the limits are calculated using the CL s prescription, as noted in the output, which takes the ratio of p-values under the signal plus background and background only hypothesis. This can be altered to using the strict p-value by using the option --rule CLsplusb You can also change the confidence level (default is 95%) to 90% using the option --cl 0.9 or any other confidence level. You can find the full list of options for AsymptoticLimits using --help -M AsymptoticLimits . [warning] You may find that combine issues a warning that the best fit for the background-only Asimov dataset returns a non-zero value for the signal strength for example; WARNING: Best fit of asimov dataset is at r = 0.220944 (0.011047 times rMax), while it should be at zero If this happens, you should check to make sure that there are no issues with the datacard or the Asimov generation used for your setup. For details on debugging it is recommended that you follow the simple checks used by the HIG PAG here . The program will also create a rootfile higgsCombineTest.AsymptoticLimits.mH120.root containing a root tree limit that contains the limit values and other bookeeping information. The important columns are limit (the limit value) and quantileExpected (-1 for observed limit, 0.5 for median expected limit, 0.16/0.84 for the edges of the 65% interval band of expected limits, 0.025/0.975 for 95%). $ root -l higgsCombineTest.AsymptoticLimits.mH120.root root [0] limit->Scan(\"*\") ************************************************************************************************************************************ * Row * limit * limitErr * mh * syst * iToy * iSeed * iChannel * t_cpu * t_real * quantileE * ************************************************************************************************************************************ * 0 * 0.9639892 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.0250000 * * 1 * 1.4329109 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.1599999 * * 2 * 2.328125 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.5 * * 3 * 3.9799661 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.8399999 * * 4 * 6.6194028 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.9750000 * * 5 * 1.6281188 * 0.0050568 * 120 * 1 * 0 * 123456 * 0 * 0.0035000 * 0.0055123 * -1 * ************************************************************************************************************************************ Blind limits The AsymptoticLimits calculation follows the frequentist paradigm for calculating expected limits. This means that the routine will first fit the observed data, conditionally for a fixed value of r and set the nuisance parameters to the values obtained in the fit for generating the Asimov data, i.e it calculates the post-fit or a-posteriori expected limit. In order to use the pre-fit nuisance parameters (to calculate an a-priori limit), you must add the option --noFitAsimov or --bypassFrequentistFit . For blinding the results completely (i.e not using the data) you can include the option --run blind . [warning] You should never use -t -1 to get blind limits! Splitting points In case your model is particularly complex, you can perform the asymptotic calculation by determining the value of CL s for a set grid of points (in r ) and merging the results. This is done by using the option --singlePoint X for multiple values of X, hadding the output files and reading them back in, combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.1 -n 0.1 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.2 -n 0.2 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.3 -n 0.3 ... hadd limits.root higgsCombine*.AsymptoticLimits.* combine -M AsymptoticLimits realistic-counting-experiment.txt --getLimitFromGrid limits.root Asymptotic Significances The significance of a result is calculated using a ratio of profiled likelihoods, one in which the signal strength is set to 0 and the other in which it is free to float, i.e the quantity is -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The distribution of this test-statistic can be determined using Wilke's theorem provided the number of events is large enough (i.e in the Asymptotic limit ). The significance (or p-value) can therefore be calculated very quickly and uses the Significance method. It is also possible to calculate the ratio of likelihoods between the freely floating signal strength to that of a fixed signal strength other than 0 , by specifying it with the option --signalForSignificance=X [info] This calculation assumes that the signal strength can only be positive (i.e we are not interested in negative signal strengths). This can be altered by including the option --uncapped Compute the observed significance The observed significance is calculated using the Significance method, as combine -M Significance datacard.txt The printed output will report the significance and the p-value, for example, when using the realistic-counting-experiment.txt datacard, you will see <<< Combine >>> >>> including systematics >>> method used is Significance [...] -- Significance -- Significance: 0 (p-value = 0.5) Done in 0.00 min (cpu), 0.01 min (real) which is not surprising since 0 events were observed in that datacard. The output root file will contain the significance value in the branch limit . To store the p-value instead, include the option --pval . These can be converted between one another using the RooFit functions RooFit::PValueToSignificance and RooFit::SignificanceToPValue . You may find it useful to resort to a brute-force fitting algorithm when calculating the significance which scans the nll (repeating fits until a tolerance is reached), bypassing MINOS, which can be activated with the option bruteForce . This can be tuned using the options setBruteForceAlgo , setBruteForceTypeAndAlgo and setBruteForceTolerance . Computing the expected significance The expected significance can be computed from an Asimov dataset of signal+background. There are two options for this a-posteriori expected: will depend on the observed dataset. a-priori expected (the default behavior): does not depend on the observed dataset, and so is a good metric for optimizing an analysis when still blinded. The a-priori expected significance from the Asimov dataset is calculated as combine -M Significance datacard.txt -t -1 --expectSignal=1 In order to produced the a-posteriori expected significance, just generate a post-fit Asimov (i.e add the option --toysFreq in the command above). The output format is the same as for observed signifiances: the variable limit in the tree will be filled with the significance (or with the p-value if you put also the option --pvalue ) Bayesian Limits and Credible regions Bayesian calculation of limits requires the user to assume a particular prior distribution for the parameter of interest (default r ). You can specify the prior using the --prior option, the default is a flat pior in r . Since the Bayesian methods are much less frequently used, the tool will not build the default prior. For running the two methods below, you should include the option --noDefaultPrior=0 . Computing the observed bayesian limit (for simple models) The BayesianSimple method computes a Bayesian limit performing classical numerical integration; very fast and accurate but only works for simple models (a few channels and nuisance parameters). combine -M BayesianSimple simple-counting-experiment.txt --noDefaultPrior=0 [...] -- BayesianSimple -- Limit: r < 0.672292 @ 95% CL Done in 0.04 min (cpu), 0.05 min (real) The output tree will contain a single entry corresponding to the observed 95% upper limit. The confidence level can be modified to 100*X% using --cl X . Computing the observed bayesian limit (for arbitrary models) The MarkovChainMC method computes a Bayesian limit performing a monte-carlo integration. From the statistics point of view it is identical to the BayesianSimple method, only the technical implementation is different. The method is slower, but can also handle complex models. For this method, you can increase the accuracy of the result by increasing the number of markov chains at the expense of a longer running time (option --tries , default is 10). Let's use the realistic counting experiment datacard to test the method To use the MarkovChainMC method, users need to specify this method in the command line, together with the options they want to use. For instance, to set the number of times the algorithm will run with different random seeds, use option --tries : combine -M MarkovChainMC realistic-counting-experiment.txt --tries 100 --noDefaultPrior=0 [...] -- MarkovChainMC -- Limit: r < 2.20438 +/- 0.0144695 @ 95% CL (100 tries) Average chain acceptance: 0.078118 Done in 0.14 min (cpu), 0.15 min (real) Again, the resulting limit tree will contain the result. You can also save the chains using the option --saveChain which will then also be included in the output file. Exclusion regions can be made from the posterior once an ordering principle is defined to decide how to grow the contour (there's infinite possible regions that contain 68% of the posterior pdf...) Below is a simple example script which can be used to plot the posterior distribution from these chains and calculate the smallest such region, import ROOT rmin = 0 rmax = 30 nbins = 100 CL = 0.95 chains = \"higgsCombineTest.MarkovChainMC.blahblahblah.root\" def findSmallestInterval(hist,CL): bins = hist.GetNbinsX() best_i = 1 best_j = 1 bd = bins+1 val = 0; for i in range(1,bins+1): integral = hist.GetBinContent(i) for j in range(i+1,bins+2): integral += hist.GetBinContent(j) if integral > CL : val = integral break if integral > CL and j-i < bd : bd = j-i best_j = j+1 best_i = i val = integral return hist.GetBinLowEdge(best_i), hist.GetBinLowEdge(best_j), val fi_MCMC = ROOT.TFile.Open(chains) # Sum up all of the chains (or we could take the average limit) mychain=0 for k in fi_MCMC.Get(\"toys\").GetListOfKeys(): obj = k.ReadObj if mychain ==0: mychain = k.ReadObj().GetAsDataSet() else : mychain.append(k.ReadObj().GetAsDataSet()) hist = ROOT.TH1F(\"h_post\",\";r;posterior probability\",nbins,rmin,rmax) for i in range(mychain.numEntries()): mychain.get(i) hist.Fill(mychain.get(i).getRealValue(\"r\"), mychain.weight()) hist.Scale(1./hist.Integral()) hist.SetLineColor(1) vl,vu,trueCL = findSmallestInterval(hist,CL) histCL = hist.Clone() for b in range(nbins): if histCL.GetBinLowEdge(b+1) < vl or histCL.GetBinLowEdge(b+2)>vu: histCL.SetBinContent(b+1,0) c6a = ROOT.TCanvas() histCL.SetFillColor(ROOT.kAzure-3) histCL.SetFillStyle(1001) hist.Draw() histCL.Draw(\"histFsame\") hist.Draw(\"histsame\") ll = ROOT.TLine(vl,0,vl,2*hist.GetBinContent(hist.FindBin(vl))); ll.SetLineColor(2); ll.SetLineWidth(2) lu = ROOT.TLine(vu,0,vu,2*hist.GetBinContent(hist.FindBin(vu))); lu.SetLineColor(2); lu.SetLineWidth(2) ll.Draw() lu.Draw() print \" %g %% (%g %%) interval (target) = %g < r < %g \"%(trueCL,CL,vl,vu) Running the script on the output file produced for the same datacard (including the --saveChain option) will produce the following output 0.950975 % (0.95 %) interval (target) = 0 < r < 2.2 along with a plot of the posterior shown below. This is the same as the output from combine but the script can also be used to find lower limits (for example) or credible intervals. An example to make contours when ordering by probability density is in bayesContours.cxx , but the implementation is very simplistic, with no clever handling of bin sizes nor any smoothing of statistical fluctuations. The MarkovChainMC algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all) Iterations, burn-in, tries Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option --tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. IThe default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous. Proposals The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter --propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. ortho ( default ): This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option --debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever) Computing the expected bayesian limit The expected limit is computed by generating many toy mc observations and compute the limit for each of them. This can be done passing the option -t . E.g. to run 100 toys with the BayesianSimple method, just do combine -M BayesianSimple datacard.txt -t 100 --noDefaultPrior=0 The program will print out the mean and median limit, and the 68% and 95% quantiles of the distributions of the limits. This time, the output root tree will contain one entry per toy . For more heavy methods (eg the MarkovChainMC ) you'll probably want to split this in multiple jobs. To do this, just run combine multiple times specifying a smaller number of toys (can be as low as 1 ) each time using a different seed to initialize the random number generator (option -s if you set it to -1, the starting seed will be initialized randomly at the beginning of the job), then merge the resulting trees with hadd and look at the distribution in the merged file. Multidimensional bayesian credible regions The MarkovChainMC method allows the user to produce the posterior pdf as a function of (in principle) any number of parameter of interest. In order to do so, you first need to create a workspace with more than one parameter, as explained in the physics models section. For example, lets use the toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV) and convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections using text2workspace with the option -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH . Now we just run one (or more) MCMC chain(s) and save them in the output tree.By default, the nuisance parameters will be marginalized (integrated) over their pdfs. You can ignore the complaints about not being able to compute an upper limit (since for more than 1D, this isn't well defined), combine -M MarkovChainMC workspace.root --tries 1 --saveChain -i 1000000 -m 125 -s seed --noDefaultPrior=0 The output of the markov chain is again a RooDataSet of weighted events distributed according to the posterior pdf (after you cut out the burn in part), so it can be used to make histograms or other distributions of the posterior pdf. See as an example bayesPosterior2D.cxx . Below is an example of the output of the macro, $ root -l higgsCombineTest.MarkovChainMC.... .L bayesPosterior2D.cxx bayesPosterior2D(\"bayes2D\",\"Posterior PDF\") Computing Limits with toys The HybridNew method is used to compute either the hybrid bayesian-frequentist limits popularly known as \"CL s of LEP or Tevatron type\" or the fully frequentist limits which are the current recommended method by the LHC Higgs Combination Group. Note that these methods can be resource intensive for complex models. It is possible to define the criterion used for setting limits using --rule CLs or --rule CLsplusb and as always the confidence level desired using --cl=X The choice of test-statistic can be made via the option --testStat and different methodologies for treatment of the nuisance parameters are available. While it is possible to mix different test-statistics with different nuisance parameter treatments, this is highly not-reccomended . Instead one should follow one of the following three procedures, LEP-style : --testStat LEP --generateNuisances=1 --fitNuisances=0 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] . The nuisance parameters are fixed to their nominal values for the purpose of evaluating the likelihood, while for generating toys, the nuisance parameters are first randomized within their pdfs before generation of the toy. TEV-style : --testStat TEV --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods $ q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta} {0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta} {r})] $, in which the nuisance parameters are profiled separately for r=0 r=0 and r r . For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditional on r), while the constraint terms are randomized for the evaluation of the likelihood. LHC-style : --LHCmode LHC-limits , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r r . The value of q_{r} q_{r} set to 0 when \\hat{r}>r \\hat{r}>r giving a one sided limit. Furthermore, the constraint r>0 r>0 is enforced in the fit. This means that if the unconstrained value of \\hat{r} \\hat{r} would be negative, the test statistic q_{r} q_{r} is evaluated as -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditionally on the value of r ), while the constraint terms are randomized in the evaluation of the likelihood. [warning] The recommended style is the LHC-style . Please note that this method is sensitive to the observation in data since the post-fit (after a fit to the data) values of the nuisance parameters (assuming different values of r ) are used when generating the toys. For completely blind limits you can first generate a pre-fit asimov toy dataset (described in the toy data generation section) and use that in place of the data. While the above shortcuts are the common variants, you can also try others. The treatment of the nuisances can be changed to the so-called \"Hybrid-Bayesian\" method which effectively integrates over the nuisance parameters. This can be achieved (with any test-statistic which is not profiled over the nuisances) by setting --generateNuisances=1 --generateExternalMeasurements=0 --fitNuisances=0 . [info] Note that (observed and toy) values of the test statistic stored in the instances of RooStats::HypoTestResult when the option --saveHybridResult has been specified, are defined without the factor 2 and therefore are twice as small as the values given by the formulas above. This factor is however included automatically by all plotting script supplied within the Combine package. Simple models For relatively simple models, the observed and expected limits can be calculated interactively. Since the LHC-style is the reccomended procedure for calculating limits using toys, we will use that in this section but the same applies to the other methods. combine realistic-counting-experiment.txt -M HybridNew --LHCmode LHC-limits Show output < < < Combine >>> >>> including systematics >>> using the Profile Likelihood test statistics modified for upper limits (Q_LHC) >>> method used is HybridNew >>> random number generator seed is 123456 Computing results starting from observation (a-posteriori) Search for upper limit to the limit r = 20 +/- 0 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.264 +/- 0.0394263 CLsplusb = 0 +/- 0 Search for lower limit to the limit Now doing proper bracketing & bisection r = 10 +/- 10 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.288 +/- 0.0405024 CLsplusb = 0 +/- 0 r = 5 +/- 5 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.152 +/- 0.0321118 CLsplusb = 0 +/- 0 r = 2.5 +/- 2.5 CLs = 0.0192308 +/- 0.0139799 CLs = 0.02008 +/- 0.0103371 CLs = 0.0271712 +/- 0.00999051 CLs = 0.0239524 +/- 0.00783634 CLs = 0.0239524 +/- 0.00783634 CLb = 0.208748 +/- 0.0181211 CLsplusb = 0.005 +/- 0.00157718 r = 2.00696 +/- 1.25 CLs = 0.0740741 +/- 0.0288829 CLs = 0.0730182 +/- 0.0200897 CLs = 0.0694474 +/- 0.0166468 CLs = 0.0640182 +/- 0.0131693 CLs = 0.0595 +/- 0.010864 CLs = 0.0650862 +/- 0.0105575 CLs = 0.0629286 +/- 0.00966301 CLs = 0.0634945 +/- 0.00914091 CLs = 0.060914 +/- 0.00852667 CLs = 0.06295 +/- 0.00830083 CLs = 0.0612758 +/- 0.00778181 CLs = 0.0608142 +/- 0.00747001 CLs = 0.0587169 +/- 0.00697039 CLs = 0.0591432 +/- 0.00678587 CLs = 0.0599683 +/- 0.00666966 CLs = 0.0574868 +/- 0.00630809 CLs = 0.0571451 +/- 0.00608177 CLs = 0.0553836 +/- 0.00585531 CLs = 0.0531612 +/- 0.0055234 CLs = 0.0516837 +/- 0.0052607 CLs = 0.0496776 +/- 0.00499783 CLs = 0.0496776 +/- 0.00499783 CLb = 0.216635 +/- 0.00801002 CLsplusb = 0.0107619 +/- 0.00100693 Trying to move the interval edges closer r = 1.00348 +/- 0 CLs = 0.191176 +/- 0.0459911 CLs = 0.191176 +/- 0.0459911 CLb = 0.272 +/- 0.0398011 CLsplusb = 0.052 +/- 0.00992935 r = 1.50522 +/- 0 CLs = 0.125 +/- 0.0444346 CLs = 0.09538 +/- 0.0248075 CLs = 0.107714 +/- 0.0226712 CLs = 0.103711 +/- 0.018789 CLs = 0.0845069 +/- 0.0142341 CLs = 0.0828468 +/- 0.0126789 CLs = 0.0879647 +/- 0.0122332 CLs = 0.0879647 +/- 0.0122332 CLb = 0.211124 +/- 0.0137494 CLsplusb = 0.0185714 +/- 0.00228201 r = 1.75609 +/- 0 CLs = 0.0703125 +/- 0.0255807 CLs = 0.0595593 +/- 0.0171995 CLs = 0.0555271 +/- 0.0137075 CLs = 0.0548727 +/- 0.0120557 CLs = 0.0527832 +/- 0.0103348 CLs = 0.0555828 +/- 0.00998248 CLs = 0.0567971 +/- 0.00923449 CLs = 0.0581822 +/- 0.00871417 CLs = 0.0588835 +/- 0.00836245 CLs = 0.0594035 +/- 0.00784761 CLs = 0.0590583 +/- 0.00752672 CLs = 0.0552067 +/- 0.00695542 CLs = 0.0560446 +/- 0.00679746 CLs = 0.0548083 +/- 0.0064351 CLs = 0.0566998 +/- 0.00627124 CLs = 0.0561576 +/- 0.00601888 CLs = 0.0551643 +/- 0.00576338 CLs = 0.0583584 +/- 0.00582854 CLs = 0.0585691 +/- 0.0057078 CLs = 0.0599114 +/- 0.00564585 CLs = 0.061987 +/- 0.00566905 CLs = 0.061836 +/- 0.00549856 CLs = 0.0616849 +/- 0.0053773 CLs = 0.0605352 +/- 0.00516844 CLs = 0.0602028 +/- 0.00502875 CLs = 0.058667 +/- 0.00486263 CLs = 0.058667 +/- 0.00486263 CLb = 0.222901 +/- 0.00727258 CLsplusb = 0.0130769 +/- 0.000996375 r = 2.25348 +/- 0 CLs = 0.0192308 +/- 0.0139799 CLs = 0.0173103 +/- 0.00886481 CLs = 0.0173103 +/- 0.00886481 CLb = 0.231076 +/- 0.0266062 CLsplusb = 0.004 +/- 0.001996 r = 2.13022 +/- 0 CLs = 0.0441176 +/- 0.0190309 CLs = 0.0557778 +/- 0.01736 CLs = 0.0496461 +/- 0.0132776 CLs = 0.0479048 +/- 0.0114407 CLs = 0.0419333 +/- 0.00925719 CLs = 0.0367934 +/- 0.0077345 CLs = 0.0339814 +/- 0.00684844 CLs = 0.03438 +/- 0.0064704 CLs = 0.0337633 +/- 0.00597315 CLs = 0.0321262 +/- 0.00551608 CLs = 0.0321262 +/- 0.00551608 CLb = 0.230342 +/- 0.0118665 CLsplusb = 0.0074 +/- 0.00121204 r = 2.06859 +/- 0 CLs = 0.0357143 +/- 0.0217521 CLs = 0.0381957 +/- 0.0152597 CLs = 0.0368622 +/- 0.0117105 CLs = 0.0415097 +/- 0.0106676 CLs = 0.0442816 +/- 0.0100457 CLs = 0.0376644 +/- 0.00847235 CLs = 0.0395133 +/- 0.0080427 CLs = 0.0377625 +/- 0.00727262 CLs = 0.0364415 +/- 0.00667827 CLs = 0.0368015 +/- 0.00628517 CLs = 0.0357251 +/- 0.00586442 CLs = 0.0341604 +/- 0.00546373 CLs = 0.0361935 +/- 0.00549648 CLs = 0.0403254 +/- 0.00565172 CLs = 0.0408613 +/- 0.00554124 CLs = 0.0416682 +/- 0.00539651 CLs = 0.0432645 +/- 0.00538062 CLs = 0.0435229 +/- 0.00516945 CLs = 0.0427647 +/- 0.00501322 CLs = 0.0414894 +/- 0.00479711 CLs = 0.0414894 +/- 0.00479711 CLb = 0.202461 +/- 0.00800632 CLsplusb = 0.0084 +/- 0.000912658 -- HybridNew, before fit -- Limit: r < 2.00696 +/- 1.25 [1.50522, 2.13022] Warning in : Could not create the Migrad minimizer. Try using the minimizer Minuit Fit to 5 points: 1.91034 +/- 0.0388334 -- Hybrid New -- Limit: r < 1.91034 +/- 0.0388334 @ 95% CL Done in 0.01 min (cpu), 4.09 min (real) Failed to delete temporary file roostats-Sprxsw.root: No such file or directory The result stored in the limit branch of the output tree will be the upper limit (and its error stored in limitErr ). The default behavior will be, as above, to search for the upper limit on r however, the values of CL s+b , CL b and CL s can be calculated for a particular value r=X by specifying the option --singlePoint=X . In this case, the value stored in the branch limit will be the value of CL s ( or CL s+b ). Expected Limits For the simple models, we can just run interactively 5 times to compute the median expected and the 68% and 95% interval boundaries. Use the HybridNew method with the same options as per the observed limit but adding a --expectedFromGrid=<quantile> where the quantile is 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The output file will contain the value of the quantile in the branch quantileExpected which can be used to separate the points. Accuracy The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N 2 more toys to improve the accuracy by a factor N, you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%). Note that if you're using the CLsplusb rule then this parameter will control the uncertainty on CL s+b not on CL s . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Note, to further improve the accuracy when searching for the upper limit, combine will also fit an exponential function to several of the points and interpolate to find the crossing. Complex models For complicated models, it is best to produce a grid of test statistic distributions at various values of the signal strength, and use it to compute the observed and expected limit and bands. This approach is good for complex models since the grid of points can be distributed across any number of jobs. In this approach we will store the distributions of the test-statistic at different values of the signal strength using the option --saveHybridResult . The distribution at a single value of r=X can be determined by combine datacard.txt -M HybridNew --LHCmode LHC-limits --singlePoint X --saveToys --saveHybridResult -T 500 --clsAcc 0 [warning] We have specified the accuracy here by including clsAcc=0 which turns off adaptive sampling and specifying the number of toys to be 500 with the -T N option. For complex models, it may be necessary to split the toys internally over a number of instances of HybridNew using the option --iterations I . The total number of toys will be the product I*N . The above can be repeated several times, in parallel, to build the distribution of the test-statistic (giving the random seed option -s -1 ). Once all of the distributions are finished, the resulting output files can be merged into one using hadd and read back to calculate the limit, specifying the merged file with --grid=merged.root . The observed limit can be obtained with combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root and similarly, the median expected and quantiles can be determined using combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root --quantileExpected <quantile> substituting <quantile> with 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The splitting of the jobs can be left to the user's preference. However, users may wish to use the combineTool for automating this as described in the section on combineTool for job submission Plotting A plot of the p-values (CL s or CL s+b ) as a function of r , which is used to find the crossing, can be produced using the option --plot=limit_scan.png . This can be useful for judging if the grid was sufficient in determining the upper limit. If we use our realistic-counting-experiment.txt datacard and generate a grid of points r\\varepsilon[1.4,2.2] r\\varepsilon[1.4,2.2] in steps of 0.1, with 5000 toys for each point, the plot of the observed CL s vs r should look like the following, You should judge in each case if the limit is accurate given the spacing of the points and the precision of CL s at each point. If it is not sufficient, simply generate more points closer to the limit and/or more toys at each point. The distributions of the test-statistic can also be plotted, at each value in the grid, using the simple python tool, python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The resulting output file will contain a canvas showing the distribution of the test statistic background only and signal+background hypothesis at each value of r . [info] If you used the TEV or LEP style test statistic (using the commands as described above), then you should include the option --doublesided , which will also take care of defining the correct integrals for CL s+b and CL b . Computing Significances with toys Computation of expected significance with toys is a two step procedure: first you need to run one or more jobs to construct the expected distribution of the test statistic. As with setting limits, there are a number of different configurations for generating toys but we will use the preferred option using, LHC-style : --LHCmode LHC-significance , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 --significance The test statistic is defined using the ratio of likelihoods q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The value of the test statistic is set to 0 when \\hat{r}<0 \\hat{r}<0 For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data assuming no signal, while the constraint terms are randomized for the evaluation of the likelihood. Observed significance To construct the distribution of the test statistic run as many times as necessary, combine -M HybridNew datacard.txt --LHCmode LHC-significance --saveToys --fullBToys --saveHybridResult -T toys -i iterations -s seed with different seeds, or using -s -1 for random seeds, then merge all those results into a single root file with hadd . The observed significance can be calculated as combine -M HybridNew datacard.txt --LHCmode LHC-significance --readHybridResult --grid=input.root [--pvalue ] where the option --pvalue will replace the result stored in the limit branch output tree to be the p-value instead of the signficance. Expected significance, assuming some signal The expected significance, assuming a signal with r=X can be calculated, by including the option --expectSignal X when generating the distribution of the test statistic and using the option --expectedFromGrid=0.5 when calculating the significance for the median. To get the \u00b11\u03c3 bands, use 0.16 and 0.84 instead of 0.5, and so on... You need a total number of background toys large enough to compute the value of the significance, but you need less signal toys (especially if you only need the median). For large significance, you can then run most of the toys without the --fullBToys option (about a factor 2 faster), and only a smaller part with that option turned on. As with calculating limits with toys, these jobs can be submitted to the grid or batch systems with the help of the combineTool as described in the section on combineTool for job submission Goodness of fit tests The GoodnessOfFit method can be used to evaluate how compatible the observed data are with the model pdf. The module can be run specifying an algorithm, and will compute a goodness of fit indicator for that algorithm and the data. The procedure is therefore to first run on the real data combine -M GoodnessOfFit datacard.txt --algo=<some-algo> and then to run on many toy mc datasets to determine the distribution of the goodness of fit indicator combine -M GoodnessOfFit datacard.txt --algo=<some-algo> -t <number-of-toys> -s <seed> When computing the goodness of fit, by default the signal strength is left floating in the fit, so that the measure is independent from the presence or absence of a signal. It is possible to instead keep it fixed to some value by passing the option --fixedSignalStrength=<value> . The following algorithms are supported: saturated : Compute a goodness-of-fit measure for binned fits based on the saturated model method, as prescribed by the StatisticsCommittee (note) . This quantity is similar to a chi-square, but can be computed for an arbitrary combination of binned channels with arbitrary constraints. KS : Compute a goodness-of-fit measure for binned fits using the Kolmogorov-Smirnov test. It is based on the highest difference between the cumulative distribution function and the empirical distribution function of any bin. AD : Compute a goodness-of-fit measure for binned fits using the Anderson-Darling test. It is based on the integral of the difference between the cumulative distribution function and the empirical distribution function over all bins. It also gives the tail ends of the distribution a higher weighting. The output tree will contain a branch called limit which contains the value of the test-statistic in each toy. You can make a histogram of this test-statistic t t and from this distribution ( f(t) f(t) ) and the single value obtained in the data ( t_{0} t_{0} ) you can calculate the p-value as $p=\\int_{t=t_{0}}^{\\mathrm{+inf}} f(t)dt $. When generating toys, the default behavior will be used. See the section on toy generation for options on how to generate/fit nuisance parameters in these tests. It is recomended to use the frequentist toys ( --toysFreq ) when running the saturated model, and the default toys for the other two tests. Further goodness of fit methods could be added on request, especially if volunteers are available to code them. The output limit tree will contain the value of the test-statistic in each toy (or the data) [warning] The above algorithms are all concerned with one-sample tests. For two-sample tests, you can follow an example CMS HIN analysis described in this Twiki Masking analysis regions in the saturated model For searches that employs a simultaneous fit across signal and control regions, it may be useful to mask one or more analysis regions either when the likelihood is maximized (fit) or when the test-statistic is computed. This can be done by using the options --setParametersForFit and --setParametersForEval , respectively. A realistic example for a binned shape analysis performed in one signal region and two control samples can be found in this directory of the Higgs-combine package Datacards-shape-analysis-multiple-regions . First of all, one needs to combine the individual datacards to build a single model and to introduce the channel-masking variables as follow: combineCards.py signal_region.txt dimuon_control_region.txt singlemuon_control_region.txt > combined_card.txt text2workspace.py combined_card.txt --channel-masks More information about the channel-masking can be found in this section Channel Masking . The saturated test-static value for a simultaneous fit across all the analysis regions can be calculated as: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_sb In this case, signal and control regions are included in both the fit and in the evaluation of the test-static, and the signal strength is freely floating. This measures the compatibility between the signal+background fit and the observed data. Moreover, it can be interesting to assess the level of compatibility between the observed data in all the regions and the background prediction obtained by only fitting the control regions (CR-only fit). This is computed as follow: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0 where the signal strength is frozen and the signal region is not considered in the fit ( --setParametersForFit mask_ch1=1 ), but it is included in the test-statistic computation ( --setParametersForEval mask_ch1=0 ). To show the differences between the two models being tested, one can perform a fit to the data using the FitDiagnostics method as: combine -M FitDiagnostics -d combined_card.root -n _fit_result --saveShapes --saveWithUncertainties combine -M FitDiagnostics -d combined_card.root -n _fit_CRonly_result --saveShapes --saveWithUncertainties --setParameters mask_ch1=1 By taking the total background, the total signal, and the data shapes from FitDiagnostics output, we can compare the post-fit predictions from the S+B fit (first case) and the CR-only fit (second case) with the observation as reported below: FitDiagnostics S+B fit FitDiagnostics CR-only fit To compute a p-value for the two results, one needs to compare the observed goodness-of-fit value previously computed with expected distribution of the test-statistic obtained in toys: combine -M GoodnessOfFit combined_card.root --algo=saturated -n result_toy_sb --toysFrequentist -t 500 combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly_toy --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0,mask_ch1=1 -t 500 --toysFrequentist where the former gives the result for the S+B model, while the latter gives the test-statistic for CR-only fit. The command --setParameters r=0,mask_ch1=1 is needed to ensure that toys are thrown using the nuisance parameters estimated from the CR-only fit to the data. The comparison between the observation and the expected distribition should look like the following two plots: Goodness-of-fit for S+B model Goodness-of-fit for CR-only model Channel Compatibility The ChannelCompatibilityCheck method can be used to evaluate how compatible are the measurements of the signal strength from the separate channels of a combination. The method performs two fits of the data, first with the nominal model in which all channels are assumed to have the same signal strength multiplier r r , and then another allowing separate signal strengths r_{i} r_{i} in each channel. A chisquare-like quantity is computed as -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(data|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(data|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) . Just like for the goodness of fit indicators, the expected distribution of this quantity under the nominal model can be computed from toy mc. By default, the signal strength is kept floating in the fit with the nominal model. It can however be fixed to a given value by passing the option --fixedSignalStrength=<value> . In the default models build from the datacards the signal strengths in all channels are constrained to be non-negative. One can allow negative signal strengths in the fits by changing the bound on the variable (option --rMin=<value> ), which should make the quantity more chisquare-like under the hypothesis of zero signal; this however can create issues in channels with small backgrounds, since total expected yields and pdfs in each channel must be positive. When run with the a verbosity of 1, as the default, the program also prints out the best fit signal strengths in all channels; as the fit to all channels is done simultaneously, the correlation between the other systematical uncertainties is taken into account, and so these results can differ from the ones obtained fitting each channel separately. Below is an example output from combine, $ combine -M ChannelCompatibilityCheck comb_hww.txt -m 160 -n HWW < < < Combine >>> >>> including systematics >>> method used to compute upper limit is ChannelCompatibilityCheck >>> random number generator seed is 123456 Sanity checks on the model: OK Computing limit starting from observation --- ChannelCompatibilityCheck --- Nominal fit : r = 0.3431 -0.1408/+0.1636 Alternate fit: r = 0.4010 -0.2173/+0.2724 in channel hww_0jsf_shape Alternate fit: r = 0.2359 -0.1854/+0.2297 in channel hww_0jof_shape Alternate fit: r = 0.7669 -0.4105/+0.5380 in channel hww_1jsf_shape Alternate fit: r = 0.3170 -0.3121/+0.3837 in channel hww_1jof_shape Alternate fit: r = 0.0000 -0.0000/+0.5129 in channel hww_2j_cut Chi2-like compatibility variable: 2.16098 Done in 0.08 min (cpu), 0.08 min (real) The output tree will contain the value of the compatibility (chisquare variable) in the limit branch. If the option --saveFitResult is specified, the output root file contains also two RooFitResult objects fit_nominal and fit_alternate with the results of the two fits. This can be read and used to extract the best fit for each channel and the overall best fit using $ root -l TFile* _file0 = TFile::Open(\"higgsCombineTest.ChannelCompatibilityCheck.mH120.root\"); fit_alternate->floatParsFinal().selectByName(\"*ChannelCompatibilityCheck*\")->Print(\"v\"); fit_nominal->floatParsFinal().selectByName(\"r\")->Print(\"v\"); The macro cccPlot.cxx can be used to produce a comparison plot of the best fit signals from all channels. Likelihood Fits and Scans The MultiDimFit method can do multi-dimensional fits and likelihood based scans/contours using models with several parameters of interest. Taking a toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV), we need to convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections text2workspace.py toy-hgg-125.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH A number of different algorithms can be used with the option --algo <algo> , none (default): Perform a maximum likelihood fit combine -M MultiDimFit toy-hgg-125.root ; The output root tree will contain two columns, one for each parameter, with the fitted values. singles : Perform a fit of each parameter separately, treating the others as unconstrained nuisances : combine -M MultiDimFit toy-hgg-125.root --algo singles --cl=0.68 . The output root tree will contain two columns, one for each parameter, with the fitted values; there will be one row with the best fit point (and quantileExpected set to -1) and two rows for each fitted parameter, where the corresponding column will contain the maximum and minimum of that parameter in the 68% CL interval, according to a one-dimensional chisquare (i.e. uncertainties on each fitted parameter do not increase when adding other parameters if they're uncorrelated). Note that if you run, for example, with --cminDefaultMinimizerStrategy=0 , these uncertainties will be derived from the Hessian, while --cminDefaultMinimizerStrategy=1 will invoke Minos to derive them. cross : Perform joint fit of all parameters: combine -M MultiDimFit toy-hgg-125.root --algo=cross --cl=0.68 . The output root tree will have one row with the best fit point, and two rows for each parameter, corresponding to the minimum and maximum of that parameter on the likelihood contour corresponding to the specified CL, according to a N-dimensional chisquare (i.e. uncertainties on each fitted parameter do increase when adding other parameters, even if they're uncorrelated). Note that the output of this way of running are not 1D uncertainties on each parameter, and shouldn't be taken as such. contour2d : Make a 68% CL contour a la minos combine -M MultiDimFit toy-hgg-125.root --algo contour2d --points=20 --cl=0.68 . The output will contain values corresponding to the best fit point (with quantileExpected set to -1) and for a set of points on the contour (with quantileExpected set to 1-CL, or something larger than that if the contour is hitting the boundary of the parameters). Probabilities are computed from the the n-dimensional \\chi^{2} \\chi^{2} distribution. For slow models, you can split it up by running several times with different number of points and merge the outputs (something better can be implemented). You can look at the contourPlot.cxx macro for how to make plots out of this algorithm. random : Scan N random points and compute the probability out of the profile likelihood combine -M MultiDimFit toy-hgg-125.root --algo random --points=20 --cl=0.68 . Again, best fit will have quantileExpected set to -1, while each random point will have quantileExpected set to the probability given by the profile likelihood at that point. fixed : Compare the log-likelihood at a fixed point compared to the best fit. combine -M MultiDimFit toy-hgg-125.root --algo fixed --fixedPointPOIs <r_fixed,MH_fixed> . The output tree will contain the difference in the negative log-likelihood between the points ( \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} ) and ( \\hat{r}_{fixed},\\hat{m}_{H,fixed} \\hat{r}_{fixed},\\hat{m}_{H,fixed} ) in the branch deltaNLL . grid : Scan on a fixed grid of points not with approximately N points in total. combine -M MultiDimFit toy-hgg-125.root --algo grid --points=10000 . You can partition the job in multiple tasks by using options --firstPoint and --lastPoint , for complicated scans, the points can be split as described in the combineTool for job submission section. The output file will contain a column deltaNLL with the difference in negative log likelihood with respect to the best fit point. Ranges/contours can be evaluated by filling TGraphs or TH2 histograms with these points. By default the \"min\" and \"max\" of the POI ranges are not included and the points which are in the scan are centered , eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 5 will scan at the points r=0.5, 1.5, 2.5, 3.5, 4.5 r=0.5, 1.5, 2.5, 3.5, 4.5 . You can instead include the option --alignEdges 1 which causes the points to be aligned with the endpoints of the parameter ranges - eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 6 --alignEdges 1 will now scan at the points r=0, 1, 2, 3, 4, 5 r=0, 1, 2, 3, 4, 5 . NB - the number of points must be increased by 1 to ensure both end points are included. With the algorithms none and singles you can save the RooFitResult from the initial fit using the option --saveFitResult . The fit result is saved into a new file called muiltidimfit.root . As usual, any floating nuisance parameters will be profiled which can be turned of using the --freezeParameters option. For most of the methods, for lower precision results you can turn off the profiling of the nuisances setting option --fastScan , which for complex models speeds up the process by several orders of magnitude. All nuisance parameters will be kept fixed at the value corresponding to the best fit point. As an example, lets produce the -2\\Delta\\ln{\\mathcal{L}} -2\\Delta\\ln{\\mathcal{L}} scan as a function of r_ggH and r_qqH from the toy H\u2192\u03b3\u03b3 datacard, with the nuisance parameters fixed to their global best fit values. combine toy-hgg-125.root -M MultiDimFit --algo grid --points 2000 --setParameterRanges r_qqH=0,10:r_ggH=0,4 -m 125 --fastScan Show output < < < Combine >>> >>> including systematics >>> method used is MultiDimFit >>> random number generator seed is 123456 ModelConfig 'ModelConfig' defines more than one parameter of interest. This is not supported in some statistical methods. Set Range of Parameter r_qqH To : (0,10) Set Range of Parameter r_ggH To : (0,4) Computing results starting from observation (a-posteriori) POI: r_ggH= 0.88152 -> [0,4] POI: r_qqH= 4.68297 -> [0,10] Point 0/2025, (i,j) = (0,0), r_ggH = 0.044444, r_qqH = 0.111111 Point 11/2025, (i,j) = (0,11), r_ggH = 0.044444, r_qqH = 2.555556 Point 22/2025, (i,j) = (0,22), r_ggH = 0.044444, r_qqH = 5.000000 Point 33/2025, (i,j) = (0,33), r_ggH = 0.044444, r_qqH = 7.444444 Point 55/2025, (i,j) = (1,10), r_ggH = 0.133333, r_qqH = 2.333333 Point 66/2025, (i,j) = (1,21), r_ggH = 0.133333, r_qqH = 4.777778 Point 77/2025, (i,j) = (1,32), r_ggH = 0.133333, r_qqH = 7.222222 Point 88/2025, (i,j) = (1,43), r_ggH = 0.133333, r_qqH = 9.666667 Point 99/2025, (i,j) = (2,9), r_ggH = 0.222222, r_qqH = 2.111111 Point 110/2025, (i,j) = (2,20), r_ggH = 0.222222, r_qqH = 4.555556 Point 121/2025, (i,j) = (2,31), r_ggH = 0.222222, r_qqH = 7.000000 Point 132/2025, (i,j) = (2,42), r_ggH = 0.222222, r_qqH = 9.444444 Point 143/2025, (i,j) = (3,8), r_ggH = 0.311111, r_qqH = 1.888889 Point 154/2025, (i,j) = (3,19), r_ggH = 0.311111, r_qqH = 4.333333 Point 165/2025, (i,j) = (3,30), r_ggH = 0.311111, r_qqH = 6.777778 Point 176/2025, (i,j) = (3,41), r_ggH = 0.311111, r_qqH = 9.222222 Point 187/2025, (i,j) = (4,7), r_ggH = 0.400000, r_qqH = 1.666667 Point 198/2025, (i,j) = (4,18), r_ggH = 0.400000, r_qqH = 4.111111 Point 209/2025, (i,j) = (4,29), r_ggH = 0.400000, r_qqH = 6.555556 Point 220/2025, (i,j) = (4,40), r_ggH = 0.400000, r_qqH = 9.000000 [...] Done in 0.00 min (cpu), 0.02 min (real) The scan, along with the best fit point can be drawn using root, $ root -l higgsCombineTest.MultiDimFit.mH125.root limit->Draw(\"2*deltaNLL:r_ggH:r_qqH>>h(44,0,10,44,0,4)\",\"2*deltaNLL<10\",\"prof colz\") limit->Draw(\"r_ggH:r_qqH\",\"quantileExpected == -1\",\"P same\") TGraph *best_fit = (TGraph*)gROOT->FindObject(\"Graph\") best_fit->SetMarkerSize(3); best_fit->SetMarkerStyle(34); best_fit->Draw(\"p same\") To make the full profiled scan just remove the --fastScan option from the combine command. Similarly, 1D scans can be drawn directly from the tree, however for 1D likelihood scans, there is a python script from the CombineHarvester/CombineTools package plot1DScan.py which can be used to make plots and extract the crossings of the 2*deltaNLL - e.g the 1\u03c3/2\u03c3 boundaries. Useful options for likelihood scans A number of common, useful options (especially for computing likelihood scans with the grid algo) are, --autoBoundsPOIs arg : Adjust bounds for the POIs if they end up close to the boundary. This can be a comma separated list of POIs, or \"*\" to get all of them. --autoMaxPOIs arg : Adjust maxima for the POIs if they end up close to the boundary. Can be a list of POIs, or \"*\" to get all. --autoRange X : Set to any X >= 0 to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X\u03c3 range, where \\hat{p} \\hat{p} and \u03c3 are the best fit parameter value and uncertainty from the initial fit (so it may be fairly approximate). In case you do not trust the estimate of the error from the initial fit, you can just centre the range on the best fit value by using the option --centeredRange X to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X range centered on the best fit value. --squareDistPoiStep : POI step size based on distance from midpoint ( either (max-min)/2 or the best fit if used with --autoRange or --centeredRange ) rather than linear separation. --skipInitialFit : Skip the initial fit (saves time if for example a snapshot is loaded from a previous fit) Below is a comparison in a likelihood scan, with 20 points, as a function of r_qqH with our toy-hgg-125.root workspace with and without some of these options. The options added tell combine to scan more points closer to the minimum (best-fit) than with the default. You may find it useful to use the --robustFit=1 option to turn on robust (brute-force) for likelihood scans (and other algorithms). You can set the strategy and tolerance when using the --robustFit option using the options --setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. If running --robustFit=1 with the algo singles , you can tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you suspect your fits/uncertainties are not stable, you may also try to run custom HESSE-style calculation of the covariance matrix. This is enabled by running MultiDimFit with the --robustHesse=1 option. A simple example of how the default behaviour in a simple datacard is given here . For a full list of options use combine -M MultiDimFit --help Fitting only some parameters If your model contains more than one parameter of interest, you can still decide to fit a smaller number of them, using the option --parameters (or -P ), with a syntax like this: combine -M MultiDimFit [...] -P poi1 -P poi2 ... --floatOtherPOIs=(0|1) If --floatOtherPOIs is set to 0, the other parameters of interest (POIs), which are not included as a -P option, are kept fixed to their nominal values. If it's set to 1, they are kept floating , which has different consequences depending on algo : When running with --algo=singles , the other floating POIs are treated as unconstrained nuisance parameters. When running with --algo=cross or --algo=contour2d , the other floating POIs are treated as other POIs, and so they increase the number of dimensions of the chi-square. As a result, when running with floatOtherPOIs set to 1, the uncertainties on each fitted parameters do not depend on what's the selection of POIs passed to MultiDimFit, but only on the number of parameters of the model. [info] Note that poi given to the the option -P can also be any nuisance parameter. However, by default, the other nuisance parameters are left floating , so you do not need to specify that. You can save the values of the other parameters of interest in the output tree by adding the option saveInactivePOI=1 . You can additionally save the post-fit values any nuisance parameter, function or discrete index (RooCategory) defined in the workspace using the following options; --saveSpecifiedNuis=arg1,arg2,... will store the fitted value of any specified constrained nuisance parameter. Use all to save every constrained nuisance parameter. Note that if you want to store the values of flatParams (or floating parameters which are not defined in the datacard) or rateParams , which are unconstrained , you should instead use the generic option --trackParameters as described here . --saveSpecifiedFunc=arg1,arg2,... will store the value of any function (eg RooFormulaVar ) in the model. --saveSpecifiedIndex=arg1,arg2,... will store the index of any RooCategory object - eg a discrete nuisance. Using best fit snapshots This can be used to save time when performing scans so that the best-fit needs not be redone and can also be used to perform scans with some nuisances frozen to the best-fit values. Sometimes it is useful to scan freezing certain nuisances to their best-fit values as opposed to the default values. To do this here is an example, Create a workspace workspace for a floating r,m_{H} r,m_{H} fit text2workspace.py hgg_datacard_mva_8TeV_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=120,130 -o testmass.root Perfom the fit, saving the workspace combine -m 123 -M MultiDimFit --saveWorkspace -n teststep1 testmass.root --verbose 9 Now we can load the best-fit \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} and fit for r r freezing m_{H} m_{H} and lumi_8TeV to the best-fit values, combine -m 123 -M MultiDimFit -d higgsCombineteststep1.MultiDimFit.mH123.root -w w --snapshotName \"MultiDimFit\" -n teststep2 --verbose 9 --freezeNuisances MH,lumi_8TeV Feldman Cousins The Feldman-Cousins (FC) procedure for computing confidence intervals for a generic model is, use the profile likelihood as the test-statistic q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) where x x is a point in the (N-dimensional) parameter space, and \\hat{x} \\hat{x} is the point corresponding to the best fit. In this test-statistic, the nuisance parameters are profiled, separately both in the numerator and denominator. for each point x x : compute the observed test statistic q_{\\mathrm{obs}}(x) q_{\\mathrm{obs}}(x) compute the expected distribution of q(x) q(x) under the hypothesis of x x as the true value. accept the point in the region if CL s+b =P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha =P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha With a critical value \\alpha \\alpha . In combine , you can perform this test on each individual point ( param1, param2,... ) = ( value1,value2,... ) by doing, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --clsAcc 0 --singlePoint param1=value1,param2=value2,param3=value3,... --saveHybridResult [Other options for toys, iterations etc as with limits] The point belongs to your confidence region if CL s+b is larger than \\alpha \\alpha (e.g. 0.3173 for a 1\u03c3 region, 1-\\alpha=0.6827 1-\\alpha=0.6827 ). [warning] You should not use this method without the option --singlePoint . Although combine will not complain, the algorithm to find the crossing will only find a single crossing and therefore not find the correct interval. Instead you should calculate the Feldman-Cousins intervals as described above. Physical boundaries Imposing physical boundaries (such as requiring r>0 r>0 ) is achieved by setting the ranges of the physics model parameters using --setParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... The boundary is imposed by restricting the parameter range(s) to those set by the user, in the fits. This can sometimes be an issue as Minuit may not know if has successfully converged when the minimum lies outside of that range. If there is no upper/lower boundary, just set that value to something far from the region of interest. [info] One can also imagine imposing the boundaries by first allowing Minuit to find the minimum in the un-restricted (and potentially unphysical) region and then setting the test-statistic to 0 in the case that minimum lies outside the physical boundary. If you are interested in implementing this version in combine, please contact the development team. As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs, as described in the combineTool for job submission section. Extracting contours As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs with hadd . You can also refer to the combineTool for job submission section for submitting the jobs to the grid/batch. 1D intervals For one-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). As with limits, read in the grid of points and extract 1D intervals using, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --readHybridResults --grid=mergedfile.root --cl <1-alpha> The output tree will contain the values of the POI which crosses the critical value ( \\alpha \\alpha ) - i.e, the boundaries of the confidence intervals, You can produce a plot of the value of CL s+b vs the parameter of interest by adding the option --plot <plotname> . 2D contours There is a tool for extracting 2D contours from the output of HybridNew located in test/makeFCcontour.py provided the option --saveHybridResult was included when running HybridNew . It can be run with the usual combine output files (or several of them) as input, ./test/makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of value of CL s+b for each point which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys in this python tool will add histograms for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored in memory.","title":"Common statistical methods"},{"location":"part3/commonstatsmethods/#common-statistical-methods","text":"In this section, the most commonly used statistical methods from combine will be covered including specific instructions on how to obtain limits, significances and likelihood scans. For all of these methods, the assumed parameters of interest (POI) is the overall signal strength r (i.e the default PhysicsModel). In general however, the first POI in the list of POIs (as defined by the PhysicsModel) will be taken instead of r which may or may not make sense for a given method ... use your judgment! This section will assume that you are using the default model unless otherwise specified.","title":"Common Statistical Methods"},{"location":"part3/commonstatsmethods/#asymptotic-frequentist-limits","text":"The AsymptoticLimits method allows to compute quickly an estimate of the observed and expected limits, which is fairly accurate when the event yields are not too small and the systematic uncertainties don't play a major role in the result. The limit calculation relies on an asymptotic approximation of the distributions of the LHC test-statistic, which is based on a profile likelihood ratio, under signal and background hypotheses to compute CL s+b , CL b and therefore CL s =CL s+b /CL b - i.e it is the asymptotic approximation of computing limits with frequentist toys. This method is so commonly used that it is the default method (i.e not specifying -M will run AsymptoticLimits ) A realistic example of datacard for a counting experiment can be found in the HiggsCombination package: data/tutorials/counting/realistic-counting-experiment.txt The method can be run using combine -M AsymptoticLimits realistic-counting-experiment.txt The program will print out the limit on the signal strength r (number of signal events / number of expected signal events) e .g. Observed Limit: r < 1.6297 @ 95% CL , the median expected limit Expected 50.0%: r < 2.3111 and edges of the 68% and 95% ranges for the expected limits. <<< Combine >>> >>> including systematics >>> method used to compute upper limit is AsymptoticLimits [...] -- AsymptoticLimits ( CLs ) -- Observed Limit: r < 1.6281 Expected 2.5%: r < 0.9640 Expected 16.0%: r < 1.4329 Expected 50.0%: r < 2.3281 Expected 84.0%: r < 3.9800 Expected 97.5%: r < 6.6194 Done in 0.01 min (cpu), 0.01 min (real) By default, the limits are calculated using the CL s prescription, as noted in the output, which takes the ratio of p-values under the signal plus background and background only hypothesis. This can be altered to using the strict p-value by using the option --rule CLsplusb You can also change the confidence level (default is 95%) to 90% using the option --cl 0.9 or any other confidence level. You can find the full list of options for AsymptoticLimits using --help -M AsymptoticLimits . [warning] You may find that combine issues a warning that the best fit for the background-only Asimov dataset returns a non-zero value for the signal strength for example; WARNING: Best fit of asimov dataset is at r = 0.220944 (0.011047 times rMax), while it should be at zero If this happens, you should check to make sure that there are no issues with the datacard or the Asimov generation used for your setup. For details on debugging it is recommended that you follow the simple checks used by the HIG PAG here . The program will also create a rootfile higgsCombineTest.AsymptoticLimits.mH120.root containing a root tree limit that contains the limit values and other bookeeping information. The important columns are limit (the limit value) and quantileExpected (-1 for observed limit, 0.5 for median expected limit, 0.16/0.84 for the edges of the 65% interval band of expected limits, 0.025/0.975 for 95%). $ root -l higgsCombineTest.AsymptoticLimits.mH120.root root [0] limit->Scan(\"*\") ************************************************************************************************************************************ * Row * limit * limitErr * mh * syst * iToy * iSeed * iChannel * t_cpu * t_real * quantileE * ************************************************************************************************************************************ * 0 * 0.9639892 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.0250000 * * 1 * 1.4329109 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.1599999 * * 2 * 2.328125 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.5 * * 3 * 3.9799661 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.8399999 * * 4 * 6.6194028 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.9750000 * * 5 * 1.6281188 * 0.0050568 * 120 * 1 * 0 * 123456 * 0 * 0.0035000 * 0.0055123 * -1 * ************************************************************************************************************************************","title":"Asymptotic Frequentist Limits"},{"location":"part3/commonstatsmethods/#blind-limits","text":"The AsymptoticLimits calculation follows the frequentist paradigm for calculating expected limits. This means that the routine will first fit the observed data, conditionally for a fixed value of r and set the nuisance parameters to the values obtained in the fit for generating the Asimov data, i.e it calculates the post-fit or a-posteriori expected limit. In order to use the pre-fit nuisance parameters (to calculate an a-priori limit), you must add the option --noFitAsimov or --bypassFrequentistFit . For blinding the results completely (i.e not using the data) you can include the option --run blind . [warning] You should never use -t -1 to get blind limits!","title":"Blind limits"},{"location":"part3/commonstatsmethods/#splitting-points","text":"In case your model is particularly complex, you can perform the asymptotic calculation by determining the value of CL s for a set grid of points (in r ) and merging the results. This is done by using the option --singlePoint X for multiple values of X, hadding the output files and reading them back in, combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.1 -n 0.1 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.2 -n 0.2 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.3 -n 0.3 ... hadd limits.root higgsCombine*.AsymptoticLimits.* combine -M AsymptoticLimits realistic-counting-experiment.txt --getLimitFromGrid limits.root","title":"Splitting points"},{"location":"part3/commonstatsmethods/#asymptotic-significances","text":"The significance of a result is calculated using a ratio of profiled likelihoods, one in which the signal strength is set to 0 and the other in which it is free to float, i.e the quantity is -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The distribution of this test-statistic can be determined using Wilke's theorem provided the number of events is large enough (i.e in the Asymptotic limit ). The significance (or p-value) can therefore be calculated very quickly and uses the Significance method. It is also possible to calculate the ratio of likelihoods between the freely floating signal strength to that of a fixed signal strength other than 0 , by specifying it with the option --signalForSignificance=X [info] This calculation assumes that the signal strength can only be positive (i.e we are not interested in negative signal strengths). This can be altered by including the option --uncapped","title":"Asymptotic Significances"},{"location":"part3/commonstatsmethods/#compute-the-observed-significance","text":"The observed significance is calculated using the Significance method, as combine -M Significance datacard.txt The printed output will report the significance and the p-value, for example, when using the realistic-counting-experiment.txt datacard, you will see <<< Combine >>> >>> including systematics >>> method used is Significance [...] -- Significance -- Significance: 0 (p-value = 0.5) Done in 0.00 min (cpu), 0.01 min (real) which is not surprising since 0 events were observed in that datacard. The output root file will contain the significance value in the branch limit . To store the p-value instead, include the option --pval . These can be converted between one another using the RooFit functions RooFit::PValueToSignificance and RooFit::SignificanceToPValue . You may find it useful to resort to a brute-force fitting algorithm when calculating the significance which scans the nll (repeating fits until a tolerance is reached), bypassing MINOS, which can be activated with the option bruteForce . This can be tuned using the options setBruteForceAlgo , setBruteForceTypeAndAlgo and setBruteForceTolerance .","title":"Compute the observed significance"},{"location":"part3/commonstatsmethods/#computing-the-expected-significance","text":"The expected significance can be computed from an Asimov dataset of signal+background. There are two options for this a-posteriori expected: will depend on the observed dataset. a-priori expected (the default behavior): does not depend on the observed dataset, and so is a good metric for optimizing an analysis when still blinded. The a-priori expected significance from the Asimov dataset is calculated as combine -M Significance datacard.txt -t -1 --expectSignal=1 In order to produced the a-posteriori expected significance, just generate a post-fit Asimov (i.e add the option --toysFreq in the command above). The output format is the same as for observed signifiances: the variable limit in the tree will be filled with the significance (or with the p-value if you put also the option --pvalue )","title":"Computing the expected significance"},{"location":"part3/commonstatsmethods/#bayesian-limits-and-credible-regions","text":"Bayesian calculation of limits requires the user to assume a particular prior distribution for the parameter of interest (default r ). You can specify the prior using the --prior option, the default is a flat pior in r . Since the Bayesian methods are much less frequently used, the tool will not build the default prior. For running the two methods below, you should include the option --noDefaultPrior=0 .","title":"Bayesian Limits and Credible regions"},{"location":"part3/commonstatsmethods/#computing-the-observed-bayesian-limit-for-simple-models","text":"The BayesianSimple method computes a Bayesian limit performing classical numerical integration; very fast and accurate but only works for simple models (a few channels and nuisance parameters). combine -M BayesianSimple simple-counting-experiment.txt --noDefaultPrior=0 [...] -- BayesianSimple -- Limit: r < 0.672292 @ 95% CL Done in 0.04 min (cpu), 0.05 min (real) The output tree will contain a single entry corresponding to the observed 95% upper limit. The confidence level can be modified to 100*X% using --cl X .","title":"Computing the observed bayesian limit (for simple models)"},{"location":"part3/commonstatsmethods/#computing-the-observed-bayesian-limit-for-arbitrary-models","text":"The MarkovChainMC method computes a Bayesian limit performing a monte-carlo integration. From the statistics point of view it is identical to the BayesianSimple method, only the technical implementation is different. The method is slower, but can also handle complex models. For this method, you can increase the accuracy of the result by increasing the number of markov chains at the expense of a longer running time (option --tries , default is 10). Let's use the realistic counting experiment datacard to test the method To use the MarkovChainMC method, users need to specify this method in the command line, together with the options they want to use. For instance, to set the number of times the algorithm will run with different random seeds, use option --tries : combine -M MarkovChainMC realistic-counting-experiment.txt --tries 100 --noDefaultPrior=0 [...] -- MarkovChainMC -- Limit: r < 2.20438 +/- 0.0144695 @ 95% CL (100 tries) Average chain acceptance: 0.078118 Done in 0.14 min (cpu), 0.15 min (real) Again, the resulting limit tree will contain the result. You can also save the chains using the option --saveChain which will then also be included in the output file. Exclusion regions can be made from the posterior once an ordering principle is defined to decide how to grow the contour (there's infinite possible regions that contain 68% of the posterior pdf...) Below is a simple example script which can be used to plot the posterior distribution from these chains and calculate the smallest such region, import ROOT rmin = 0 rmax = 30 nbins = 100 CL = 0.95 chains = \"higgsCombineTest.MarkovChainMC.blahblahblah.root\" def findSmallestInterval(hist,CL): bins = hist.GetNbinsX() best_i = 1 best_j = 1 bd = bins+1 val = 0; for i in range(1,bins+1): integral = hist.GetBinContent(i) for j in range(i+1,bins+2): integral += hist.GetBinContent(j) if integral > CL : val = integral break if integral > CL and j-i < bd : bd = j-i best_j = j+1 best_i = i val = integral return hist.GetBinLowEdge(best_i), hist.GetBinLowEdge(best_j), val fi_MCMC = ROOT.TFile.Open(chains) # Sum up all of the chains (or we could take the average limit) mychain=0 for k in fi_MCMC.Get(\"toys\").GetListOfKeys(): obj = k.ReadObj if mychain ==0: mychain = k.ReadObj().GetAsDataSet() else : mychain.append(k.ReadObj().GetAsDataSet()) hist = ROOT.TH1F(\"h_post\",\";r;posterior probability\",nbins,rmin,rmax) for i in range(mychain.numEntries()): mychain.get(i) hist.Fill(mychain.get(i).getRealValue(\"r\"), mychain.weight()) hist.Scale(1./hist.Integral()) hist.SetLineColor(1) vl,vu,trueCL = findSmallestInterval(hist,CL) histCL = hist.Clone() for b in range(nbins): if histCL.GetBinLowEdge(b+1) < vl or histCL.GetBinLowEdge(b+2)>vu: histCL.SetBinContent(b+1,0) c6a = ROOT.TCanvas() histCL.SetFillColor(ROOT.kAzure-3) histCL.SetFillStyle(1001) hist.Draw() histCL.Draw(\"histFsame\") hist.Draw(\"histsame\") ll = ROOT.TLine(vl,0,vl,2*hist.GetBinContent(hist.FindBin(vl))); ll.SetLineColor(2); ll.SetLineWidth(2) lu = ROOT.TLine(vu,0,vu,2*hist.GetBinContent(hist.FindBin(vu))); lu.SetLineColor(2); lu.SetLineWidth(2) ll.Draw() lu.Draw() print \" %g %% (%g %%) interval (target) = %g < r < %g \"%(trueCL,CL,vl,vu) Running the script on the output file produced for the same datacard (including the --saveChain option) will produce the following output 0.950975 % (0.95 %) interval (target) = 0 < r < 2.2 along with a plot of the posterior shown below. This is the same as the output from combine but the script can also be used to find lower limits (for example) or credible intervals. An example to make contours when ordering by probability density is in bayesContours.cxx , but the implementation is very simplistic, with no clever handling of bin sizes nor any smoothing of statistical fluctuations. The MarkovChainMC algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all)","title":"Computing the observed bayesian limit (for arbitrary models)"},{"location":"part3/commonstatsmethods/#iterations-burn-in-tries","text":"Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option --tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. IThe default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous.","title":"Iterations, burn-in, tries"},{"location":"part3/commonstatsmethods/#proposals","text":"The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter --propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. ortho ( default ): This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option --debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever)","title":"Proposals"},{"location":"part3/commonstatsmethods/#computing-the-expected-bayesian-limit","text":"The expected limit is computed by generating many toy mc observations and compute the limit for each of them. This can be done passing the option -t . E.g. to run 100 toys with the BayesianSimple method, just do combine -M BayesianSimple datacard.txt -t 100 --noDefaultPrior=0 The program will print out the mean and median limit, and the 68% and 95% quantiles of the distributions of the limits. This time, the output root tree will contain one entry per toy . For more heavy methods (eg the MarkovChainMC ) you'll probably want to split this in multiple jobs. To do this, just run combine multiple times specifying a smaller number of toys (can be as low as 1 ) each time using a different seed to initialize the random number generator (option -s if you set it to -1, the starting seed will be initialized randomly at the beginning of the job), then merge the resulting trees with hadd and look at the distribution in the merged file.","title":"Computing the expected bayesian limit"},{"location":"part3/commonstatsmethods/#multidimensional-bayesian-credible-regions","text":"The MarkovChainMC method allows the user to produce the posterior pdf as a function of (in principle) any number of parameter of interest. In order to do so, you first need to create a workspace with more than one parameter, as explained in the physics models section. For example, lets use the toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV) and convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections using text2workspace with the option -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH . Now we just run one (or more) MCMC chain(s) and save them in the output tree.By default, the nuisance parameters will be marginalized (integrated) over their pdfs. You can ignore the complaints about not being able to compute an upper limit (since for more than 1D, this isn't well defined), combine -M MarkovChainMC workspace.root --tries 1 --saveChain -i 1000000 -m 125 -s seed --noDefaultPrior=0 The output of the markov chain is again a RooDataSet of weighted events distributed according to the posterior pdf (after you cut out the burn in part), so it can be used to make histograms or other distributions of the posterior pdf. See as an example bayesPosterior2D.cxx . Below is an example of the output of the macro, $ root -l higgsCombineTest.MarkovChainMC.... .L bayesPosterior2D.cxx bayesPosterior2D(\"bayes2D\",\"Posterior PDF\")","title":"Multidimensional bayesian credible regions"},{"location":"part3/commonstatsmethods/#computing-limits-with-toys","text":"The HybridNew method is used to compute either the hybrid bayesian-frequentist limits popularly known as \"CL s of LEP or Tevatron type\" or the fully frequentist limits which are the current recommended method by the LHC Higgs Combination Group. Note that these methods can be resource intensive for complex models. It is possible to define the criterion used for setting limits using --rule CLs or --rule CLsplusb and as always the confidence level desired using --cl=X The choice of test-statistic can be made via the option --testStat and different methodologies for treatment of the nuisance parameters are available. While it is possible to mix different test-statistics with different nuisance parameter treatments, this is highly not-reccomended . Instead one should follow one of the following three procedures, LEP-style : --testStat LEP --generateNuisances=1 --fitNuisances=0 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] . The nuisance parameters are fixed to their nominal values for the purpose of evaluating the likelihood, while for generating toys, the nuisance parameters are first randomized within their pdfs before generation of the toy. TEV-style : --testStat TEV --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods $ q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta} {0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta} {r})] $, in which the nuisance parameters are profiled separately for r=0 r=0 and r r . For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditional on r), while the constraint terms are randomized for the evaluation of the likelihood. LHC-style : --LHCmode LHC-limits , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r r . The value of q_{r} q_{r} set to 0 when \\hat{r}>r \\hat{r}>r giving a one sided limit. Furthermore, the constraint r>0 r>0 is enforced in the fit. This means that if the unconstrained value of \\hat{r} \\hat{r} would be negative, the test statistic q_{r} q_{r} is evaluated as -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditionally on the value of r ), while the constraint terms are randomized in the evaluation of the likelihood. [warning] The recommended style is the LHC-style . Please note that this method is sensitive to the observation in data since the post-fit (after a fit to the data) values of the nuisance parameters (assuming different values of r ) are used when generating the toys. For completely blind limits you can first generate a pre-fit asimov toy dataset (described in the toy data generation section) and use that in place of the data. While the above shortcuts are the common variants, you can also try others. The treatment of the nuisances can be changed to the so-called \"Hybrid-Bayesian\" method which effectively integrates over the nuisance parameters. This can be achieved (with any test-statistic which is not profiled over the nuisances) by setting --generateNuisances=1 --generateExternalMeasurements=0 --fitNuisances=0 . [info] Note that (observed and toy) values of the test statistic stored in the instances of RooStats::HypoTestResult when the option --saveHybridResult has been specified, are defined without the factor 2 and therefore are twice as small as the values given by the formulas above. This factor is however included automatically by all plotting script supplied within the Combine package.","title":"Computing Limits with toys"},{"location":"part3/commonstatsmethods/#simple-models","text":"For relatively simple models, the observed and expected limits can be calculated interactively. Since the LHC-style is the reccomended procedure for calculating limits using toys, we will use that in this section but the same applies to the other methods. combine realistic-counting-experiment.txt -M HybridNew --LHCmode LHC-limits Show output < < < Combine >>> >>> including systematics >>> using the Profile Likelihood test statistics modified for upper limits (Q_LHC) >>> method used is HybridNew >>> random number generator seed is 123456 Computing results starting from observation (a-posteriori) Search for upper limit to the limit r = 20 +/- 0 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.264 +/- 0.0394263 CLsplusb = 0 +/- 0 Search for lower limit to the limit Now doing proper bracketing & bisection r = 10 +/- 10 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.288 +/- 0.0405024 CLsplusb = 0 +/- 0 r = 5 +/- 5 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.152 +/- 0.0321118 CLsplusb = 0 +/- 0 r = 2.5 +/- 2.5 CLs = 0.0192308 +/- 0.0139799 CLs = 0.02008 +/- 0.0103371 CLs = 0.0271712 +/- 0.00999051 CLs = 0.0239524 +/- 0.00783634 CLs = 0.0239524 +/- 0.00783634 CLb = 0.208748 +/- 0.0181211 CLsplusb = 0.005 +/- 0.00157718 r = 2.00696 +/- 1.25 CLs = 0.0740741 +/- 0.0288829 CLs = 0.0730182 +/- 0.0200897 CLs = 0.0694474 +/- 0.0166468 CLs = 0.0640182 +/- 0.0131693 CLs = 0.0595 +/- 0.010864 CLs = 0.0650862 +/- 0.0105575 CLs = 0.0629286 +/- 0.00966301 CLs = 0.0634945 +/- 0.00914091 CLs = 0.060914 +/- 0.00852667 CLs = 0.06295 +/- 0.00830083 CLs = 0.0612758 +/- 0.00778181 CLs = 0.0608142 +/- 0.00747001 CLs = 0.0587169 +/- 0.00697039 CLs = 0.0591432 +/- 0.00678587 CLs = 0.0599683 +/- 0.00666966 CLs = 0.0574868 +/- 0.00630809 CLs = 0.0571451 +/- 0.00608177 CLs = 0.0553836 +/- 0.00585531 CLs = 0.0531612 +/- 0.0055234 CLs = 0.0516837 +/- 0.0052607 CLs = 0.0496776 +/- 0.00499783 CLs = 0.0496776 +/- 0.00499783 CLb = 0.216635 +/- 0.00801002 CLsplusb = 0.0107619 +/- 0.00100693 Trying to move the interval edges closer r = 1.00348 +/- 0 CLs = 0.191176 +/- 0.0459911 CLs = 0.191176 +/- 0.0459911 CLb = 0.272 +/- 0.0398011 CLsplusb = 0.052 +/- 0.00992935 r = 1.50522 +/- 0 CLs = 0.125 +/- 0.0444346 CLs = 0.09538 +/- 0.0248075 CLs = 0.107714 +/- 0.0226712 CLs = 0.103711 +/- 0.018789 CLs = 0.0845069 +/- 0.0142341 CLs = 0.0828468 +/- 0.0126789 CLs = 0.0879647 +/- 0.0122332 CLs = 0.0879647 +/- 0.0122332 CLb = 0.211124 +/- 0.0137494 CLsplusb = 0.0185714 +/- 0.00228201 r = 1.75609 +/- 0 CLs = 0.0703125 +/- 0.0255807 CLs = 0.0595593 +/- 0.0171995 CLs = 0.0555271 +/- 0.0137075 CLs = 0.0548727 +/- 0.0120557 CLs = 0.0527832 +/- 0.0103348 CLs = 0.0555828 +/- 0.00998248 CLs = 0.0567971 +/- 0.00923449 CLs = 0.0581822 +/- 0.00871417 CLs = 0.0588835 +/- 0.00836245 CLs = 0.0594035 +/- 0.00784761 CLs = 0.0590583 +/- 0.00752672 CLs = 0.0552067 +/- 0.00695542 CLs = 0.0560446 +/- 0.00679746 CLs = 0.0548083 +/- 0.0064351 CLs = 0.0566998 +/- 0.00627124 CLs = 0.0561576 +/- 0.00601888 CLs = 0.0551643 +/- 0.00576338 CLs = 0.0583584 +/- 0.00582854 CLs = 0.0585691 +/- 0.0057078 CLs = 0.0599114 +/- 0.00564585 CLs = 0.061987 +/- 0.00566905 CLs = 0.061836 +/- 0.00549856 CLs = 0.0616849 +/- 0.0053773 CLs = 0.0605352 +/- 0.00516844 CLs = 0.0602028 +/- 0.00502875 CLs = 0.058667 +/- 0.00486263 CLs = 0.058667 +/- 0.00486263 CLb = 0.222901 +/- 0.00727258 CLsplusb = 0.0130769 +/- 0.000996375 r = 2.25348 +/- 0 CLs = 0.0192308 +/- 0.0139799 CLs = 0.0173103 +/- 0.00886481 CLs = 0.0173103 +/- 0.00886481 CLb = 0.231076 +/- 0.0266062 CLsplusb = 0.004 +/- 0.001996 r = 2.13022 +/- 0 CLs = 0.0441176 +/- 0.0190309 CLs = 0.0557778 +/- 0.01736 CLs = 0.0496461 +/- 0.0132776 CLs = 0.0479048 +/- 0.0114407 CLs = 0.0419333 +/- 0.00925719 CLs = 0.0367934 +/- 0.0077345 CLs = 0.0339814 +/- 0.00684844 CLs = 0.03438 +/- 0.0064704 CLs = 0.0337633 +/- 0.00597315 CLs = 0.0321262 +/- 0.00551608 CLs = 0.0321262 +/- 0.00551608 CLb = 0.230342 +/- 0.0118665 CLsplusb = 0.0074 +/- 0.00121204 r = 2.06859 +/- 0 CLs = 0.0357143 +/- 0.0217521 CLs = 0.0381957 +/- 0.0152597 CLs = 0.0368622 +/- 0.0117105 CLs = 0.0415097 +/- 0.0106676 CLs = 0.0442816 +/- 0.0100457 CLs = 0.0376644 +/- 0.00847235 CLs = 0.0395133 +/- 0.0080427 CLs = 0.0377625 +/- 0.00727262 CLs = 0.0364415 +/- 0.00667827 CLs = 0.0368015 +/- 0.00628517 CLs = 0.0357251 +/- 0.00586442 CLs = 0.0341604 +/- 0.00546373 CLs = 0.0361935 +/- 0.00549648 CLs = 0.0403254 +/- 0.00565172 CLs = 0.0408613 +/- 0.00554124 CLs = 0.0416682 +/- 0.00539651 CLs = 0.0432645 +/- 0.00538062 CLs = 0.0435229 +/- 0.00516945 CLs = 0.0427647 +/- 0.00501322 CLs = 0.0414894 +/- 0.00479711 CLs = 0.0414894 +/- 0.00479711 CLb = 0.202461 +/- 0.00800632 CLsplusb = 0.0084 +/- 0.000912658 -- HybridNew, before fit -- Limit: r < 2.00696 +/- 1.25 [1.50522, 2.13022] Warning in : Could not create the Migrad minimizer. Try using the minimizer Minuit Fit to 5 points: 1.91034 +/- 0.0388334 -- Hybrid New -- Limit: r < 1.91034 +/- 0.0388334 @ 95% CL Done in 0.01 min (cpu), 4.09 min (real) Failed to delete temporary file roostats-Sprxsw.root: No such file or directory The result stored in the limit branch of the output tree will be the upper limit (and its error stored in limitErr ). The default behavior will be, as above, to search for the upper limit on r however, the values of CL s+b , CL b and CL s can be calculated for a particular value r=X by specifying the option --singlePoint=X . In this case, the value stored in the branch limit will be the value of CL s ( or CL s+b ).","title":"Simple models"},{"location":"part3/commonstatsmethods/#expected-limits","text":"For the simple models, we can just run interactively 5 times to compute the median expected and the 68% and 95% interval boundaries. Use the HybridNew method with the same options as per the observed limit but adding a --expectedFromGrid=<quantile> where the quantile is 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The output file will contain the value of the quantile in the branch quantileExpected which can be used to separate the points.","title":"Expected Limits"},{"location":"part3/commonstatsmethods/#accuracy","text":"The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N 2 more toys to improve the accuracy by a factor N, you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%). Note that if you're using the CLsplusb rule then this parameter will control the uncertainty on CL s+b not on CL s . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Note, to further improve the accuracy when searching for the upper limit, combine will also fit an exponential function to several of the points and interpolate to find the crossing.","title":"Accuracy"},{"location":"part3/commonstatsmethods/#complex-models","text":"For complicated models, it is best to produce a grid of test statistic distributions at various values of the signal strength, and use it to compute the observed and expected limit and bands. This approach is good for complex models since the grid of points can be distributed across any number of jobs. In this approach we will store the distributions of the test-statistic at different values of the signal strength using the option --saveHybridResult . The distribution at a single value of r=X can be determined by combine datacard.txt -M HybridNew --LHCmode LHC-limits --singlePoint X --saveToys --saveHybridResult -T 500 --clsAcc 0 [warning] We have specified the accuracy here by including clsAcc=0 which turns off adaptive sampling and specifying the number of toys to be 500 with the -T N option. For complex models, it may be necessary to split the toys internally over a number of instances of HybridNew using the option --iterations I . The total number of toys will be the product I*N . The above can be repeated several times, in parallel, to build the distribution of the test-statistic (giving the random seed option -s -1 ). Once all of the distributions are finished, the resulting output files can be merged into one using hadd and read back to calculate the limit, specifying the merged file with --grid=merged.root . The observed limit can be obtained with combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root and similarly, the median expected and quantiles can be determined using combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root --quantileExpected <quantile> substituting <quantile> with 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The splitting of the jobs can be left to the user's preference. However, users may wish to use the combineTool for automating this as described in the section on combineTool for job submission","title":"Complex models"},{"location":"part3/commonstatsmethods/#plotting","text":"A plot of the p-values (CL s or CL s+b ) as a function of r , which is used to find the crossing, can be produced using the option --plot=limit_scan.png . This can be useful for judging if the grid was sufficient in determining the upper limit. If we use our realistic-counting-experiment.txt datacard and generate a grid of points r\\varepsilon[1.4,2.2] r\\varepsilon[1.4,2.2] in steps of 0.1, with 5000 toys for each point, the plot of the observed CL s vs r should look like the following, You should judge in each case if the limit is accurate given the spacing of the points and the precision of CL s at each point. If it is not sufficient, simply generate more points closer to the limit and/or more toys at each point. The distributions of the test-statistic can also be plotted, at each value in the grid, using the simple python tool, python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The resulting output file will contain a canvas showing the distribution of the test statistic background only and signal+background hypothesis at each value of r . [info] If you used the TEV or LEP style test statistic (using the commands as described above), then you should include the option --doublesided , which will also take care of defining the correct integrals for CL s+b and CL b .","title":"Plotting"},{"location":"part3/commonstatsmethods/#computing-significances-with-toys","text":"Computation of expected significance with toys is a two step procedure: first you need to run one or more jobs to construct the expected distribution of the test statistic. As with setting limits, there are a number of different configurations for generating toys but we will use the preferred option using, LHC-style : --LHCmode LHC-significance , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 --significance The test statistic is defined using the ratio of likelihoods q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The value of the test statistic is set to 0 when \\hat{r}<0 \\hat{r}<0 For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data assuming no signal, while the constraint terms are randomized for the evaluation of the likelihood.","title":"Computing Significances with toys"},{"location":"part3/commonstatsmethods/#observed-significance","text":"To construct the distribution of the test statistic run as many times as necessary, combine -M HybridNew datacard.txt --LHCmode LHC-significance --saveToys --fullBToys --saveHybridResult -T toys -i iterations -s seed with different seeds, or using -s -1 for random seeds, then merge all those results into a single root file with hadd . The observed significance can be calculated as combine -M HybridNew datacard.txt --LHCmode LHC-significance --readHybridResult --grid=input.root [--pvalue ] where the option --pvalue will replace the result stored in the limit branch output tree to be the p-value instead of the signficance.","title":"Observed significance"},{"location":"part3/commonstatsmethods/#expected-significance-assuming-some-signal","text":"The expected significance, assuming a signal with r=X can be calculated, by including the option --expectSignal X when generating the distribution of the test statistic and using the option --expectedFromGrid=0.5 when calculating the significance for the median. To get the \u00b11\u03c3 bands, use 0.16 and 0.84 instead of 0.5, and so on... You need a total number of background toys large enough to compute the value of the significance, but you need less signal toys (especially if you only need the median). For large significance, you can then run most of the toys without the --fullBToys option (about a factor 2 faster), and only a smaller part with that option turned on. As with calculating limits with toys, these jobs can be submitted to the grid or batch systems with the help of the combineTool as described in the section on combineTool for job submission","title":"Expected significance, assuming some signal"},{"location":"part3/commonstatsmethods/#goodness-of-fit-tests","text":"The GoodnessOfFit method can be used to evaluate how compatible the observed data are with the model pdf. The module can be run specifying an algorithm, and will compute a goodness of fit indicator for that algorithm and the data. The procedure is therefore to first run on the real data combine -M GoodnessOfFit datacard.txt --algo=<some-algo> and then to run on many toy mc datasets to determine the distribution of the goodness of fit indicator combine -M GoodnessOfFit datacard.txt --algo=<some-algo> -t <number-of-toys> -s <seed> When computing the goodness of fit, by default the signal strength is left floating in the fit, so that the measure is independent from the presence or absence of a signal. It is possible to instead keep it fixed to some value by passing the option --fixedSignalStrength=<value> . The following algorithms are supported: saturated : Compute a goodness-of-fit measure for binned fits based on the saturated model method, as prescribed by the StatisticsCommittee (note) . This quantity is similar to a chi-square, but can be computed for an arbitrary combination of binned channels with arbitrary constraints. KS : Compute a goodness-of-fit measure for binned fits using the Kolmogorov-Smirnov test. It is based on the highest difference between the cumulative distribution function and the empirical distribution function of any bin. AD : Compute a goodness-of-fit measure for binned fits using the Anderson-Darling test. It is based on the integral of the difference between the cumulative distribution function and the empirical distribution function over all bins. It also gives the tail ends of the distribution a higher weighting. The output tree will contain a branch called limit which contains the value of the test-statistic in each toy. You can make a histogram of this test-statistic t t and from this distribution ( f(t) f(t) ) and the single value obtained in the data ( t_{0} t_{0} ) you can calculate the p-value as $p=\\int_{t=t_{0}}^{\\mathrm{+inf}} f(t)dt $. When generating toys, the default behavior will be used. See the section on toy generation for options on how to generate/fit nuisance parameters in these tests. It is recomended to use the frequentist toys ( --toysFreq ) when running the saturated model, and the default toys for the other two tests. Further goodness of fit methods could be added on request, especially if volunteers are available to code them. The output limit tree will contain the value of the test-statistic in each toy (or the data) [warning] The above algorithms are all concerned with one-sample tests. For two-sample tests, you can follow an example CMS HIN analysis described in this Twiki","title":"Goodness of fit tests"},{"location":"part3/commonstatsmethods/#masking-analysis-regions-in-the-saturated-model","text":"For searches that employs a simultaneous fit across signal and control regions, it may be useful to mask one or more analysis regions either when the likelihood is maximized (fit) or when the test-statistic is computed. This can be done by using the options --setParametersForFit and --setParametersForEval , respectively. A realistic example for a binned shape analysis performed in one signal region and two control samples can be found in this directory of the Higgs-combine package Datacards-shape-analysis-multiple-regions . First of all, one needs to combine the individual datacards to build a single model and to introduce the channel-masking variables as follow: combineCards.py signal_region.txt dimuon_control_region.txt singlemuon_control_region.txt > combined_card.txt text2workspace.py combined_card.txt --channel-masks More information about the channel-masking can be found in this section Channel Masking . The saturated test-static value for a simultaneous fit across all the analysis regions can be calculated as: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_sb In this case, signal and control regions are included in both the fit and in the evaluation of the test-static, and the signal strength is freely floating. This measures the compatibility between the signal+background fit and the observed data. Moreover, it can be interesting to assess the level of compatibility between the observed data in all the regions and the background prediction obtained by only fitting the control regions (CR-only fit). This is computed as follow: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0 where the signal strength is frozen and the signal region is not considered in the fit ( --setParametersForFit mask_ch1=1 ), but it is included in the test-statistic computation ( --setParametersForEval mask_ch1=0 ). To show the differences between the two models being tested, one can perform a fit to the data using the FitDiagnostics method as: combine -M FitDiagnostics -d combined_card.root -n _fit_result --saveShapes --saveWithUncertainties combine -M FitDiagnostics -d combined_card.root -n _fit_CRonly_result --saveShapes --saveWithUncertainties --setParameters mask_ch1=1 By taking the total background, the total signal, and the data shapes from FitDiagnostics output, we can compare the post-fit predictions from the S+B fit (first case) and the CR-only fit (second case) with the observation as reported below: FitDiagnostics S+B fit FitDiagnostics CR-only fit To compute a p-value for the two results, one needs to compare the observed goodness-of-fit value previously computed with expected distribution of the test-statistic obtained in toys: combine -M GoodnessOfFit combined_card.root --algo=saturated -n result_toy_sb --toysFrequentist -t 500 combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly_toy --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0,mask_ch1=1 -t 500 --toysFrequentist where the former gives the result for the S+B model, while the latter gives the test-statistic for CR-only fit. The command --setParameters r=0,mask_ch1=1 is needed to ensure that toys are thrown using the nuisance parameters estimated from the CR-only fit to the data. The comparison between the observation and the expected distribition should look like the following two plots: Goodness-of-fit for S+B model Goodness-of-fit for CR-only model","title":"Masking analysis regions in the saturated model"},{"location":"part3/commonstatsmethods/#channel-compatibility","text":"The ChannelCompatibilityCheck method can be used to evaluate how compatible are the measurements of the signal strength from the separate channels of a combination. The method performs two fits of the data, first with the nominal model in which all channels are assumed to have the same signal strength multiplier r r , and then another allowing separate signal strengths r_{i} r_{i} in each channel. A chisquare-like quantity is computed as -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(data|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(data|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) . Just like for the goodness of fit indicators, the expected distribution of this quantity under the nominal model can be computed from toy mc. By default, the signal strength is kept floating in the fit with the nominal model. It can however be fixed to a given value by passing the option --fixedSignalStrength=<value> . In the default models build from the datacards the signal strengths in all channels are constrained to be non-negative. One can allow negative signal strengths in the fits by changing the bound on the variable (option --rMin=<value> ), which should make the quantity more chisquare-like under the hypothesis of zero signal; this however can create issues in channels with small backgrounds, since total expected yields and pdfs in each channel must be positive. When run with the a verbosity of 1, as the default, the program also prints out the best fit signal strengths in all channels; as the fit to all channels is done simultaneously, the correlation between the other systematical uncertainties is taken into account, and so these results can differ from the ones obtained fitting each channel separately. Below is an example output from combine, $ combine -M ChannelCompatibilityCheck comb_hww.txt -m 160 -n HWW < < < Combine >>> >>> including systematics >>> method used to compute upper limit is ChannelCompatibilityCheck >>> random number generator seed is 123456 Sanity checks on the model: OK Computing limit starting from observation --- ChannelCompatibilityCheck --- Nominal fit : r = 0.3431 -0.1408/+0.1636 Alternate fit: r = 0.4010 -0.2173/+0.2724 in channel hww_0jsf_shape Alternate fit: r = 0.2359 -0.1854/+0.2297 in channel hww_0jof_shape Alternate fit: r = 0.7669 -0.4105/+0.5380 in channel hww_1jsf_shape Alternate fit: r = 0.3170 -0.3121/+0.3837 in channel hww_1jof_shape Alternate fit: r = 0.0000 -0.0000/+0.5129 in channel hww_2j_cut Chi2-like compatibility variable: 2.16098 Done in 0.08 min (cpu), 0.08 min (real) The output tree will contain the value of the compatibility (chisquare variable) in the limit branch. If the option --saveFitResult is specified, the output root file contains also two RooFitResult objects fit_nominal and fit_alternate with the results of the two fits. This can be read and used to extract the best fit for each channel and the overall best fit using $ root -l TFile* _file0 = TFile::Open(\"higgsCombineTest.ChannelCompatibilityCheck.mH120.root\"); fit_alternate->floatParsFinal().selectByName(\"*ChannelCompatibilityCheck*\")->Print(\"v\"); fit_nominal->floatParsFinal().selectByName(\"r\")->Print(\"v\"); The macro cccPlot.cxx can be used to produce a comparison plot of the best fit signals from all channels.","title":"Channel Compatibility"},{"location":"part3/commonstatsmethods/#likelihood-fits-and-scans","text":"The MultiDimFit method can do multi-dimensional fits and likelihood based scans/contours using models with several parameters of interest. Taking a toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV), we need to convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections text2workspace.py toy-hgg-125.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH A number of different algorithms can be used with the option --algo <algo> , none (default): Perform a maximum likelihood fit combine -M MultiDimFit toy-hgg-125.root ; The output root tree will contain two columns, one for each parameter, with the fitted values. singles : Perform a fit of each parameter separately, treating the others as unconstrained nuisances : combine -M MultiDimFit toy-hgg-125.root --algo singles --cl=0.68 . The output root tree will contain two columns, one for each parameter, with the fitted values; there will be one row with the best fit point (and quantileExpected set to -1) and two rows for each fitted parameter, where the corresponding column will contain the maximum and minimum of that parameter in the 68% CL interval, according to a one-dimensional chisquare (i.e. uncertainties on each fitted parameter do not increase when adding other parameters if they're uncorrelated). Note that if you run, for example, with --cminDefaultMinimizerStrategy=0 , these uncertainties will be derived from the Hessian, while --cminDefaultMinimizerStrategy=1 will invoke Minos to derive them. cross : Perform joint fit of all parameters: combine -M MultiDimFit toy-hgg-125.root --algo=cross --cl=0.68 . The output root tree will have one row with the best fit point, and two rows for each parameter, corresponding to the minimum and maximum of that parameter on the likelihood contour corresponding to the specified CL, according to a N-dimensional chisquare (i.e. uncertainties on each fitted parameter do increase when adding other parameters, even if they're uncorrelated). Note that the output of this way of running are not 1D uncertainties on each parameter, and shouldn't be taken as such. contour2d : Make a 68% CL contour a la minos combine -M MultiDimFit toy-hgg-125.root --algo contour2d --points=20 --cl=0.68 . The output will contain values corresponding to the best fit point (with quantileExpected set to -1) and for a set of points on the contour (with quantileExpected set to 1-CL, or something larger than that if the contour is hitting the boundary of the parameters). Probabilities are computed from the the n-dimensional \\chi^{2} \\chi^{2} distribution. For slow models, you can split it up by running several times with different number of points and merge the outputs (something better can be implemented). You can look at the contourPlot.cxx macro for how to make plots out of this algorithm. random : Scan N random points and compute the probability out of the profile likelihood combine -M MultiDimFit toy-hgg-125.root --algo random --points=20 --cl=0.68 . Again, best fit will have quantileExpected set to -1, while each random point will have quantileExpected set to the probability given by the profile likelihood at that point. fixed : Compare the log-likelihood at a fixed point compared to the best fit. combine -M MultiDimFit toy-hgg-125.root --algo fixed --fixedPointPOIs <r_fixed,MH_fixed> . The output tree will contain the difference in the negative log-likelihood between the points ( \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} ) and ( \\hat{r}_{fixed},\\hat{m}_{H,fixed} \\hat{r}_{fixed},\\hat{m}_{H,fixed} ) in the branch deltaNLL . grid : Scan on a fixed grid of points not with approximately N points in total. combine -M MultiDimFit toy-hgg-125.root --algo grid --points=10000 . You can partition the job in multiple tasks by using options --firstPoint and --lastPoint , for complicated scans, the points can be split as described in the combineTool for job submission section. The output file will contain a column deltaNLL with the difference in negative log likelihood with respect to the best fit point. Ranges/contours can be evaluated by filling TGraphs or TH2 histograms with these points. By default the \"min\" and \"max\" of the POI ranges are not included and the points which are in the scan are centered , eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 5 will scan at the points r=0.5, 1.5, 2.5, 3.5, 4.5 r=0.5, 1.5, 2.5, 3.5, 4.5 . You can instead include the option --alignEdges 1 which causes the points to be aligned with the endpoints of the parameter ranges - eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 6 --alignEdges 1 will now scan at the points r=0, 1, 2, 3, 4, 5 r=0, 1, 2, 3, 4, 5 . NB - the number of points must be increased by 1 to ensure both end points are included. With the algorithms none and singles you can save the RooFitResult from the initial fit using the option --saveFitResult . The fit result is saved into a new file called muiltidimfit.root . As usual, any floating nuisance parameters will be profiled which can be turned of using the --freezeParameters option. For most of the methods, for lower precision results you can turn off the profiling of the nuisances setting option --fastScan , which for complex models speeds up the process by several orders of magnitude. All nuisance parameters will be kept fixed at the value corresponding to the best fit point. As an example, lets produce the -2\\Delta\\ln{\\mathcal{L}} -2\\Delta\\ln{\\mathcal{L}} scan as a function of r_ggH and r_qqH from the toy H\u2192\u03b3\u03b3 datacard, with the nuisance parameters fixed to their global best fit values. combine toy-hgg-125.root -M MultiDimFit --algo grid --points 2000 --setParameterRanges r_qqH=0,10:r_ggH=0,4 -m 125 --fastScan Show output < < < Combine >>> >>> including systematics >>> method used is MultiDimFit >>> random number generator seed is 123456 ModelConfig 'ModelConfig' defines more than one parameter of interest. This is not supported in some statistical methods. Set Range of Parameter r_qqH To : (0,10) Set Range of Parameter r_ggH To : (0,4) Computing results starting from observation (a-posteriori) POI: r_ggH= 0.88152 -> [0,4] POI: r_qqH= 4.68297 -> [0,10] Point 0/2025, (i,j) = (0,0), r_ggH = 0.044444, r_qqH = 0.111111 Point 11/2025, (i,j) = (0,11), r_ggH = 0.044444, r_qqH = 2.555556 Point 22/2025, (i,j) = (0,22), r_ggH = 0.044444, r_qqH = 5.000000 Point 33/2025, (i,j) = (0,33), r_ggH = 0.044444, r_qqH = 7.444444 Point 55/2025, (i,j) = (1,10), r_ggH = 0.133333, r_qqH = 2.333333 Point 66/2025, (i,j) = (1,21), r_ggH = 0.133333, r_qqH = 4.777778 Point 77/2025, (i,j) = (1,32), r_ggH = 0.133333, r_qqH = 7.222222 Point 88/2025, (i,j) = (1,43), r_ggH = 0.133333, r_qqH = 9.666667 Point 99/2025, (i,j) = (2,9), r_ggH = 0.222222, r_qqH = 2.111111 Point 110/2025, (i,j) = (2,20), r_ggH = 0.222222, r_qqH = 4.555556 Point 121/2025, (i,j) = (2,31), r_ggH = 0.222222, r_qqH = 7.000000 Point 132/2025, (i,j) = (2,42), r_ggH = 0.222222, r_qqH = 9.444444 Point 143/2025, (i,j) = (3,8), r_ggH = 0.311111, r_qqH = 1.888889 Point 154/2025, (i,j) = (3,19), r_ggH = 0.311111, r_qqH = 4.333333 Point 165/2025, (i,j) = (3,30), r_ggH = 0.311111, r_qqH = 6.777778 Point 176/2025, (i,j) = (3,41), r_ggH = 0.311111, r_qqH = 9.222222 Point 187/2025, (i,j) = (4,7), r_ggH = 0.400000, r_qqH = 1.666667 Point 198/2025, (i,j) = (4,18), r_ggH = 0.400000, r_qqH = 4.111111 Point 209/2025, (i,j) = (4,29), r_ggH = 0.400000, r_qqH = 6.555556 Point 220/2025, (i,j) = (4,40), r_ggH = 0.400000, r_qqH = 9.000000 [...] Done in 0.00 min (cpu), 0.02 min (real) The scan, along with the best fit point can be drawn using root, $ root -l higgsCombineTest.MultiDimFit.mH125.root limit->Draw(\"2*deltaNLL:r_ggH:r_qqH>>h(44,0,10,44,0,4)\",\"2*deltaNLL<10\",\"prof colz\") limit->Draw(\"r_ggH:r_qqH\",\"quantileExpected == -1\",\"P same\") TGraph *best_fit = (TGraph*)gROOT->FindObject(\"Graph\") best_fit->SetMarkerSize(3); best_fit->SetMarkerStyle(34); best_fit->Draw(\"p same\") To make the full profiled scan just remove the --fastScan option from the combine command. Similarly, 1D scans can be drawn directly from the tree, however for 1D likelihood scans, there is a python script from the CombineHarvester/CombineTools package plot1DScan.py which can be used to make plots and extract the crossings of the 2*deltaNLL - e.g the 1\u03c3/2\u03c3 boundaries.","title":"Likelihood Fits and Scans"},{"location":"part3/commonstatsmethods/#useful-options-for-likelihood-scans","text":"A number of common, useful options (especially for computing likelihood scans with the grid algo) are, --autoBoundsPOIs arg : Adjust bounds for the POIs if they end up close to the boundary. This can be a comma separated list of POIs, or \"*\" to get all of them. --autoMaxPOIs arg : Adjust maxima for the POIs if they end up close to the boundary. Can be a list of POIs, or \"*\" to get all. --autoRange X : Set to any X >= 0 to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X\u03c3 range, where \\hat{p} \\hat{p} and \u03c3 are the best fit parameter value and uncertainty from the initial fit (so it may be fairly approximate). In case you do not trust the estimate of the error from the initial fit, you can just centre the range on the best fit value by using the option --centeredRange X to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X range centered on the best fit value. --squareDistPoiStep : POI step size based on distance from midpoint ( either (max-min)/2 or the best fit if used with --autoRange or --centeredRange ) rather than linear separation. --skipInitialFit : Skip the initial fit (saves time if for example a snapshot is loaded from a previous fit) Below is a comparison in a likelihood scan, with 20 points, as a function of r_qqH with our toy-hgg-125.root workspace with and without some of these options. The options added tell combine to scan more points closer to the minimum (best-fit) than with the default. You may find it useful to use the --robustFit=1 option to turn on robust (brute-force) for likelihood scans (and other algorithms). You can set the strategy and tolerance when using the --robustFit option using the options --setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. If running --robustFit=1 with the algo singles , you can tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you suspect your fits/uncertainties are not stable, you may also try to run custom HESSE-style calculation of the covariance matrix. This is enabled by running MultiDimFit with the --robustHesse=1 option. A simple example of how the default behaviour in a simple datacard is given here . For a full list of options use combine -M MultiDimFit --help","title":"Useful options for likelihood scans"},{"location":"part3/commonstatsmethods/#fitting-only-some-parameters","text":"If your model contains more than one parameter of interest, you can still decide to fit a smaller number of them, using the option --parameters (or -P ), with a syntax like this: combine -M MultiDimFit [...] -P poi1 -P poi2 ... --floatOtherPOIs=(0|1) If --floatOtherPOIs is set to 0, the other parameters of interest (POIs), which are not included as a -P option, are kept fixed to their nominal values. If it's set to 1, they are kept floating , which has different consequences depending on algo : When running with --algo=singles , the other floating POIs are treated as unconstrained nuisance parameters. When running with --algo=cross or --algo=contour2d , the other floating POIs are treated as other POIs, and so they increase the number of dimensions of the chi-square. As a result, when running with floatOtherPOIs set to 1, the uncertainties on each fitted parameters do not depend on what's the selection of POIs passed to MultiDimFit, but only on the number of parameters of the model. [info] Note that poi given to the the option -P can also be any nuisance parameter. However, by default, the other nuisance parameters are left floating , so you do not need to specify that. You can save the values of the other parameters of interest in the output tree by adding the option saveInactivePOI=1 . You can additionally save the post-fit values any nuisance parameter, function or discrete index (RooCategory) defined in the workspace using the following options; --saveSpecifiedNuis=arg1,arg2,... will store the fitted value of any specified constrained nuisance parameter. Use all to save every constrained nuisance parameter. Note that if you want to store the values of flatParams (or floating parameters which are not defined in the datacard) or rateParams , which are unconstrained , you should instead use the generic option --trackParameters as described here . --saveSpecifiedFunc=arg1,arg2,... will store the value of any function (eg RooFormulaVar ) in the model. --saveSpecifiedIndex=arg1,arg2,... will store the index of any RooCategory object - eg a discrete nuisance.","title":"Fitting only some parameters"},{"location":"part3/commonstatsmethods/#using-best-fit-snapshots","text":"This can be used to save time when performing scans so that the best-fit needs not be redone and can also be used to perform scans with some nuisances frozen to the best-fit values. Sometimes it is useful to scan freezing certain nuisances to their best-fit values as opposed to the default values. To do this here is an example, Create a workspace workspace for a floating r,m_{H} r,m_{H} fit text2workspace.py hgg_datacard_mva_8TeV_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=120,130 -o testmass.root Perfom the fit, saving the workspace combine -m 123 -M MultiDimFit --saveWorkspace -n teststep1 testmass.root --verbose 9 Now we can load the best-fit \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} and fit for r r freezing m_{H} m_{H} and lumi_8TeV to the best-fit values, combine -m 123 -M MultiDimFit -d higgsCombineteststep1.MultiDimFit.mH123.root -w w --snapshotName \"MultiDimFit\" -n teststep2 --verbose 9 --freezeNuisances MH,lumi_8TeV","title":"Using best fit snapshots"},{"location":"part3/commonstatsmethods/#feldman-cousins","text":"The Feldman-Cousins (FC) procedure for computing confidence intervals for a generic model is, use the profile likelihood as the test-statistic q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) where x x is a point in the (N-dimensional) parameter space, and \\hat{x} \\hat{x} is the point corresponding to the best fit. In this test-statistic, the nuisance parameters are profiled, separately both in the numerator and denominator. for each point x x : compute the observed test statistic q_{\\mathrm{obs}}(x) q_{\\mathrm{obs}}(x) compute the expected distribution of q(x) q(x) under the hypothesis of x x as the true value. accept the point in the region if CL s+b =P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha =P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha With a critical value \\alpha \\alpha . In combine , you can perform this test on each individual point ( param1, param2,... ) = ( value1,value2,... ) by doing, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --clsAcc 0 --singlePoint param1=value1,param2=value2,param3=value3,... --saveHybridResult [Other options for toys, iterations etc as with limits] The point belongs to your confidence region if CL s+b is larger than \\alpha \\alpha (e.g. 0.3173 for a 1\u03c3 region, 1-\\alpha=0.6827 1-\\alpha=0.6827 ). [warning] You should not use this method without the option --singlePoint . Although combine will not complain, the algorithm to find the crossing will only find a single crossing and therefore not find the correct interval. Instead you should calculate the Feldman-Cousins intervals as described above.","title":"Feldman Cousins"},{"location":"part3/commonstatsmethods/#physical-boundaries","text":"Imposing physical boundaries (such as requiring r>0 r>0 ) is achieved by setting the ranges of the physics model parameters using --setParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... The boundary is imposed by restricting the parameter range(s) to those set by the user, in the fits. This can sometimes be an issue as Minuit may not know if has successfully converged when the minimum lies outside of that range. If there is no upper/lower boundary, just set that value to something far from the region of interest. [info] One can also imagine imposing the boundaries by first allowing Minuit to find the minimum in the un-restricted (and potentially unphysical) region and then setting the test-statistic to 0 in the case that minimum lies outside the physical boundary. If you are interested in implementing this version in combine, please contact the development team. As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs, as described in the combineTool for job submission section.","title":"Physical boundaries"},{"location":"part3/commonstatsmethods/#extracting-contours","text":"As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs with hadd . You can also refer to the combineTool for job submission section for submitting the jobs to the grid/batch.","title":"Extracting contours"},{"location":"part3/commonstatsmethods/#1d-intervals","text":"For one-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). As with limits, read in the grid of points and extract 1D intervals using, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --readHybridResults --grid=mergedfile.root --cl <1-alpha> The output tree will contain the values of the POI which crosses the critical value ( \\alpha \\alpha ) - i.e, the boundaries of the confidence intervals, You can produce a plot of the value of CL s+b vs the parameter of interest by adding the option --plot <plotname> .","title":"1D intervals"},{"location":"part3/commonstatsmethods/#2d-contours","text":"There is a tool for extracting 2D contours from the output of HybridNew located in test/makeFCcontour.py provided the option --saveHybridResult was included when running HybridNew . It can be run with the usual combine output files (or several of them) as input, ./test/makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of value of CL s+b for each point which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys in this python tool will add histograms for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored in memory.","title":"2D contours"},{"location":"part3/nonstandard/","text":"Advanced Use Cases This section will cover some of the more specific use cases for combine which are not necessarily related to the main statistics results. Fitting Diagnostics You may first want to look at the HIG PAG standard checks applied to all datacards if you want to diagnose your limit setting/fitting results which can be found here If you have already found the higgs boson but it's an exotic one, instead of computing a limit or significance you might want to extract it's cross section by performing a maximum-likelihood fit. Or, more seriously, you might want to use this same package to extract the cross section of some other process (e.g. the di-boson production). Or you might want to know how well the data compares to you model, e.g. how strongly it constraints your other nuisance parameters, what's their correlation, etc. These general diagnostic tools are contained in the method FitDiagnostics . combine -M FitDiagnostics datacard.txt The program will print out the result of the two fits performed with signal strength r (or first POI in the list) set to zero and a second with floating r . The output root tree will contain the best fit value for r and it's uncertainty. You will also get a fitDiagnostics.root file containing the following objects: Object Description nuisances_prefit RooArgSet containing the pre-fit values of the nuisance parameters, and their uncertainties from the external constraint terms only fit_b RooFitResult object containing the outcome of the fit of the data with signal strength set to zero fit_s RooFitResult object containing the outcome of the fit of the data with floating signal strength covariance_fit_s TH2D Covariance matrix of the parameters in the fit with floating signal strength covariance_fit_b TH2D Covariance matrix of the parameters in the fit with signal strength set to zero tree_prefit TTree of pre-fit nuisance parameter values and constraint terms (_In) tree_fit_sb TTree of fitted nuisance parameter values and constraint terms (_In) with floating signal strength tree_fit_b TTree of fitted nuisance parameter values and constraint terms (_In) with signal strength set to 0 [info] If you use the option --name this name will be inserted into the file name for this output file too. As well as values of the constrained nuisance parameters (and their constraint values) in the toys, you will also find branches for the number of \"bad\" nll calls (which you should check is not too large) and the status of the fit fit_status . The fit status is computed as follows fit_status = 100 * hesse_status + 10 * minos_status + minuit_summary_status The minuit_summary_status is the usual status from Minuit, details of which can be found here . For the other status values, check these documentation links for the hesse_status and the minos_status . A fit status of -1 indicates that the fit failed (Minuit summary was not 0 or 1) and hence the fit is not valid. Fit options If you need only the signal+background fit, you can run with --justFit . This can be useful if the background-only fit is not interesting or not converging (e.g. if the significance of your signal is very very large) You can use --rMin and --rMax to set the range of the first POI; a range that is not too large compared to the uncertainties you expect from the fit usually gives more stable and accurate results. By default, the uncertainties are computed using MINOS for the first POI and HESSE for all other parameters (and hence they will be symmetric for the nuisance parameters). You can run MINOS for all parameters using the option --minos all , or for none of the parameters using --minos none . Note that running MINOS is slower so you should only consider using it if you think the HESSE uncertainties are not accurate. If MINOS or HESSE fails to converge, you can try running with --robustFit=1 that will do a slower but more robust likelihood scan; this can be further controlled by the parameter --stepSize (the default is 0.1, and is relative to the range of the parameter) You can set the strategy and tolerance when using the --robustFit option using the options setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. You can also tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you find the covariance matrix provided by HESSE is not accurate (i.e. fit_s->Print() reports this was forced positive-definite) then a custom HESSE-style calculation of the covariance matrix can be used instead. This is enabled by running FitDiagnostics with the --robustHesse 1 option. Please note that the status reported by RooFitResult::Print() will contain covariance matrix quality: Unknown, matrix was externally provided when robustHesse is used, this is normal and does not indicate a problem. NB: one feature of the robustHesse algorithm is that if it still cannot calculate a positive-definite covariance matrix it will try to do so by dropping parameters from the hessian matrix before inverting. If this happens it will be reported in the output to the screen. For other fitting options see the generic minimizer options section. Fit parameter uncertainties If you get a warning message when running FitDiagnostics which says Unable to determine uncertainties on all fit parameters . This means the covariance matrix calculated in FitDiagnostics was not correct. The most common problem is that the covariance matrix is forced positive-definite. In this case the constraints on fit parameters as taken from the covariance matrix are incorrect and should not be used. In particular, if you want to make post-fit plots of the distribution used in the signal extraction fit and are extracting the uncertainties on the signal and background expectations from the covariance matrix, the resulting values will not reflect the truth if the covariance matrix was incorrect. By default if this happens and you passed the --saveWithUncertainties flag when calling FitDiagnostics , this option will be ignored as calculating the uncertainties would lead to incorrect results. This behaviour can be overridden by passing --ignoreCovWarning . Such problems with the covariance matrix can be caused by a number of things, for example: Parameters being close to their boundaries after the fit. Strong (anti-) correlations between some parameters. A discontinuity in the NLL function or its derivatives at or near the minimum. If you are aware that your analysis has any of these features you could try resolving these. Setting --cminDefaultMinimizerStrategy 0 can also help with this problem. Pre and post fit nuisance parameters and pulls It is possible to compare pre-fit and post-fit nuisance parameters with the script diffNuisances.py . Taking as input a fitDiagnostics.root file, the script will by default print out the parameters which have changed significantly w.r.t. their initial estimate. For each of those parameters, it will print out the shift in value and the post-fit uncertainty, both normalized to the input values, and the linear correlation between the parameter and the signal strength. python diffNuisances.py fitDiagnostics.root The script has several options to toggle the thresholds used to decide if a parameter has changed significantly, to get the printout of the absolute value of the nuisance parameters, and to get the output in another format for easy cut-n-paste (supported formats are html , latex , twiki ). To print all of the parameters, use the option --all . The output by default will be the changes in the nuisance parameter values and uncertainties, relative to their initial (pre-fit) values (usually relative to initial values of 0 and 1 for most nuisance types). The values in the output will be (\\theta-\\theta_{I})/\\sigma_{I} (\\theta-\\theta_{I})/\\sigma_{I} if the nuisance has a pre-fit uncertainty, otherwise it will be \\theta-\\theta_{I} \\theta-\\theta_{I} if not (eg, a flatParam has no pre-fit uncertainty). The uncertainty reported will be the ratio \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} - i.e the ratio of the post-fit to the pre-fit uncertainty. If there is no pre-fit uncertainty (as for flatParam nuisances) then the post-fit uncertainty is shown. With the option --abs , instead the pre-fit and post-fit values and (asymmetric) uncertainties will be reported in full. [info] We recommend you include the options --abs and --all to get the full information on all of the parameters (including unconstrained nuisance parameters) at least once when checking your datacards. If instead of the plain values, you wish to report the pulls , you can do so with the option --pullDef X with X being one of the following options; You should note that since the pulls below are only defined when the pre-fit uncertainty exists, nothing will be reported for parameters which have no prior constraint (except in the case of the unconstPullAsym choice as described below). You may want to run without this option and --all to get information on those parameters. relDiffAsymErrs : This is the same as the default output of the tool except that only constrained parameters (pre-fit uncertainty defined) are reported. The error is also reported and calculated as \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} . unconstPullAsym : Report the pull as \\frac{\\theta-\\theta_{I}}{\\sigma} \\frac{\\theta-\\theta_{I}}{\\sigma} where \\theta_{I} \\theta_{I} and \\sigma \\sigma are the initial value and post-fit uncertainty of that nuisance parameter. The pull defined in this way will have no error bar, but all nuisance parameters will have a result in this case. compatAsym : The pull is defined as \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} , where \\theta_{D} \\theta_{D} and \\sigma_{D} \\sigma_{D} are calculated as $\\sigma_{D} = ( \\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}} )^{-1} $ and \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the initial value and uncertainty of that nuisance parameter. This can be thought of as a compatibility between the initial measurement (prior) an imagined measurement where only the data (with no constraint) is used to measure the nuisance parameter. There is no error bar associated to this value. diffPullAsym : The pull is defined as \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the pre-fit value and uncertainty (from L. Demortier and L. Lyons ). If the denominator is close to 0 or the post-fit uncertainty is larger than the pre-fit (usually due to some failure in the calculation), the pull is not defined and the result will be reported as 0 +/- 999 . If using --pullDef , the results for all parameters for which the pull can be calculated will be shown (i.e --all will be set to true ), not just those which have moved by some metric. This script has the option ( -g outputfile.root ) to produce plots of the fitted values of the nuisance parameters and their post-fit, asymmetric uncertainties. Instead, the pulls defined using one of the options above, can be plotted using the option --pullDef X . In addition this will produce a plot showing directly a comparison of the post-fit to pre-fit nuisance (symmetrized) uncertainties. [info] In the above options, if an asymmetric uncertainty is associated to the nuisance parameter, then the choice of which uncertainty is used in the definition of the pull will depend on the sign of \\theta-\\theta_{I} \\theta-\\theta_{I} . Normalizations For a certain class of models, like those made from datacards for shape-based analysis, the tool can also compute and save to the output root file the best fit yields of all processes. If this feature is turned on with the option --saveNormalizations , the file will also contain three RooArgSet norm_prefit , norm_fit_s , norm_fit_b objects each containing one RooConstVar for each channel xxx and process yyy with name xxx/yyy and value equal to the best fit yield. You can use RooRealVar::getVal and RooRealVar::getError to estimate both the post-(or pre-)fit values and uncertainties of these normalisations. The sample pyroot macro mlfitNormsToText.py can be used to convert the root file into a text table with four columns: channel, process, yield from the signal+background fit and yield from the background-only fit. To include the uncertainties in the table, add the option --uncertainties [warning] Note that when running with multiple toys, the norm_fit_s , norm_fit_b and norm_prefit objects will be stored for the last toy dataset generated and so may not be useful to you. Note that this procedure works only for \"extended likelihoods\" like the ones used in shape-based analysis, not for the cut-and-count datacards. You can however convert a cut-and-count datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax , kmax or using combineCards.py countingcard.txt -S > shapecard.txt . Per-bin norms for shape analyses If you have a shape based analysis, you can also (instead) include the option --savePredictionsPerToy . With this option, additional branches will be filled in the three output trees contained in fitDiagnostics.root . The normalisation values for each toy will be stored in the branches inside the TTrees named n_exp[_final]_binxxx_proc_yyy . The _final will only be there if there are systematics affecting this process. Additionally, there will be filled branches which provide the value of the expected bin content for each process, in each channel. These will are named as n_exp[_final]_binxxx_proc_yyy_i (where _final will only be in the name if there are systematics affecting this process) for channel xxx , process yyy bin number i . In the case of the post-fit trees ( tree_fit_s/b ), these will be resulting expectations from the fitted models, while for the pre-fit tree, they will be the expectation from the generated model (i.e if running toys with -t N and using --genNuisances , they will be randomised for each toy). These can be useful, for example, for calculating correlations/covariances between different bins, in different channels or processes, within the model from toys. [info] Be aware that for unbinned models, a binning scheme is adopted based on the RooRealVar::getBinning for the observable defining the shape, if it exists, or combine will adopt some appropriate binning for each observable. Plotting FitDiagnostics can also produce pre- and post-fit plots the model in the same directory as fitDiagnostics.root along with the data. To get them, you have to specify the option --plots , and then optionally specify what are the names of the signal and background pdfs, e.g. --signalPdfNames='ggH*,vbfH*' and --backgroundPdfNames='*DY*,*WW*,*Top*' (by default, the definitions of signal and background are taken from the datacard). For models with more than 1 observable, a separate projection onto each observable will be produced. An alternative is to use the options --saveShapes . The result will be additional folders in fitDiagnostics.root for each category, with pre and post-fit distributions of the signals and backgrounds as TH1s and the data as TGraphAsymmErrors (with Poisson intervals as error bars). Three additional folders ( shapes_prefit , shapes_fit_sb and shapes_fit_b ) will contain the following distributions, Object Description data TGraphAsymmErrors containing the observed data (or toy data if using -t ). The vertical error bars correspond to the 68% interval for a Poisson distribution centered on the observed count. $PROCESS (id <= 0) TH1F for each signal process in channel, named as in the datacard $PROCESS (id > 0) TH1F for each background process in channel, named as in the datacard total_signal TH1F Sum over the signal components total_background TH1F Sum over the background components total TH1F Sum over all of the signal and background components The above distributions are provided for each channel included in the datacard , in separate sub-folders, named as in the datacard: There will be one sub-folder per channel. [warning] The pre-fit signal is by default for r=1 but this can be modified using the option --preFitValue . The distributions and normalisations are guaranteed to give the correct interpretation: For shape datacards whose inputs are TH1, the histograms/data points will have the bin number as the x-axis and the content of each bin will be a number of events. For datacards whose inputs are RooAbsPdf/RooDataHists, the x-axis will correspond to the observable and the bin content will be the PDF density / events divided by the bin width. This means the absolute number of events in a given bin, i, can be obtained from h.GetBinContent(i)*h.GetBinWidth(i) or similar for the data graphs. Note that for unbinned analyses combine will make a reasonable guess as to an appropriate binning. Uncertainties on the shapes will be added with the option --saveWithUncertainties . These uncertainties are generated by re-sampling of the fit covariance matrix, thereby accounting for the full correlation between the parameters of the fit. [warning] It may be tempting to sum up the uncertainties in each bin (in quadrature) to get the total uncertainty on a process however, this is (usually) incorrect as doing so would not account for correlations between the bins . Instead you can refer to the uncertainties which will be added to the post-fit normalizations described above. Additionally, the covariance matrix between bin yields (or yields/bin-widths) in each channel will also be saved as a TH2F named total_covar . If the covariance between all bins across all channels is desired, this can be added using the option --saveOverallShapes . Each folder will now contain additional distributions (and covariance matrices) corresponding to the concatenation of the bins in each channel (and therefore the covaraince between every bin in the analysis). The bin labels should make it clear as to which bin corresponds to which channel. Toy-by-toy diagnostics FitDiagnostics can also be used to diagnose the fitting procedure in toy experiments to identify potentially problematic nuisance parameters when running the full limits/p-values. This can be done by adding the option -t <num toys> . The output file, fitDiagnostics.root the three TTrees will contain the value of the constraint fitted result in each toy, as a separate entry. It is recommended to use the following options when investigating toys to reduce the running time: --toysFrequentist --noErrors --minos none The results can be plotted using the macro test/plotParametersFromToys.C $ root -l .L plotParametersFromToys.C+ plotParametersFomToys(\"fitDiagnosticsToys.root\",\"fitDiagnosticsData.root\",\"workspace.root\",\"r<0\") The first argument is the name of the output file from running with toys, and the second and third (optional) arguments are the name of the file containing the result from a fit to the data and the workspace (created from text2workspace.py ). The fourth argument can be used to specify a cut string applied to one of the branches in the tree which can be used to correlate strange behaviour with specific conditions. The output will be 2 pdf files ( tree_fit_(s)b.pdf ) and 2 root files ( tree_fit_(s)b.root ) containing canvases of the fit results of the tool. For details on the output plots, consult AN-2012/317 . Scaling constraints It possible to scale the constraints on the nuisance parameters when converting the datacard to a workspace (see the section on physics models ) with text2workspace.py . This can be useful for projection studies of the analysis to higher luminosities or with different assumptions about the sizes of certain systematics without changing the datacard by hand. We consider two kinds of scaling; A constant scaling factor to scale the constraints A functional scale factor that depends on some other parameters in the workspace, eg a luminosity scaling parameter (as a rateParam affecting all processes). In both cases these scalings can be introduced by adding some extra options at the text2workspace.py step. To add a constant scaling factor we use the option --X-rescale-nuisance , eg text2workspace.py datacard.txt --X-rescale-nuisance '[some regular expression]' 0.5 will create the workspace in which ever nuisance parameter whose name matches the specified regular expression will have the width of the gaussian constraint scaled by a factor 0.5. Multiple --X-rescale-nuisance options can be specified to set different scalings for different nuisances (note that you actually have to write --X-rescale-nuisance each time as in --X-rescale-nuisance 'theory.*' 0.5 --X-rescale-nuisance 'exp.*' 0.1 ). To add a functional scaling factor we use the option --X-nuisance-function , which works in a similar way. Instead of a constant value you should specify a RooFit factory expression. A typical case would be scaling by 1/\\sqrt{L} 1/\\sqrt{L} , where L L is a luminosity scale factor eg assuming there is some parameter in the datacard/workspace called lumiscale , text2workspace.py datacard.txt --X-nuisance-function '[some regular expression]' 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' This factory syntax is quite flexible, but for our use case the typical format will be: expr::[function name](\"[formula]\", [arg0], [arg1], ...) . The arg0 , arg1 ... are represented in the formula by @0 , @1 ,... placeholders. [warning] We are playing a slight trick here with the lumiscale parameter. At the point at which text2workspace.py is building these scaling terms the lumiscale for the rateParam has not yet been created. By writing lumiscale[1] we are telling RooFit to create this variable with an initial value of 1, and then later this will be re-used by the rateParam creation. A similar option, --X-nuisance-group-function , can be used to scale whole groups of nuisances (see groups of nuisances ). Instead of a regular expression just give the group name instead, text2workspace.py datacard.txt --X-nuisance-group-function [group name] 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' Nuisance parameter impacts The impact of a nuisance parameter (NP) \u03b8 on a parameter of interest (POI) \u03bc is defined as the shift \u0394\u03bc that is induced as \u03b8 is fixed and brought to its +1\u03c3 or \u22121\u03c3 post-fit values, with all other parameters profiled as normal. This is effectively a measure of the correlation between the NP and the POI, and is useful for determining which NPs have the largest effect on the POI uncertainty. It is possible to use the FitDiagnostics method of combine with the option --algo impact -P parameter to calculate the impact of a particular nuisance parameter on the parameter(s) of interest. We will use the combineTool.py script to automate the fits (see the combineTool section to check out the tool. We will use an example workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ cp HiggsAnalysis/CombinedLimit/data/tutorials/htt/125/htt_tt.txt . $ text2workspace.py htt_tt.txt -m 125 Calculating the impacts is done in a few stages. First we just fit for each POI, using the --doInitialFit option with combineTool.py , and adding the --robustFit 1 option that will be passed through to combine, combineTool.py -M Impacts -d htt_tt.root -m 125 --doInitialFit --robustFit 1 Have a look at the options as for likelihood scans when using robustFit 1 . Next we perform a similar scan for each nuisance parameter with the --doFits options, combineTool.py -M Impacts -d htt_tt.root -m 125 --robustFit 1 --doFits Note that this will run approximately 60 scans, and to speed things up the option --parallel X can be given to run X combine jobs simultaneously. The batch and grid submission methods described in the combineTool for job submission section can also be used. Once all jobs are completed the output can be collected and written into a json file: combineTool.py -M Impacts -d htt_tt.root -m 125 -o impacts.json A plot summarising the nuisance parameter values and impacts can be made with plotImpacts.py , plotImpacts.py -i impacts.json -o impacts The first page of the output is shown below. The direction of the +1\u03c3 and -1\u03c3 impacts (i.e. when the NP is moved to its +1\u03c3 or -1\u03c3 values) on the POI indicates whether the parameter is correlated or anti-correlated with it. [warning] The plot also shows the best fit value of the POI at the top and its uncertainty. You may wish to allow the range to go -ve (i.e using --setPhysicsModelParameterRanges or --rMin ) to avoid getting one-sided impacts! This script also accepts an optional json-file argument with - t which can be used to provide a dictionary for renaming parameters. A simple example would be to create a file rename.json , { \"r\" : \"#mu\" } that will rename the POI label on the plot. [info] Since combineTool accepts the usual options for combine you can also generate the impacts on an Asimov or toy dataset. The left panel in the summary plot shows the value of (\\theta-\\theta_{0})/\\Delta_{\\theta} (\\theta-\\theta_{0})/\\Delta_{\\theta} where \\theta \\theta and \\theta_{0} \\theta_{0} are the post and pre -fit values of the nuisance parameter and \\Delta_{\\theta} \\Delta_{\\theta} is the pre -fit uncertainty. The asymmetric error bars show the pre -fit uncertainty divided by the post -fit uncertainty meaning that parameters with error bars smaller than \\pm 1 \\pm 1 are constrained in the fit. As with the diffNuisances.py script, use the option --pullDef are defined (eg to show the pull instead). Channel Masking The combine tool has a number of features for diagnostics and plotting results of fits. It can often be useful to turn off particular channels in a combined analysis to see how constraints/pulls can vary. It can also be helpful to plot post-fit shapes + uncertainties of a particular channel (for example a signal region) without including the constraints from the data in that region. This can in some cases be achieved by removing a specific datacard when running combineCards.py however, when doing so the information of particular nuisances and pdfs in that region will be lost. Instead, it is possible to mask that channel from the likelihood! This is acheived at the text2Workspace step using the option --channel-masks . Example: removing constraints from the signal region We will take the control region example from the rate parameters tutorial from data/tutorials/rate_params/ . The first step is to combine the cards combineCards.py signal=signal_region.txt dimuon=dimuon_control_region.txt singlemuon=singlemuon_control_region.txt > datacard.txt Note that we use the directive CHANNELNAME=CHANNEL_DATACARD.txt so that the names of the channels are under our control and easier to interpret. Next, we make a workspace and tell combine to create the parameters used to mask channels text2workspace.py datacard.txt --channel-masks Now lets try a fit ignoring the signal region. We can turn off the signal region by setting the channel mask parameter on: --setParameters mask_signal=1 . Note that text2workspace has created a masking parameter for every channel with the naming scheme mask_CHANNELNAME . By default, every parameter is set to 0 so that the channel is unmasked by default. combine datacard.root -M FitDiagnostics --saveShapes --saveWithUncertainties --setParameters mask_signal=1 [warning] There will be a lot of warning from combine. This is safe to ignore as this is due to the s+b fit not converging since the free signal parameter cannot be constrained as the data in the signal region is being ignored. We can compare the background post-fit and uncertainties with and without the signal region by re-running with --setParameters mask_signal=0 (or just removing that command). Below is a comparison of the background in the signal region with and without masking the data in the signal region. We take these from the shapes folder shapes_fit_b/signal/total_background in the fitDiagnostics.root output. Clearly the background shape is different and much less constrained without including the signal region , as expected. Channel masking can be used with any method in combine. RooMultiPdf conventional bias studies Several analyses within the Higgs group use a functional form to describe their background which is fit to the data (eg the Higgs to two photons (Hgg) analysis). Often however, there is some uncertainty associated to the choice of which background function to use and this choice will impact results of a fit. It is therefore often the case that in these analyses, a Bias study is performed which will indicate how much potential bias can be present given a certain choice of functional form. These studies can be conducted using combine. Below is an example script which will produce a workspace based on a simplified Hgg analysis with a single category. You will need to get the file data/tutorials/bias_studies/toyhgg_in.root . void makeRooMultiPdfWorkspace(){ // Load the combine Library gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Open the dummy H->gg workspace TFile *f_hgg = TFile::Open(\"toyhgg_in.root\"); RooWorkspace *w_hgg = (RooWorkspace*)f_hgg->Get(\"multipdf\"); // The observable (CMS_hgg_mass in the workspace) RooRealVar *mass = w_hgg->var(\"CMS_hgg_mass\"); // Get three of the functions inside, exponential, linear polynomial, power law RooAbsPdf *pdf_exp = w_hgg->pdf(\"env_pdf_1_8TeV_exp1\"); RooAbsPdf *pdf_pol = w_hgg->pdf(\"env_pdf_1_8TeV_bern2\"); RooAbsPdf *pdf_pow = w_hgg->pdf(\"env_pdf_1_8TeV_pow1\"); // Fit the functions to the data to set the \"prefit\" state (note this can and should be redone with combine when doing // bias studies as one typically throws toys from the \"best-fit\" RooDataSet *data = (RooDataSet*)w_hgg->data(\"roohist_data_mass_cat1_toy1_cutrange__CMS_hgg_mass\"); pdf_exp->fitTo(*data); // index 0 pdf_pow->fitTo(*data); // index 1 pdf_pol->fitTo(*data); // index 2 // Make a plot (data is a toy dataset) RooPlot *plot = mass->frame(); data->plotOn(plot); pdf_exp->plotOn(plot,RooFit::LineColor(kGreen)); pdf_pol->plotOn(plot,RooFit::LineColor(kBlue)); pdf_pow->plotOn(plot,RooFit::LineColor(kRed)); plot->SetTitle(\"PDF fits to toy data\"); plot->Draw(); // Make a RooCategory object. This will control which of the pdfs is \"active\" RooCategory cat(\"pdf_index\",\"Index of Pdf which is active\"); // Make a RooMultiPdf object. The order of the pdfs will be the order of their index, ie for below // 0 == exponential // 1 == linear function // 2 == powerlaw RooArgList mypdfs; mypdfs.add(*pdf_exp); mypdfs.add(*pdf_pol); mypdfs.add(*pdf_pow); RooMultiPdf multipdf(\"roomultipdf\",\"All Pdfs\",cat,mypdfs); // As usual make an extended term for the background with _norm for freely floating yield RooRealVar norm(\"roomultipdf_norm\",\"Number of background events\",0,10000); // Save to a new workspace TFile *fout = new TFile(\"background_pdfs.root\",\"RECREATE\"); RooWorkspace wout(\"backgrounds\",\"backgrounds\"); wout.import(cat); wout.import(norm); wout.import(multipdf); wout.Print(); wout.Write(); } The signal is modelled as a simple Gaussian with a width approximately that of the diphoton resolution and the background is a choice of 3 functions. An exponential, a power-law and a 2nd order polynomial. This choice is accessible inside combine through the use of the RooMultiPdf object which can switch between the functions by setting its associated index (herein called pdf_index ). This (as with all parameters in combine) is accessible via the --setPhysicsModelParameters option. To asses the bias, one can throw toys using one function and fit with another. All of this only needs to use one datacard hgg_toy_datacard.txt The bias studies are performed in two stages. The first is to generate toys using one of the functions under some value of the signal strength r (or \\mu \\mu ). This can be repeated for several values of r and also at different masses, but here the Higgs mass is fixed to 125 GeV. [warning] It is important to freeze pdf_index otherwise combine will try to iterate over the index in the frequentist fit. combine hgg_toy_datacard.txt -M GenerateOnly --setParameters pdf_index=0 --toysFrequentist -t 100 --expectSignal 1 --saveToys -m 125 --freezeParameters pdf_index Now we have 100 toys which, by setting pdf_index=0 , sets the background pdf to the exponential function i.e assumes the exponential is the true function. Note that the option --toysFrequentist is added. This first performs a fit of the pdf, assuming a signal strength of 1, to the data before generating the toys. This is the most obvious choice as to where to throw the toys from. The next step is to fit the toys under a different background pdf hypothesis. This time we set the pdf_index to be 1, the powerlaw and run fits with the FitDiagnostics method again freezing pdf_index. [warning] You may get warnings about non-accurate errors but these can be ignored and is related to the free parameters of the background pdfs which are not active. combine hgg_toy_datacard.txt -M FitDiagnostics --setParameters pdf_index=1 --toysFile higgsCombineTest.GenerateOnly.mH125.123456.root -t 100 --rMin -10 --rMax 10 --freezeParameters pdf_index In the output file fitDiagnostics.root there is a tree which contains the best fit results under the signal+background hypothesis. One measure of the bias is the pull defined as the difference between the measured value of \\mu \\mu and the generated value (here we used 1) relative to the uncertainty on \\mu \\mu . The pull distribution can be drawn and the mean provides an estimate of the pull... root -l fitDiagnostics.root tree_fit_sb->Draw(\"(r-1)/rErr>>h(20,-4,4)\") h->Fit(\"gaus\") From the fitted Gaussian, we see the mean is at +0.30 which would indicate a bias of ~30% of the uncertainty on mu from choosing the powerlaw when the true function is an exponential. [danger] If the discrete nuisance is left floating, it will be profiled by looping through the possible index values and finding the pdf which gives the best fit. This allows for the discrete profiling method to be applied for any method which involves a profiled likelihood (frequentist methods). You should be careful however since MINOS knows nothing about these nuisances and hence estimations of uncertainties will be incorrect. Instead, uncertainties from scans and limits will correctly account for these nuisances. Currently the Bayesian methods will not properly treat the nuisances so some care should be taken when interpreting Bayesian results. RooSplineND multidimensional splines RooSplineND can be used to interpolate from tree of points to produce a continuous function in N-dimensions. This function can then be used as input to workspaces allowing for parametric rates/cross-sections/efficiencies etc OR can be used to up-scale the resolution of likelihood scans (i.e like those produced from combine) to produce smooth contours. The following script is an example of its use which produces a 2D spline from a set of points generated from a function. void splinend(){ // library containing the RooSplineND gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); TTree *tree = new TTree(\"tree_vals\",\"tree_vals\"); float xb,yb,fb; tree->Branch(\"f\",&fb,\"f/Float_t\"); tree->Branch(\"x\",&xb,\"x/Float_t\"); tree->Branch(\"y\",&yb,\"y/Float_t\"); TRandom3 *r = new TRandom3(); int nentries = 20; // just use a regular grid of 20x20 double xmin = -3.2; double xmax = 3.2; double ymin = -3.2; double ymax = 3.2; for (int n=0;n<nentries;n++){ for (int k=0;k<nentries;k++){ xb=xmin+n*((xmax-xmin)/nentries); yb=ymin+k*((ymax-ymin)/nentries); // Gaussian * cosine function radial in \"F(x^2+y^2)\" double R = (xb*xb)+(yb*yb); fb = 0.1*TMath::Exp(-1*(R)/9)*TMath::Cos(2.5*TMath::Sqrt(R)); tree->Fill(); } } // 2D graph of points in tree TGraph2D *p0 = new TGraph2D(); p0->SetMarkerSize(0.8); p0->SetMarkerStyle(20); int c0=0; for (int p=0;p<tree->GetEntries();p++){ tree->GetEntry(p); p0->SetPoint(c0,xb,yb,fb); c0++; } // ------------------------------ THIS IS WHERE WE BUILD THE SPLINE ------------------------ // // Create 2 Real-vars, one for each of the parameters of the spline // The variables MUST be named the same as the corresponding branches in the tree RooRealVar x(\"x\",\"x\",0.1,xmin,xmax); RooRealVar y(\"y\",\"y\",0.1,ymin,ymax); // And the spline - arguments are // Required -> name, title, arglist of dependants, input tree, // Optional -> function branch name, interpolation width (tunable parameter), rescale Axis bool, cutstring // The tunable parameter gives the radial basis a \"width\", over which the interpolation will be effectively taken // the reascale Axis bool (if true) will first try to rescale the points so that they are of order 1 in range // This can be helpful if for example one dimension is in much larger units than another. // The cutstring is just a ROOT string which can be used to apply cuts to the tree in case only a sub-set of the points should be used RooArgList args(x,y); RooSplineND *spline = new RooSplineND(\"spline\",\"spline\",args,tree,\"f\",1,true); // ----------------------------------------------------------------------------------------- // //TGraph *gr = spline->getGraph(\"x\",0.1); // Return 1D graph. Will be a slice of the spline for fixed y generated at steps of 0.1 // Plot the 2D spline TGraph2D *gr = new TGraph2D(); int pt = 0; for (double xx=xmin;xx<xmax;xx+=0.1){ for (double yy=xmin;yy<ymax;yy+=0.1){ x.setVal(xx); y.setVal(yy); gr->SetPoint(pt,xx,yy,spline.getVal()); pt++; } } gr->SetTitle(\"\"); gr->SetLineColor(1); //p0->SetTitle(\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\"); gr->Draw(\"surf\"); gr->GetXaxis()->SetTitle(\"x\"); gr->GetYaxis()->SetTitle(\"y\"); p0->Draw(\"Pcolsame\"); //p0->Draw(\"surfsame\"); TLegend *leg = new TLegend(0.2,0.82,0.82,0.98); leg->SetFillColor(0); leg->AddEntry(p0,\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\",\"p\"); leg->AddEntry(gr,\"RooSplineND (N=2) interpolation\",\"L\"); leg->Draw(); } Running the script will produce the following plot. The plot shows the sampled points and the spline produced from them. RooParametricHist gammaN for shapes Currently, there is no straight-forward implementation of using per-bin gmN like uncertainties with shape (histogram) analyses. Instead, it is possible to tie control regions (written as datacards) with the signal region using three methods. For analyses who take the normalisation of some process from a control region, it is possible to use either lnU or rateParam directives to float the normalisation in a correlated way of some process between two regions. Instead if each bin is intended to be determined via a control region, one can use a number of RooFit histogram pdfs/functions to accomplish this. The example below shows a simple implementation of a RooParametricHist to achieve this. copy the script below into a file called examplews.C and create the input workspace using root -l examplews.C ... void examplews(){ // As usual, load the combine library to get access to the RooParametricHist gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Output file and workspace TFile *fOut = new TFile(\"param_ws.root\",\"RECREATE\"); RooWorkspace wspace(\"wspace\",\"wspace\"); // A search in a MET tail, define MET as our variable RooRealVar met(\"met\",\"E_{T}^{miss}\",200,1000); RooArgList vars(met); // ---------------------------- SIGNAL REGION -------------------------------------------------------------------// // Make a dataset, this will be just four bins in MET. // its easiest to make this from a histogram. Set the contents to \"somehting\" TH1F data_th1(\"data_obs_SR\",\"Data observed in signal region\",4,200,1000); data_th1.SetBinContent(1,100); data_th1.SetBinContent(2,50); data_th1.SetBinContent(3,25); data_th1.SetBinContent(4,10); RooDataHist data_hist(\"data_obs_SR\",\"Data observed\",vars,&data_th1); wspace.import(data_hist); // In the signal region, our background process will be freely floating, // Create one parameter per bin representing the yield. (note of course we can have multiple processes like this) RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); // Create a RooParametericHist which contains those yields, last argument is just for the binning, // can use the data TH1 for that RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); // Always include a _norm term which should be the sum of the yields (thats how combine likes to play with pdfs) RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); // Every signal region needs a signal TH1F signal_th1(\"signal_SR\",\"Signal expected in signal region\",4,200,1000); signal_th1.SetBinContent(1,1); signal_th1.SetBinContent(2,2); signal_th1.SetBinContent(3,3); signal_th1.SetBinContent(4,8); RooDataHist signal_hist(\"signal\",\"Data observed\",vars,&signal_th1); wspace.import(signal_hist); // -------------------------------------------------------------------------------------------------------------// // ---------------------------- CONTROL REGION -----------------------------------------------------------------// TH1F data_CRth1(\"data_obs_CR\",\"Data observed in control region\",4,200,1000); data_CRth1.SetBinContent(1,200); data_CRth1.SetBinContent(2,100); data_CRth1.SetBinContent(3,50); data_CRth1.SetBinContent(4,20); RooDataHist data_CRhist(\"data_obs_CR\",\"Data observed\",vars,&data_CRth1); wspace.import(data_CRhist); // This time, the background process will be dependent on the yields of the background in the signal region. // The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control // In this example lets assume the control region is populated by the same process decaying to clean daughters with 2xBR // compared to the signal region // NB You could have a different transfer factor for each bin represented by a completely different RooRealVar // We can imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. // We need to make these nuisance parameters ourselves and give them a nominal value of 0 RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); // We would need to make the transfer factor a function of those too. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) // but note that we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could // use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); // Finally, we need to make each bin of the background in the control region a function of the background in the signal and the transfer factor // N_CR = N_SR x TF RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); // -------------------------------------------------------------------------------------------------------------// // import the pdfs wspace.import(p_bkg); wspace.import(p_bkg_norm,RooFit::RecycleConflictNodes()); wspace.import(p_CRbkg); wspace.import(p_CRbkg_norm,RooFit::RecycleConflictNodes()); fOut->cd(); wspace.Write(); // Clean up fOut->Close(); fOut->Delete(); } Lets go through what the script is doing. First, the observable for the search is the missing energy so we create a parameter to represent that. RooRealVar met(\"met\",\"E_{T}^{miss}\",200,1000); First, the following lines create a freely floating parameter for each of our bins (in this example, there are only 4 bins, defined for our observable met . RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); They are put into a list so that we can create a RooParametricHist and its normalisation from that list RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); For the control region, the background process will be dependent on the yields of the background in the signal region using a transfer factor . The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control regions. In this example lets assume the control region is populated by the same process decaying to a different final state with twice as large branching ratio compared to the one in the signal region. We could imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. We need to make nuisance parameters ourselves to model this and give them a nominal value of 0. RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); We need to make the transfer factor a function of these parameters since variations in these uncertainties will lead to variations of the transfer factor. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) but we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); Then need to make each bin of the background in the control region a function of the background in the signal and the transfer factor - i.e $N_{CR} = N_{SR} \\times TF $. RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); As before, we also need to create the RooParametricHist for this process in the control region but this time the bin yields will be the RooFormulaVars we just created instead of free floating parameters. RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); Below are datacards (for signal and control regions) which can be used in conjunction with the workspace built above. In order to \"use\" the control region, simply combine the two cards as usual using combineCards.py . Signal Region Datacard -- signal category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs signal param_ws.root wspace:data_obs_SR shapes background signal param_ws.root wspace:bkg_SR # the background model pdf which is freely floating, note other backgrounds can be added as usual shapes signal signal param_ws.root wspace:signal ------------------------------------------------------------------------------------------------------------------------------------------- bin signal observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin signal signal process background signal process 1 0 rate 1 -1 ------------------------------------------------------------------------------------------------------------------------------------------- # Normal uncertainties in the signal region lumi_8TeV lnN - 1.026 ------------------------------------------------------------------------------------------------------------------------------------------- # free floating parameters, we do not need to declare them, but its a good idea to bkg_SR_bin1 flatParam bkg_SR_bin2 flatParam bkg_SR_bin3 flatParam bkg_SR_bin4 flatParam Control Region Datacard -- control category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs control param_ws.root wspace:data_obs_CR shapes background control param_ws.root wspace:bkg_CR # the background model pdf which is dependant on that in the SR, note other backgrounds can be added as usual ------------------------------------------------------------------------------------------------------------------------------------------- bin control observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin control process background process 1 rate 1 ------------------------------------------------------------------------------------------------------------------------------------------- efficiency param 0 1 acceptance param 0 1 Note that for the control region, our nuisance parameters appear as param types so that combine will correctly constrain them. If we combine the two cards and fit the result with -M MultiDimFit -v 3 we can see that the parameters which give the rate of background in each bin of the signal region, along with the nuisance parameters and signal strength, are determined by the fit - i.e we have properly included the constraint from the control region, just as with the 1-bin gmN . acceptance = 0.00374312 +/- 0.964632 (limited) bkg_SR_bin1 = 99.9922 +/- 5.92062 (limited) bkg_SR_bin2 = 49.9951 +/- 4.13535 (limited) bkg_SR_bin3 = 24.9915 +/- 2.9267 (limited) bkg_SR_bin4 = 9.96478 +/- 2.1348 (limited) efficiency = 0.00109195 +/- 0.979334 (limited) lumi_8TeV = -0.0025911 +/- 0.994458 r = 0.00716347 +/- 12.513 (limited) The example given here is extremely basic and it should be noted that additional complexity in the transfer factors, additional uncertainties/backgrounds etc in the cards are supported as always. [danger] If trying to implement parametric uncertainties in this setup (eg on transfer factors) which are correlated with other channels and implemented separately, you MUST normalise the uncertainty effect so that the datacard line can read param name X 1 . That is the uncertainty on this parameter must be 1. Without this, there will be inconsistency with other nuisances of the same name in other channels implemented as shape or lnN .","title":"Advanced use cases"},{"location":"part3/nonstandard/#advanced-use-cases","text":"This section will cover some of the more specific use cases for combine which are not necessarily related to the main statistics results.","title":"Advanced Use Cases"},{"location":"part3/nonstandard/#fitting-diagnostics","text":"You may first want to look at the HIG PAG standard checks applied to all datacards if you want to diagnose your limit setting/fitting results which can be found here If you have already found the higgs boson but it's an exotic one, instead of computing a limit or significance you might want to extract it's cross section by performing a maximum-likelihood fit. Or, more seriously, you might want to use this same package to extract the cross section of some other process (e.g. the di-boson production). Or you might want to know how well the data compares to you model, e.g. how strongly it constraints your other nuisance parameters, what's their correlation, etc. These general diagnostic tools are contained in the method FitDiagnostics . combine -M FitDiagnostics datacard.txt The program will print out the result of the two fits performed with signal strength r (or first POI in the list) set to zero and a second with floating r . The output root tree will contain the best fit value for r and it's uncertainty. You will also get a fitDiagnostics.root file containing the following objects: Object Description nuisances_prefit RooArgSet containing the pre-fit values of the nuisance parameters, and their uncertainties from the external constraint terms only fit_b RooFitResult object containing the outcome of the fit of the data with signal strength set to zero fit_s RooFitResult object containing the outcome of the fit of the data with floating signal strength covariance_fit_s TH2D Covariance matrix of the parameters in the fit with floating signal strength covariance_fit_b TH2D Covariance matrix of the parameters in the fit with signal strength set to zero tree_prefit TTree of pre-fit nuisance parameter values and constraint terms (_In) tree_fit_sb TTree of fitted nuisance parameter values and constraint terms (_In) with floating signal strength tree_fit_b TTree of fitted nuisance parameter values and constraint terms (_In) with signal strength set to 0 [info] If you use the option --name this name will be inserted into the file name for this output file too. As well as values of the constrained nuisance parameters (and their constraint values) in the toys, you will also find branches for the number of \"bad\" nll calls (which you should check is not too large) and the status of the fit fit_status . The fit status is computed as follows fit_status = 100 * hesse_status + 10 * minos_status + minuit_summary_status The minuit_summary_status is the usual status from Minuit, details of which can be found here . For the other status values, check these documentation links for the hesse_status and the minos_status . A fit status of -1 indicates that the fit failed (Minuit summary was not 0 or 1) and hence the fit is not valid.","title":"Fitting Diagnostics"},{"location":"part3/nonstandard/#fit-options","text":"If you need only the signal+background fit, you can run with --justFit . This can be useful if the background-only fit is not interesting or not converging (e.g. if the significance of your signal is very very large) You can use --rMin and --rMax to set the range of the first POI; a range that is not too large compared to the uncertainties you expect from the fit usually gives more stable and accurate results. By default, the uncertainties are computed using MINOS for the first POI and HESSE for all other parameters (and hence they will be symmetric for the nuisance parameters). You can run MINOS for all parameters using the option --minos all , or for none of the parameters using --minos none . Note that running MINOS is slower so you should only consider using it if you think the HESSE uncertainties are not accurate. If MINOS or HESSE fails to converge, you can try running with --robustFit=1 that will do a slower but more robust likelihood scan; this can be further controlled by the parameter --stepSize (the default is 0.1, and is relative to the range of the parameter) You can set the strategy and tolerance when using the --robustFit option using the options setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. You can also tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you find the covariance matrix provided by HESSE is not accurate (i.e. fit_s->Print() reports this was forced positive-definite) then a custom HESSE-style calculation of the covariance matrix can be used instead. This is enabled by running FitDiagnostics with the --robustHesse 1 option. Please note that the status reported by RooFitResult::Print() will contain covariance matrix quality: Unknown, matrix was externally provided when robustHesse is used, this is normal and does not indicate a problem. NB: one feature of the robustHesse algorithm is that if it still cannot calculate a positive-definite covariance matrix it will try to do so by dropping parameters from the hessian matrix before inverting. If this happens it will be reported in the output to the screen. For other fitting options see the generic minimizer options section.","title":"Fit options"},{"location":"part3/nonstandard/#fit-parameter-uncertainties","text":"If you get a warning message when running FitDiagnostics which says Unable to determine uncertainties on all fit parameters . This means the covariance matrix calculated in FitDiagnostics was not correct. The most common problem is that the covariance matrix is forced positive-definite. In this case the constraints on fit parameters as taken from the covariance matrix are incorrect and should not be used. In particular, if you want to make post-fit plots of the distribution used in the signal extraction fit and are extracting the uncertainties on the signal and background expectations from the covariance matrix, the resulting values will not reflect the truth if the covariance matrix was incorrect. By default if this happens and you passed the --saveWithUncertainties flag when calling FitDiagnostics , this option will be ignored as calculating the uncertainties would lead to incorrect results. This behaviour can be overridden by passing --ignoreCovWarning . Such problems with the covariance matrix can be caused by a number of things, for example: Parameters being close to their boundaries after the fit. Strong (anti-) correlations between some parameters. A discontinuity in the NLL function or its derivatives at or near the minimum. If you are aware that your analysis has any of these features you could try resolving these. Setting --cminDefaultMinimizerStrategy 0 can also help with this problem.","title":"Fit parameter uncertainties"},{"location":"part3/nonstandard/#pre-and-post-fit-nuisance-parameters-and-pulls","text":"It is possible to compare pre-fit and post-fit nuisance parameters with the script diffNuisances.py . Taking as input a fitDiagnostics.root file, the script will by default print out the parameters which have changed significantly w.r.t. their initial estimate. For each of those parameters, it will print out the shift in value and the post-fit uncertainty, both normalized to the input values, and the linear correlation between the parameter and the signal strength. python diffNuisances.py fitDiagnostics.root The script has several options to toggle the thresholds used to decide if a parameter has changed significantly, to get the printout of the absolute value of the nuisance parameters, and to get the output in another format for easy cut-n-paste (supported formats are html , latex , twiki ). To print all of the parameters, use the option --all . The output by default will be the changes in the nuisance parameter values and uncertainties, relative to their initial (pre-fit) values (usually relative to initial values of 0 and 1 for most nuisance types). The values in the output will be (\\theta-\\theta_{I})/\\sigma_{I} (\\theta-\\theta_{I})/\\sigma_{I} if the nuisance has a pre-fit uncertainty, otherwise it will be \\theta-\\theta_{I} \\theta-\\theta_{I} if not (eg, a flatParam has no pre-fit uncertainty). The uncertainty reported will be the ratio \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} - i.e the ratio of the post-fit to the pre-fit uncertainty. If there is no pre-fit uncertainty (as for flatParam nuisances) then the post-fit uncertainty is shown. With the option --abs , instead the pre-fit and post-fit values and (asymmetric) uncertainties will be reported in full. [info] We recommend you include the options --abs and --all to get the full information on all of the parameters (including unconstrained nuisance parameters) at least once when checking your datacards. If instead of the plain values, you wish to report the pulls , you can do so with the option --pullDef X with X being one of the following options; You should note that since the pulls below are only defined when the pre-fit uncertainty exists, nothing will be reported for parameters which have no prior constraint (except in the case of the unconstPullAsym choice as described below). You may want to run without this option and --all to get information on those parameters. relDiffAsymErrs : This is the same as the default output of the tool except that only constrained parameters (pre-fit uncertainty defined) are reported. The error is also reported and calculated as \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} . unconstPullAsym : Report the pull as \\frac{\\theta-\\theta_{I}}{\\sigma} \\frac{\\theta-\\theta_{I}}{\\sigma} where \\theta_{I} \\theta_{I} and \\sigma \\sigma are the initial value and post-fit uncertainty of that nuisance parameter. The pull defined in this way will have no error bar, but all nuisance parameters will have a result in this case. compatAsym : The pull is defined as \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} , where \\theta_{D} \\theta_{D} and \\sigma_{D} \\sigma_{D} are calculated as $\\sigma_{D} = ( \\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}} )^{-1} $ and \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the initial value and uncertainty of that nuisance parameter. This can be thought of as a compatibility between the initial measurement (prior) an imagined measurement where only the data (with no constraint) is used to measure the nuisance parameter. There is no error bar associated to this value. diffPullAsym : The pull is defined as \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the pre-fit value and uncertainty (from L. Demortier and L. Lyons ). If the denominator is close to 0 or the post-fit uncertainty is larger than the pre-fit (usually due to some failure in the calculation), the pull is not defined and the result will be reported as 0 +/- 999 . If using --pullDef , the results for all parameters for which the pull can be calculated will be shown (i.e --all will be set to true ), not just those which have moved by some metric. This script has the option ( -g outputfile.root ) to produce plots of the fitted values of the nuisance parameters and their post-fit, asymmetric uncertainties. Instead, the pulls defined using one of the options above, can be plotted using the option --pullDef X . In addition this will produce a plot showing directly a comparison of the post-fit to pre-fit nuisance (symmetrized) uncertainties. [info] In the above options, if an asymmetric uncertainty is associated to the nuisance parameter, then the choice of which uncertainty is used in the definition of the pull will depend on the sign of \\theta-\\theta_{I} \\theta-\\theta_{I} .","title":"Pre and post fit nuisance parameters and pulls"},{"location":"part3/nonstandard/#normalizations","text":"For a certain class of models, like those made from datacards for shape-based analysis, the tool can also compute and save to the output root file the best fit yields of all processes. If this feature is turned on with the option --saveNormalizations , the file will also contain three RooArgSet norm_prefit , norm_fit_s , norm_fit_b objects each containing one RooConstVar for each channel xxx and process yyy with name xxx/yyy and value equal to the best fit yield. You can use RooRealVar::getVal and RooRealVar::getError to estimate both the post-(or pre-)fit values and uncertainties of these normalisations. The sample pyroot macro mlfitNormsToText.py can be used to convert the root file into a text table with four columns: channel, process, yield from the signal+background fit and yield from the background-only fit. To include the uncertainties in the table, add the option --uncertainties [warning] Note that when running with multiple toys, the norm_fit_s , norm_fit_b and norm_prefit objects will be stored for the last toy dataset generated and so may not be useful to you. Note that this procedure works only for \"extended likelihoods\" like the ones used in shape-based analysis, not for the cut-and-count datacards. You can however convert a cut-and-count datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax , kmax or using combineCards.py countingcard.txt -S > shapecard.txt .","title":"Normalizations"},{"location":"part3/nonstandard/#per-bin-norms-for-shape-analyses","text":"If you have a shape based analysis, you can also (instead) include the option --savePredictionsPerToy . With this option, additional branches will be filled in the three output trees contained in fitDiagnostics.root . The normalisation values for each toy will be stored in the branches inside the TTrees named n_exp[_final]_binxxx_proc_yyy . The _final will only be there if there are systematics affecting this process. Additionally, there will be filled branches which provide the value of the expected bin content for each process, in each channel. These will are named as n_exp[_final]_binxxx_proc_yyy_i (where _final will only be in the name if there are systematics affecting this process) for channel xxx , process yyy bin number i . In the case of the post-fit trees ( tree_fit_s/b ), these will be resulting expectations from the fitted models, while for the pre-fit tree, they will be the expectation from the generated model (i.e if running toys with -t N and using --genNuisances , they will be randomised for each toy). These can be useful, for example, for calculating correlations/covariances between different bins, in different channels or processes, within the model from toys. [info] Be aware that for unbinned models, a binning scheme is adopted based on the RooRealVar::getBinning for the observable defining the shape, if it exists, or combine will adopt some appropriate binning for each observable.","title":"Per-bin norms for shape analyses"},{"location":"part3/nonstandard/#plotting","text":"FitDiagnostics can also produce pre- and post-fit plots the model in the same directory as fitDiagnostics.root along with the data. To get them, you have to specify the option --plots , and then optionally specify what are the names of the signal and background pdfs, e.g. --signalPdfNames='ggH*,vbfH*' and --backgroundPdfNames='*DY*,*WW*,*Top*' (by default, the definitions of signal and background are taken from the datacard). For models with more than 1 observable, a separate projection onto each observable will be produced. An alternative is to use the options --saveShapes . The result will be additional folders in fitDiagnostics.root for each category, with pre and post-fit distributions of the signals and backgrounds as TH1s and the data as TGraphAsymmErrors (with Poisson intervals as error bars). Three additional folders ( shapes_prefit , shapes_fit_sb and shapes_fit_b ) will contain the following distributions, Object Description data TGraphAsymmErrors containing the observed data (or toy data if using -t ). The vertical error bars correspond to the 68% interval for a Poisson distribution centered on the observed count. $PROCESS (id <= 0) TH1F for each signal process in channel, named as in the datacard $PROCESS (id > 0) TH1F for each background process in channel, named as in the datacard total_signal TH1F Sum over the signal components total_background TH1F Sum over the background components total TH1F Sum over all of the signal and background components The above distributions are provided for each channel included in the datacard , in separate sub-folders, named as in the datacard: There will be one sub-folder per channel. [warning] The pre-fit signal is by default for r=1 but this can be modified using the option --preFitValue . The distributions and normalisations are guaranteed to give the correct interpretation: For shape datacards whose inputs are TH1, the histograms/data points will have the bin number as the x-axis and the content of each bin will be a number of events. For datacards whose inputs are RooAbsPdf/RooDataHists, the x-axis will correspond to the observable and the bin content will be the PDF density / events divided by the bin width. This means the absolute number of events in a given bin, i, can be obtained from h.GetBinContent(i)*h.GetBinWidth(i) or similar for the data graphs. Note that for unbinned analyses combine will make a reasonable guess as to an appropriate binning. Uncertainties on the shapes will be added with the option --saveWithUncertainties . These uncertainties are generated by re-sampling of the fit covariance matrix, thereby accounting for the full correlation between the parameters of the fit. [warning] It may be tempting to sum up the uncertainties in each bin (in quadrature) to get the total uncertainty on a process however, this is (usually) incorrect as doing so would not account for correlations between the bins . Instead you can refer to the uncertainties which will be added to the post-fit normalizations described above. Additionally, the covariance matrix between bin yields (or yields/bin-widths) in each channel will also be saved as a TH2F named total_covar . If the covariance between all bins across all channels is desired, this can be added using the option --saveOverallShapes . Each folder will now contain additional distributions (and covariance matrices) corresponding to the concatenation of the bins in each channel (and therefore the covaraince between every bin in the analysis). The bin labels should make it clear as to which bin corresponds to which channel.","title":"Plotting"},{"location":"part3/nonstandard/#toy-by-toy-diagnostics","text":"FitDiagnostics can also be used to diagnose the fitting procedure in toy experiments to identify potentially problematic nuisance parameters when running the full limits/p-values. This can be done by adding the option -t <num toys> . The output file, fitDiagnostics.root the three TTrees will contain the value of the constraint fitted result in each toy, as a separate entry. It is recommended to use the following options when investigating toys to reduce the running time: --toysFrequentist --noErrors --minos none The results can be plotted using the macro test/plotParametersFromToys.C $ root -l .L plotParametersFromToys.C+ plotParametersFomToys(\"fitDiagnosticsToys.root\",\"fitDiagnosticsData.root\",\"workspace.root\",\"r<0\") The first argument is the name of the output file from running with toys, and the second and third (optional) arguments are the name of the file containing the result from a fit to the data and the workspace (created from text2workspace.py ). The fourth argument can be used to specify a cut string applied to one of the branches in the tree which can be used to correlate strange behaviour with specific conditions. The output will be 2 pdf files ( tree_fit_(s)b.pdf ) and 2 root files ( tree_fit_(s)b.root ) containing canvases of the fit results of the tool. For details on the output plots, consult AN-2012/317 .","title":"Toy-by-toy diagnostics"},{"location":"part3/nonstandard/#scaling-constraints","text":"It possible to scale the constraints on the nuisance parameters when converting the datacard to a workspace (see the section on physics models ) with text2workspace.py . This can be useful for projection studies of the analysis to higher luminosities or with different assumptions about the sizes of certain systematics without changing the datacard by hand. We consider two kinds of scaling; A constant scaling factor to scale the constraints A functional scale factor that depends on some other parameters in the workspace, eg a luminosity scaling parameter (as a rateParam affecting all processes). In both cases these scalings can be introduced by adding some extra options at the text2workspace.py step. To add a constant scaling factor we use the option --X-rescale-nuisance , eg text2workspace.py datacard.txt --X-rescale-nuisance '[some regular expression]' 0.5 will create the workspace in which ever nuisance parameter whose name matches the specified regular expression will have the width of the gaussian constraint scaled by a factor 0.5. Multiple --X-rescale-nuisance options can be specified to set different scalings for different nuisances (note that you actually have to write --X-rescale-nuisance each time as in --X-rescale-nuisance 'theory.*' 0.5 --X-rescale-nuisance 'exp.*' 0.1 ). To add a functional scaling factor we use the option --X-nuisance-function , which works in a similar way. Instead of a constant value you should specify a RooFit factory expression. A typical case would be scaling by 1/\\sqrt{L} 1/\\sqrt{L} , where L L is a luminosity scale factor eg assuming there is some parameter in the datacard/workspace called lumiscale , text2workspace.py datacard.txt --X-nuisance-function '[some regular expression]' 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' This factory syntax is quite flexible, but for our use case the typical format will be: expr::[function name](\"[formula]\", [arg0], [arg1], ...) . The arg0 , arg1 ... are represented in the formula by @0 , @1 ,... placeholders. [warning] We are playing a slight trick here with the lumiscale parameter. At the point at which text2workspace.py is building these scaling terms the lumiscale for the rateParam has not yet been created. By writing lumiscale[1] we are telling RooFit to create this variable with an initial value of 1, and then later this will be re-used by the rateParam creation. A similar option, --X-nuisance-group-function , can be used to scale whole groups of nuisances (see groups of nuisances ). Instead of a regular expression just give the group name instead, text2workspace.py datacard.txt --X-nuisance-group-function [group name] 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])'","title":"Scaling constraints"},{"location":"part3/nonstandard/#nuisance-parameter-impacts","text":"The impact of a nuisance parameter (NP) \u03b8 on a parameter of interest (POI) \u03bc is defined as the shift \u0394\u03bc that is induced as \u03b8 is fixed and brought to its +1\u03c3 or \u22121\u03c3 post-fit values, with all other parameters profiled as normal. This is effectively a measure of the correlation between the NP and the POI, and is useful for determining which NPs have the largest effect on the POI uncertainty. It is possible to use the FitDiagnostics method of combine with the option --algo impact -P parameter to calculate the impact of a particular nuisance parameter on the parameter(s) of interest. We will use the combineTool.py script to automate the fits (see the combineTool section to check out the tool. We will use an example workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ cp HiggsAnalysis/CombinedLimit/data/tutorials/htt/125/htt_tt.txt . $ text2workspace.py htt_tt.txt -m 125 Calculating the impacts is done in a few stages. First we just fit for each POI, using the --doInitialFit option with combineTool.py , and adding the --robustFit 1 option that will be passed through to combine, combineTool.py -M Impacts -d htt_tt.root -m 125 --doInitialFit --robustFit 1 Have a look at the options as for likelihood scans when using robustFit 1 . Next we perform a similar scan for each nuisance parameter with the --doFits options, combineTool.py -M Impacts -d htt_tt.root -m 125 --robustFit 1 --doFits Note that this will run approximately 60 scans, and to speed things up the option --parallel X can be given to run X combine jobs simultaneously. The batch and grid submission methods described in the combineTool for job submission section can also be used. Once all jobs are completed the output can be collected and written into a json file: combineTool.py -M Impacts -d htt_tt.root -m 125 -o impacts.json A plot summarising the nuisance parameter values and impacts can be made with plotImpacts.py , plotImpacts.py -i impacts.json -o impacts The first page of the output is shown below. The direction of the +1\u03c3 and -1\u03c3 impacts (i.e. when the NP is moved to its +1\u03c3 or -1\u03c3 values) on the POI indicates whether the parameter is correlated or anti-correlated with it. [warning] The plot also shows the best fit value of the POI at the top and its uncertainty. You may wish to allow the range to go -ve (i.e using --setPhysicsModelParameterRanges or --rMin ) to avoid getting one-sided impacts! This script also accepts an optional json-file argument with - t which can be used to provide a dictionary for renaming parameters. A simple example would be to create a file rename.json , { \"r\" : \"#mu\" } that will rename the POI label on the plot. [info] Since combineTool accepts the usual options for combine you can also generate the impacts on an Asimov or toy dataset. The left panel in the summary plot shows the value of (\\theta-\\theta_{0})/\\Delta_{\\theta} (\\theta-\\theta_{0})/\\Delta_{\\theta} where \\theta \\theta and \\theta_{0} \\theta_{0} are the post and pre -fit values of the nuisance parameter and \\Delta_{\\theta} \\Delta_{\\theta} is the pre -fit uncertainty. The asymmetric error bars show the pre -fit uncertainty divided by the post -fit uncertainty meaning that parameters with error bars smaller than \\pm 1 \\pm 1 are constrained in the fit. As with the diffNuisances.py script, use the option --pullDef are defined (eg to show the pull instead).","title":"Nuisance parameter impacts"},{"location":"part3/nonstandard/#channel-masking","text":"The combine tool has a number of features for diagnostics and plotting results of fits. It can often be useful to turn off particular channels in a combined analysis to see how constraints/pulls can vary. It can also be helpful to plot post-fit shapes + uncertainties of a particular channel (for example a signal region) without including the constraints from the data in that region. This can in some cases be achieved by removing a specific datacard when running combineCards.py however, when doing so the information of particular nuisances and pdfs in that region will be lost. Instead, it is possible to mask that channel from the likelihood! This is acheived at the text2Workspace step using the option --channel-masks .","title":"Channel Masking"},{"location":"part3/nonstandard/#example-removing-constraints-from-the-signal-region","text":"We will take the control region example from the rate parameters tutorial from data/tutorials/rate_params/ . The first step is to combine the cards combineCards.py signal=signal_region.txt dimuon=dimuon_control_region.txt singlemuon=singlemuon_control_region.txt > datacard.txt Note that we use the directive CHANNELNAME=CHANNEL_DATACARD.txt so that the names of the channels are under our control and easier to interpret. Next, we make a workspace and tell combine to create the parameters used to mask channels text2workspace.py datacard.txt --channel-masks Now lets try a fit ignoring the signal region. We can turn off the signal region by setting the channel mask parameter on: --setParameters mask_signal=1 . Note that text2workspace has created a masking parameter for every channel with the naming scheme mask_CHANNELNAME . By default, every parameter is set to 0 so that the channel is unmasked by default. combine datacard.root -M FitDiagnostics --saveShapes --saveWithUncertainties --setParameters mask_signal=1 [warning] There will be a lot of warning from combine. This is safe to ignore as this is due to the s+b fit not converging since the free signal parameter cannot be constrained as the data in the signal region is being ignored. We can compare the background post-fit and uncertainties with and without the signal region by re-running with --setParameters mask_signal=0 (or just removing that command). Below is a comparison of the background in the signal region with and without masking the data in the signal region. We take these from the shapes folder shapes_fit_b/signal/total_background in the fitDiagnostics.root output. Clearly the background shape is different and much less constrained without including the signal region , as expected. Channel masking can be used with any method in combine.","title":"Example: removing constraints from the signal region"},{"location":"part3/nonstandard/#roomultipdf-conventional-bias-studies","text":"Several analyses within the Higgs group use a functional form to describe their background which is fit to the data (eg the Higgs to two photons (Hgg) analysis). Often however, there is some uncertainty associated to the choice of which background function to use and this choice will impact results of a fit. It is therefore often the case that in these analyses, a Bias study is performed which will indicate how much potential bias can be present given a certain choice of functional form. These studies can be conducted using combine. Below is an example script which will produce a workspace based on a simplified Hgg analysis with a single category. You will need to get the file data/tutorials/bias_studies/toyhgg_in.root . void makeRooMultiPdfWorkspace(){ // Load the combine Library gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Open the dummy H->gg workspace TFile *f_hgg = TFile::Open(\"toyhgg_in.root\"); RooWorkspace *w_hgg = (RooWorkspace*)f_hgg->Get(\"multipdf\"); // The observable (CMS_hgg_mass in the workspace) RooRealVar *mass = w_hgg->var(\"CMS_hgg_mass\"); // Get three of the functions inside, exponential, linear polynomial, power law RooAbsPdf *pdf_exp = w_hgg->pdf(\"env_pdf_1_8TeV_exp1\"); RooAbsPdf *pdf_pol = w_hgg->pdf(\"env_pdf_1_8TeV_bern2\"); RooAbsPdf *pdf_pow = w_hgg->pdf(\"env_pdf_1_8TeV_pow1\"); // Fit the functions to the data to set the \"prefit\" state (note this can and should be redone with combine when doing // bias studies as one typically throws toys from the \"best-fit\" RooDataSet *data = (RooDataSet*)w_hgg->data(\"roohist_data_mass_cat1_toy1_cutrange__CMS_hgg_mass\"); pdf_exp->fitTo(*data); // index 0 pdf_pow->fitTo(*data); // index 1 pdf_pol->fitTo(*data); // index 2 // Make a plot (data is a toy dataset) RooPlot *plot = mass->frame(); data->plotOn(plot); pdf_exp->plotOn(plot,RooFit::LineColor(kGreen)); pdf_pol->plotOn(plot,RooFit::LineColor(kBlue)); pdf_pow->plotOn(plot,RooFit::LineColor(kRed)); plot->SetTitle(\"PDF fits to toy data\"); plot->Draw(); // Make a RooCategory object. This will control which of the pdfs is \"active\" RooCategory cat(\"pdf_index\",\"Index of Pdf which is active\"); // Make a RooMultiPdf object. The order of the pdfs will be the order of their index, ie for below // 0 == exponential // 1 == linear function // 2 == powerlaw RooArgList mypdfs; mypdfs.add(*pdf_exp); mypdfs.add(*pdf_pol); mypdfs.add(*pdf_pow); RooMultiPdf multipdf(\"roomultipdf\",\"All Pdfs\",cat,mypdfs); // As usual make an extended term for the background with _norm for freely floating yield RooRealVar norm(\"roomultipdf_norm\",\"Number of background events\",0,10000); // Save to a new workspace TFile *fout = new TFile(\"background_pdfs.root\",\"RECREATE\"); RooWorkspace wout(\"backgrounds\",\"backgrounds\"); wout.import(cat); wout.import(norm); wout.import(multipdf); wout.Print(); wout.Write(); } The signal is modelled as a simple Gaussian with a width approximately that of the diphoton resolution and the background is a choice of 3 functions. An exponential, a power-law and a 2nd order polynomial. This choice is accessible inside combine through the use of the RooMultiPdf object which can switch between the functions by setting its associated index (herein called pdf_index ). This (as with all parameters in combine) is accessible via the --setPhysicsModelParameters option. To asses the bias, one can throw toys using one function and fit with another. All of this only needs to use one datacard hgg_toy_datacard.txt The bias studies are performed in two stages. The first is to generate toys using one of the functions under some value of the signal strength r (or \\mu \\mu ). This can be repeated for several values of r and also at different masses, but here the Higgs mass is fixed to 125 GeV. [warning] It is important to freeze pdf_index otherwise combine will try to iterate over the index in the frequentist fit. combine hgg_toy_datacard.txt -M GenerateOnly --setParameters pdf_index=0 --toysFrequentist -t 100 --expectSignal 1 --saveToys -m 125 --freezeParameters pdf_index Now we have 100 toys which, by setting pdf_index=0 , sets the background pdf to the exponential function i.e assumes the exponential is the true function. Note that the option --toysFrequentist is added. This first performs a fit of the pdf, assuming a signal strength of 1, to the data before generating the toys. This is the most obvious choice as to where to throw the toys from. The next step is to fit the toys under a different background pdf hypothesis. This time we set the pdf_index to be 1, the powerlaw and run fits with the FitDiagnostics method again freezing pdf_index. [warning] You may get warnings about non-accurate errors but these can be ignored and is related to the free parameters of the background pdfs which are not active. combine hgg_toy_datacard.txt -M FitDiagnostics --setParameters pdf_index=1 --toysFile higgsCombineTest.GenerateOnly.mH125.123456.root -t 100 --rMin -10 --rMax 10 --freezeParameters pdf_index In the output file fitDiagnostics.root there is a tree which contains the best fit results under the signal+background hypothesis. One measure of the bias is the pull defined as the difference between the measured value of \\mu \\mu and the generated value (here we used 1) relative to the uncertainty on \\mu \\mu . The pull distribution can be drawn and the mean provides an estimate of the pull... root -l fitDiagnostics.root tree_fit_sb->Draw(\"(r-1)/rErr>>h(20,-4,4)\") h->Fit(\"gaus\") From the fitted Gaussian, we see the mean is at +0.30 which would indicate a bias of ~30% of the uncertainty on mu from choosing the powerlaw when the true function is an exponential. [danger] If the discrete nuisance is left floating, it will be profiled by looping through the possible index values and finding the pdf which gives the best fit. This allows for the discrete profiling method to be applied for any method which involves a profiled likelihood (frequentist methods). You should be careful however since MINOS knows nothing about these nuisances and hence estimations of uncertainties will be incorrect. Instead, uncertainties from scans and limits will correctly account for these nuisances. Currently the Bayesian methods will not properly treat the nuisances so some care should be taken when interpreting Bayesian results.","title":"RooMultiPdf conventional bias studies"},{"location":"part3/nonstandard/#roosplinend-multidimensional-splines","text":"RooSplineND can be used to interpolate from tree of points to produce a continuous function in N-dimensions. This function can then be used as input to workspaces allowing for parametric rates/cross-sections/efficiencies etc OR can be used to up-scale the resolution of likelihood scans (i.e like those produced from combine) to produce smooth contours. The following script is an example of its use which produces a 2D spline from a set of points generated from a function. void splinend(){ // library containing the RooSplineND gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); TTree *tree = new TTree(\"tree_vals\",\"tree_vals\"); float xb,yb,fb; tree->Branch(\"f\",&fb,\"f/Float_t\"); tree->Branch(\"x\",&xb,\"x/Float_t\"); tree->Branch(\"y\",&yb,\"y/Float_t\"); TRandom3 *r = new TRandom3(); int nentries = 20; // just use a regular grid of 20x20 double xmin = -3.2; double xmax = 3.2; double ymin = -3.2; double ymax = 3.2; for (int n=0;n<nentries;n++){ for (int k=0;k<nentries;k++){ xb=xmin+n*((xmax-xmin)/nentries); yb=ymin+k*((ymax-ymin)/nentries); // Gaussian * cosine function radial in \"F(x^2+y^2)\" double R = (xb*xb)+(yb*yb); fb = 0.1*TMath::Exp(-1*(R)/9)*TMath::Cos(2.5*TMath::Sqrt(R)); tree->Fill(); } } // 2D graph of points in tree TGraph2D *p0 = new TGraph2D(); p0->SetMarkerSize(0.8); p0->SetMarkerStyle(20); int c0=0; for (int p=0;p<tree->GetEntries();p++){ tree->GetEntry(p); p0->SetPoint(c0,xb,yb,fb); c0++; } // ------------------------------ THIS IS WHERE WE BUILD THE SPLINE ------------------------ // // Create 2 Real-vars, one for each of the parameters of the spline // The variables MUST be named the same as the corresponding branches in the tree RooRealVar x(\"x\",\"x\",0.1,xmin,xmax); RooRealVar y(\"y\",\"y\",0.1,ymin,ymax); // And the spline - arguments are // Required -> name, title, arglist of dependants, input tree, // Optional -> function branch name, interpolation width (tunable parameter), rescale Axis bool, cutstring // The tunable parameter gives the radial basis a \"width\", over which the interpolation will be effectively taken // the reascale Axis bool (if true) will first try to rescale the points so that they are of order 1 in range // This can be helpful if for example one dimension is in much larger units than another. // The cutstring is just a ROOT string which can be used to apply cuts to the tree in case only a sub-set of the points should be used RooArgList args(x,y); RooSplineND *spline = new RooSplineND(\"spline\",\"spline\",args,tree,\"f\",1,true); // ----------------------------------------------------------------------------------------- // //TGraph *gr = spline->getGraph(\"x\",0.1); // Return 1D graph. Will be a slice of the spline for fixed y generated at steps of 0.1 // Plot the 2D spline TGraph2D *gr = new TGraph2D(); int pt = 0; for (double xx=xmin;xx<xmax;xx+=0.1){ for (double yy=xmin;yy<ymax;yy+=0.1){ x.setVal(xx); y.setVal(yy); gr->SetPoint(pt,xx,yy,spline.getVal()); pt++; } } gr->SetTitle(\"\"); gr->SetLineColor(1); //p0->SetTitle(\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\"); gr->Draw(\"surf\"); gr->GetXaxis()->SetTitle(\"x\"); gr->GetYaxis()->SetTitle(\"y\"); p0->Draw(\"Pcolsame\"); //p0->Draw(\"surfsame\"); TLegend *leg = new TLegend(0.2,0.82,0.82,0.98); leg->SetFillColor(0); leg->AddEntry(p0,\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\",\"p\"); leg->AddEntry(gr,\"RooSplineND (N=2) interpolation\",\"L\"); leg->Draw(); } Running the script will produce the following plot. The plot shows the sampled points and the spline produced from them.","title":"RooSplineND multidimensional splines"},{"location":"part3/nonstandard/#rooparametrichist-gamman-for-shapes","text":"Currently, there is no straight-forward implementation of using per-bin gmN like uncertainties with shape (histogram) analyses. Instead, it is possible to tie control regions (written as datacards) with the signal region using three methods. For analyses who take the normalisation of some process from a control region, it is possible to use either lnU or rateParam directives to float the normalisation in a correlated way of some process between two regions. Instead if each bin is intended to be determined via a control region, one can use a number of RooFit histogram pdfs/functions to accomplish this. The example below shows a simple implementation of a RooParametricHist to achieve this. copy the script below into a file called examplews.C and create the input workspace using root -l examplews.C ... void examplews(){ // As usual, load the combine library to get access to the RooParametricHist gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Output file and workspace TFile *fOut = new TFile(\"param_ws.root\",\"RECREATE\"); RooWorkspace wspace(\"wspace\",\"wspace\"); // A search in a MET tail, define MET as our variable RooRealVar met(\"met\",\"E_{T}^{miss}\",200,1000); RooArgList vars(met); // ---------------------------- SIGNAL REGION -------------------------------------------------------------------// // Make a dataset, this will be just four bins in MET. // its easiest to make this from a histogram. Set the contents to \"somehting\" TH1F data_th1(\"data_obs_SR\",\"Data observed in signal region\",4,200,1000); data_th1.SetBinContent(1,100); data_th1.SetBinContent(2,50); data_th1.SetBinContent(3,25); data_th1.SetBinContent(4,10); RooDataHist data_hist(\"data_obs_SR\",\"Data observed\",vars,&data_th1); wspace.import(data_hist); // In the signal region, our background process will be freely floating, // Create one parameter per bin representing the yield. (note of course we can have multiple processes like this) RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); // Create a RooParametericHist which contains those yields, last argument is just for the binning, // can use the data TH1 for that RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); // Always include a _norm term which should be the sum of the yields (thats how combine likes to play with pdfs) RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); // Every signal region needs a signal TH1F signal_th1(\"signal_SR\",\"Signal expected in signal region\",4,200,1000); signal_th1.SetBinContent(1,1); signal_th1.SetBinContent(2,2); signal_th1.SetBinContent(3,3); signal_th1.SetBinContent(4,8); RooDataHist signal_hist(\"signal\",\"Data observed\",vars,&signal_th1); wspace.import(signal_hist); // -------------------------------------------------------------------------------------------------------------// // ---------------------------- CONTROL REGION -----------------------------------------------------------------// TH1F data_CRth1(\"data_obs_CR\",\"Data observed in control region\",4,200,1000); data_CRth1.SetBinContent(1,200); data_CRth1.SetBinContent(2,100); data_CRth1.SetBinContent(3,50); data_CRth1.SetBinContent(4,20); RooDataHist data_CRhist(\"data_obs_CR\",\"Data observed\",vars,&data_CRth1); wspace.import(data_CRhist); // This time, the background process will be dependent on the yields of the background in the signal region. // The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control // In this example lets assume the control region is populated by the same process decaying to clean daughters with 2xBR // compared to the signal region // NB You could have a different transfer factor for each bin represented by a completely different RooRealVar // We can imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. // We need to make these nuisance parameters ourselves and give them a nominal value of 0 RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); // We would need to make the transfer factor a function of those too. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) // but note that we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could // use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); // Finally, we need to make each bin of the background in the control region a function of the background in the signal and the transfer factor // N_CR = N_SR x TF RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); // -------------------------------------------------------------------------------------------------------------// // import the pdfs wspace.import(p_bkg); wspace.import(p_bkg_norm,RooFit::RecycleConflictNodes()); wspace.import(p_CRbkg); wspace.import(p_CRbkg_norm,RooFit::RecycleConflictNodes()); fOut->cd(); wspace.Write(); // Clean up fOut->Close(); fOut->Delete(); } Lets go through what the script is doing. First, the observable for the search is the missing energy so we create a parameter to represent that. RooRealVar met(\"met\",\"E_{T}^{miss}\",200,1000); First, the following lines create a freely floating parameter for each of our bins (in this example, there are only 4 bins, defined for our observable met . RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); They are put into a list so that we can create a RooParametricHist and its normalisation from that list RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); For the control region, the background process will be dependent on the yields of the background in the signal region using a transfer factor . The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control regions. In this example lets assume the control region is populated by the same process decaying to a different final state with twice as large branching ratio compared to the one in the signal region. We could imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. We need to make nuisance parameters ourselves to model this and give them a nominal value of 0. RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); We need to make the transfer factor a function of these parameters since variations in these uncertainties will lead to variations of the transfer factor. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) but we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); Then need to make each bin of the background in the control region a function of the background in the signal and the transfer factor - i.e $N_{CR} = N_{SR} \\times TF $. RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); As before, we also need to create the RooParametricHist for this process in the control region but this time the bin yields will be the RooFormulaVars we just created instead of free floating parameters. RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); Below are datacards (for signal and control regions) which can be used in conjunction with the workspace built above. In order to \"use\" the control region, simply combine the two cards as usual using combineCards.py . Signal Region Datacard -- signal category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs signal param_ws.root wspace:data_obs_SR shapes background signal param_ws.root wspace:bkg_SR # the background model pdf which is freely floating, note other backgrounds can be added as usual shapes signal signal param_ws.root wspace:signal ------------------------------------------------------------------------------------------------------------------------------------------- bin signal observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin signal signal process background signal process 1 0 rate 1 -1 ------------------------------------------------------------------------------------------------------------------------------------------- # Normal uncertainties in the signal region lumi_8TeV lnN - 1.026 ------------------------------------------------------------------------------------------------------------------------------------------- # free floating parameters, we do not need to declare them, but its a good idea to bkg_SR_bin1 flatParam bkg_SR_bin2 flatParam bkg_SR_bin3 flatParam bkg_SR_bin4 flatParam Control Region Datacard -- control category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs control param_ws.root wspace:data_obs_CR shapes background control param_ws.root wspace:bkg_CR # the background model pdf which is dependant on that in the SR, note other backgrounds can be added as usual ------------------------------------------------------------------------------------------------------------------------------------------- bin control observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin control process background process 1 rate 1 ------------------------------------------------------------------------------------------------------------------------------------------- efficiency param 0 1 acceptance param 0 1 Note that for the control region, our nuisance parameters appear as param types so that combine will correctly constrain them. If we combine the two cards and fit the result with -M MultiDimFit -v 3 we can see that the parameters which give the rate of background in each bin of the signal region, along with the nuisance parameters and signal strength, are determined by the fit - i.e we have properly included the constraint from the control region, just as with the 1-bin gmN . acceptance = 0.00374312 +/- 0.964632 (limited) bkg_SR_bin1 = 99.9922 +/- 5.92062 (limited) bkg_SR_bin2 = 49.9951 +/- 4.13535 (limited) bkg_SR_bin3 = 24.9915 +/- 2.9267 (limited) bkg_SR_bin4 = 9.96478 +/- 2.1348 (limited) efficiency = 0.00109195 +/- 0.979334 (limited) lumi_8TeV = -0.0025911 +/- 0.994458 r = 0.00716347 +/- 12.513 (limited) The example given here is extremely basic and it should be noted that additional complexity in the transfer factors, additional uncertainties/backgrounds etc in the cards are supported as always. [danger] If trying to implement parametric uncertainties in this setup (eg on transfer factors) which are correlated with other channels and implemented separately, you MUST normalise the uncertainty effect so that the datacard line can read param name X 1 . That is the uncertainty on this parameter must be 1. Without this, there will be inconsistency with other nuisances of the same name in other channels implemented as shape or lnN .","title":"RooParametricHist gammaN for shapes"},{"location":"part3/runningthetool/","text":"How to run the tool The executable combine provided by the package allows to use the Higgs Combination Tool indicating by command line which is the method to use for limit combination and which are user's preferences to run it. To see the entire list of all available options ask for the help: combine --help The option -M allows to chose the method used. There are several groups of statistical methods: Asymptotic likelihood methods: AsymptoticLimits : limits calculated according to the asymptotic formulas in arxiv:1007.1727 Significance : simple profile likelihood approximation, for calculating significances. Bayesian methods: BayesianSimple : performing a classical numerical integration (for simple models only) MarkovChainMC : performing Markov Chain integration, for arbitrarily complex models. Frequentist or hybrid bayesian-frequentist methods: HybridNew : compute modified frequentist limits according to several possible prescriptions Fitting FitDiagnostics : performs maximum likelihood fits to extract the signal yield and provide diagnostic tools such as pre and post-fit models and correlations MultiDimFit : perform maximum likelihood fits in multiple parameters and likelihood scans Miscellaneous other modules that don't compute limits but use the same framework: GoodnessOfFit : perform a goodness of fit test for models including shape information using several GOF estimators ChannelConsistencyCheck : check how consistent are the individual channels of a combination are GenerateOnly : generate random or asimov toy datasets for use as input to other methods The command help is organized into five parts: Main options section indicates how to pass the datacard as input to the tool ( -d datacardName ) and how to choose the statistical method ( -M MethodName ) to compute a limit and level of verbosity for output -v Common statistics options include options common to different statistical methods such as -S , used to indicate wether to include or not systematics (default is 1, include them), --cl to specify the CL (default is 0.95) or -t to give the number of toy MC extractions required. Common input-output options . Is it possible to specify hypothesis point under analysis using -m or include specific string in output filename --name . Common miscellaneous options . Method specific options sections are dedicated to each method. By providing the Method name with the -M option, only the options for that specific method are shown in addition to the common options Those options reported above are just a sample of all available.The command --help provides documentation of all of them. Common command line options There are a number of useful command line options which can be used to alter the model (or parameters of the model) at run These are the most commonly used, generic options, -H : run first another faster algorithm (e.g. the ProfileLikelihood described below) to get a hint of the limit, allowing the real algorithm to converge more quickly. We strongly recommend to use this option when using MarkovChainMC, HybridNew or FeldmanCousins calculators, unless you know in which range your limit lies and you set it manually (the default is [0, 20] ) --rMax , --rMin : manually restrict the range of signal strengths to consider. For Bayesian limits with MCMC, rMax a rule of thumb is that rMax should be 3-5 times the limit (a too small value of rMax will bias your limit towards low values, since you are restricting the integration range, while a too large value will bias you to higher limits) -S : if set to 1 (default), systematic uncertainties are taken into account; if set to 0, only statistical uncertainties are considered. If your model has no systematic uncertainties, this flag has no effect. In example reported above the option S has been not specified, and systematics have been included. --setParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. This option supports the use of regexp via by replacing name with rgx{some regular expression} --setParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration). This option supports the use of regexp via by replacing name with rgx{some regular expression} --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, AsymptoticLimits, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --parameters (-P) option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above). --freezeParameters name1[,name2,...] Will freeze the parameters with the given names to their set values. This option supports the use of regexp via by replacing name with rgx{some regular expression} similarly the option --floatParameters sets the parameter floating. groups of nuisances (constrained or otherwise), as defined in the datacard, can be frozen using --freezeNuisanceGroups . You can also specify to freeze nuisances which are not contained in a particular group using a ^ before the group name ( --freezeNuisanceGroups=^group_name will freeze everything except nuisance parameters in the group \"group_name\".) all constrained nuisance parameters (not flatParam or rateParam can be set floating using --floatAllNuisances . [warning] Note that the floating/freezing options have a priority ordering from lowest to highest as floatParameters < freezeParameters < freezeNuisanceGroups < floatAllNuisances . Options with higher priority will override those with lower priority. --trackParameters name1[,name2,...] will add a branch to the output tree for each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedParam_*name *. the exact behaviour depends on the method. For example, when using MultiDimFit with the --algo scan , the value of the parameter at each point in the scan will be saved while for FitDiagnostics , only the value at the end of the method will be saved. Generic Minimizer Options Combine uses its own minimizer class which is used to steer Minuit (via RooMinimizer) named the CascadeMinimizer . This allows for sequential minimization which can help in case a particular setting/algo fails. Also, the CascadeMinimizer knows about extra features of Combine such as discrete nuisance parameters. All of the fits which are performed in several of the methods available use this minimizer. This means that the fits can be tuned using these common options, --cminPoiOnlyFit : First, perform a fit floating only the parameters of interest. This can be useful to find, roughly, where the global minimum is. --cminPreScan : Do a scan before first minimization --cminPreFit arg: If set to a value N > 0, the minimizer will perform a pre-fit with strategy (N-1) with frozen nuisance parameters. --cminApproxPreFitTolerance arg : If non-zero, do first a pre-fit with this tolerance (or 10 times the final tolerance, whichever is largest) --cminApproxPreFitStrategy arg : Strategy to use in the pre-fit. The default is strategy 0. --cminDefaultMinimizerType arg : Set the default minimizer Type. Default is Minuit2. --cminDefaultMinimizerAlgo arg : Set the default minimizer Algo. The default is Migrad --cminDefaultMinimizerTolerance arg : Set the default minimizer Tolerance, the default is 0.1 --cminDefaultMinimizerStrategy arg : Set the default minimizer Strategy between 0 (speed), 1 (balance - default ), 2 (robustness). The Minuit documentation for this is pretty sparse but in general, 0 means evaluate the function less often, while 2 will waste function calls to get precise answers. An important note is that Hesse (error/correlation estimation) will be run only if the strategy is 1 or 2. --cminFallbackAlgo arg : Provides a list of fallback algorithms if the default minimizer fails. You can provide multiple ones using the syntax is Type[,algo],strategy[:tolerance] : eg --cminFallbackAlgo Minuit2,Simplex,0:0.1 will fall back to the simplex algo of Minuit2 with strategy 0 and a tolerance 0.1, while --cminFallbackAlgo Minuit2,1 will use the default algo (migrad) of Minuit2 with strategy 1. --cminSetZeroPoint (0/1) : Set the reference of the NLL to 0 when minimizing, this can help faster convergence to the minimum if the NLL itself is large. The default is true (1), set to 0 to turn off. The allowed combinations of minimizer types and minimizer algos are as follows Minimizer Type Minimizer Algo Minuit Migrad , Simplex , Combined , Scan Minuit2 Migrad , Simplex , Combined , Scan GSLMultiMin ConjugateFR , ConjugatePR , BFGS , BFGS2 , SteepestDescent More of these options can be found in the Cascade Minimizer options section when running --help . Output from combine Most methods will print the results of the computation to the screen, however, in addition, combine will also produce a root file containing a tree called limit with these results. The name of this file will be of the format, higgsCombineTest.MethodName.mH$MASS.[word$WORD].root where $WORD is any user defined keyword from the datacard which has been set to a particular value. A few command line options of combine can be used to control this output: The option -n allows you to specify part of the name of the rootfile. e.g. if you do -n HWW the roofile will be called higgsCombineHWW.... instead of higgsCombineTest The option -m allows you to specify the higgs boson mass, which gets written in the filename and also in the tree (this simplifies the bookeeping because you can merge together multiple trees corresponding to different higgs masses using hadd and then use the tree to plot the value of the limit vs mass) (default is m=120) The option -s allows to specify the seed (eg -s 12345 ) used in toy generation. If this option is given, the name of the file will be extended by this seed, eg higgsCombineTest.AsymptoticLimits.mH120.12345.root The option --keyword-value allows you to specify the value of a keyword in the datacard such that $WORD (in the datacard) will be given the value of VALUE in the command --keyword-value WORD=VALUE , eg higgsCombineTest.AsymptoticLimits.mH120.WORDVALUE.12345.root The output file will contain a TDirectory named toys , which will be empty if no toys are generated (see below for details) and a TTree called limit with the following branches; Branch name Type Description limit Double_t Main result of combine run with method dependent meaning limitErr Double_t Estimated uncertainty on the result mh Double_t Value of MH , specified with -m option syst Int_t Whether or not systematics (constrained nuisances) are included (floating) as determined with -S 0/1 iToy Int_t Toy number identifier if running with -t iSeed Int_t Seed specified with -s t_cpu Float_t Estimated CPU time for algorithm t_real Float_t Estimated real time for algorithm quantileExpected Float_t Quantile identifier for methods which calculated expected (quantiles) and observed results (eg conversions from \\Delta\\ln L \\Delta\\ln L values) with method dependent meaning. Negative values are reserved for entries which do not related to quantiles of a calculation with the default being set to -1 (usually meaning the observed result). The value of any user defined keyword $WORD which is set using keyword-value described above will also be included as a branch with type string named WORD . The option can be repeated multiple times for multiple keywords. In some cases, the precise meanings of the branches will depend on the Method being used, which is included in this documentation. Toy data generation By default, each of these methods will be run using the observed data as the input. In several cases (as detailed below), it might be useful to run the tool using toy datasets, including Asimov data. The option -t is used to specify to combine to first generate a toy dataset(s) which will be used in replacement of the real data. There are two versions of this, -t N with N > 0. Combine will generate N toy datasets from the model and re-run the method once per toy. The seed for the toy generation can be modified with the option -s (use -s -1 for a random seed). The output file will contain one entry in the tree for each of these toys. -t -1 will produce an Asimov dataset in which statistical fluctuations are suppressed. The procedure to generate this Asimov dataset depends on which type of analysis you are using, see below for details. The output file will contain the toys (as RooDataSets for the observables, including global observables) in the toys directory if the option --saveToys is provided. If you include this option, the limit TTree in the output will have an entry corresponding to the state of the POI used for the generation of the toy, with the value of quantileExpected set to -2 . You can add additional branches using the --trackParameters option as described in the common command line options section above. [warning] The default values of the nuisance parameters (or any parameter) are used to generate the toy. This means that if, for example, you are using parametric shapes and the parameters inside the workspace are set to arbitrary values, those arbitrary values will be used to generate the toy. This behaviour can be modified through the use of the option --setParameters x=value_x,y=value_y... which will set the values of the parameters ( x and y ) before toy generation. You can also load a snap-shot from a previous fit to set the nuisances to their post-fit values (see below). Asimov datasets If you are using wither -t -1 or using AsymptoticLimits , combine will calculate results based on an Asimov dataset. For counting experiments, the Asimov data will just be set to the total number of expected events (given the values of the nuisance parameters and POIs of the model) For shape analyses with templates, the Asimov dataset will be constructed as a histogram using the same binning which is defined for your analysis. If your model uses parametric shapes (for example when you are using binned data, there are some options as to what Asimov dataset to produce. By default , combine will produce the Asimov data as a histogram using the binning which is associated to each observable (ie as set using RooRealVar::setBins ). If this binning doesn't exist, combine will guess a suitable binning - it is therefore best to use RooRealVar::setBins to associate a binning to each observable, even if your data is unbinned, if you intend to use Asimov datasets. You can also ask combine to use a Pseudo-Asimov dataset, which is created from many weighted unbinned events. Setting --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta with \\beta>0 \\beta>0 will trigger the internal logic of whether to produce a Pseudo-Asimov dataset. This logic is as follows; For each observable in your dataset, the number of bins, n_{b} n_{b} is determined either from the value of RooRealVar::getBins if it exists or assumed to be 100. If N_{b}=\\prod_{b}n_{b}>5000 N_{b}=\\prod_{b}n_{b}>5000 , the number of expected events N_{ev} N_{ev} is determined. Note if you are combining multiple channels, N_{ev} N_{ev} refers to the number of expected events in a single channel, the logic is separate for each channel. If N_{ev}/N_{b}<0.01 N_{ev}/N_{b}<0.01 then a Pseudo-Asimov dataset is created with the number of events equal to \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} . If N_{ev}/N_{b}\\geq 0.01 N_{ev}/N_{b}\\geq 0.01 , then a normal Asimov dataset is produced. If N_{b}\\leq 5000 N_{b}\\leq 5000 then a normal Asimov dataset will be produced The production of a Pseudo-Asimov dataset can be forced by using the option --X-rtd TMCSO_PseudoAsimov=X where X>0 will determine the number of weighted events for the Pseudo-Asimov dataset. You should try different values of X since larger values leads to more events in the Pseudo-Asimov dataset resulting in higher precision but in general the fit will be slower. Note that if yu You can turn off the internal logic by setting --X-rtd TMCSO_AdaptivePseudoAsimov=0 --X-rtd TMCSO_PseudoAsimov=0 thereby forcing histograms to be generated. [info] If you set --X-rtd TMCSO_PseudoAsimov=X with X>0 and also turn on --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta , with \\beta>0 \\beta>0 , the internal logic will be used but this time the default will be to generate Pseudo-Asimov datasets, rather than the normal Asimov ones. Nuisance parameter generation The default method of dealing with systematics is to generate random values (around their nominal values, see above) for the nuisance parameters, according to their prior pdfs centred around their default values, before generating the data. The unconstrained nuisance parameters (eg flatParam or rateParam ) or those with flat priors are not randomised before the data generation. The following are options which define how the toys will be generated, --toysNoSystematics the nuisance parameters in each toy are not randomised when generating the toy datasets - i.e their nominal values are used to generate the data. Note that for methods which profile (fit) the nuisances, the parameters are still floating when evaluating the likelihood. --toysFrequentist the nuisance parameters in each toy are set to their nominal values which are obtained after fitting first to the data , with POIs fixed, before generating the data. For evaluating likelihoods, the constraint terms are instead randomised within their Gaussian constraint pdfs around the post-fit nuisance parameter values. If you are using toysFrequentist , be aware that the values set by --setParameters will be ignored for the toy generation as the post-fit values will instead be used (except for any parameter which is also a parameter of interest). You can override this behaviour and choose the nominal values for toy generation for any parameter by adding the option --bypassFrequentistFit which will skip the initial fit to data or by loading a snapshot (see below). [warning] The methods such as AsymptoticLimits and HybridNew --LHCmode LHC-limits , the \"nominal\" nuisance parameter values are taken from fits to the data and are therefore not \"blind\" to the observed data by default (following the fully frequentist paradigm). See the detailed documentation on these methods for avoiding this and running in a completely \"blind\" mode. Generate only It is also possible to generate the toys first and then feed them to the Methods in combine. This can be done using -M GenerateOnly --saveToys . The toys can then be read and used with the other methods by specifying --toysFile=higgsCombineTest.GenerateOnly... and using the same options for the toy generation. [warning] Some Methods also use toys within the method itself (eg AsymptoticLimits and HybridNew ). For these, you should not specify the toy generation with -t or the options above and instead follow the specific instructions. Loading snapshots Snapshots from workspaces can be loaded and used in order to generate toys using the option --snapshotName <name of snapshot> . This will first set the parameters to the values in the snapshot before any other parameter options are set and toys are generated. See the section on saving post-fit workspaces for creating workspaces with post-fit snapshots from MultiDimFit . Here are a few examples of calculations with toys from post-fit workspaces using a workspace with r, m_{H} r, m_{H} as parameters of interest Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0 , m=best fit MH , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected r uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0, m=128.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeParameters MH Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=0.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M AsymptoticLimits --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeParameters MH combineTool for job submission For longer tasks which cannot be run locally, several methods in combine can be split to run on the LSF batch or the Grid . The splitting and submission is handled using the combineTool (see this getting started section to get the tool) Submission to the LSF Batch The syntax for running on the batch with the tool is combineTool.py -M ALGO [options] --job-mode lxbatch --sub-opts=\"-q QUEUE\" --task-name NAME [--dry-run] with options being the usual list of combine options. The help option -h will give a list of both combine and combineTool sets of options. This can be used with several different methods from combine . The --dry-run option will show what will be run without actually doing so / submitting the jobs. For example, to generate toys (eg for use with limit setting) users running on lxplus at CERN the lxbatch mode can be used eg combineTool.py -d workspace.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveHybridResult -m 125 --job-mode lxbatch --task-name lxbatch-test --sub-opts='-q 1nd' The --singlePoint option is over-ridden so that this will produce a script for each value of the POI in the range 0.2 to 2.0 in steps of 0.05. You can merge multiple points into a script using --merge - e.g adding --merge 10 to the above command will mean that each job contains at most 10 of the values. The scripts are labelled by the --task-name option. These will be submitted directly to lxbatch adding any options in --sub-opts to the bsub call. The jobs will run and produce output in the current directory . Below is an example for splitting points in a multi-dimensional likelihood scan. Splitting jobs for a multi-dimensional likelihood scan The option --split-points issues the command to split the jobs for MultiDimFit when using --algo grid . The following example will split the jobs such that there are 10 points in each of the jobs, which will be submitted to the 8nh queue. combineTool.py datacard.txt -M MultiDimFit --algo grid --points 50 --rMin 0 --rMax 1 --job-mode lxbatch --split-points 10 --sub-opts='-q 8nh' --task-name mytask -n mytask Remember, any usual options (such as redefining POIs or freezing parameters) are passed to combine and can be added to the command line for combineTool . [info] The option -n NAME should be included to avoid overwriting output files as the jobs will be run inside the directory from which the command is issued. Running combine jobs on the Grid For more CPU-intensive tasks, for example determining limits for complex models using toys, it is generally not feasible to compute all the results interactively. Instead, these jobs can be submitted to the Grid. In this example we will use the HybridNew method of combine to determine an upper limit for a sub-channel of the Run 1 SM H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau analysis. For full documentation, see the section on computing limits with toys . With this model it would take too long to find the limit in one go, so instead we create a set of jobs in which each one throws toys and builds up the test statistic distributions for a fixed value of the signal strength. These jobs can then be submitted to a batch system or to the Grid using crab3 . From the set of output distributions it is possible to extract the expected and observed limits. For this we will use combineTool.py First we need to build a workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ text2workspace.py data/tutorials/htt/125/htt_mt.txt -m 125 $ mv data/tutorials/htt/125/htt_mt.root ./ To get an idea of the range of signal strength values we will need to build test-statistic distributions for we will first use the AsymptoticLimits method of combine, $ combine -M Asymptotic htt_mt.root -m 125 << Combine >> [...] -- AsymptoticLimits (CLs) -- Observed Limit: r < 1.7384 Expected 2.5%: r < 0.4394 Expected 16.0%: r < 0.5971 Expected 50.0%: r < 0.8555 Expected 84.0%: r < 1.2340 Expected 97.5%: r < 1.7200 Based on this, a range of 0.2 to 2.0 should be suitable. We can use the same command for generating the distribution of test statistics with combineTool . The --singlePoint option is now enhanced to support expressions that generate a set of calls to combine with different values. The accepted syntax is of the form MIN:MAX:STEPSIZE , and multiple comma-separated expressions can be specified. The script also adds an option --dry-run which will not actually call combine but just prints out the commands that would be run, e.g, combineTool.py -M HybridNew -d htt_mt.root --LHCmode LHC-limits --singlePoint 0.2:2.0:0.2 -T 2000 -s -1 --saveToys --saveHybridResult -m 125 --dry-run ... [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.2 -n .Test.POINT.0.2 [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.4 -n .Test.POINT.0.4 [...] [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 2.0 -n .Test.POINT.2.0 When the --dry-run option is removed each command will be run in sequence. Grid submission Submission to the grid with crab3 works in a similar way. Before doing so ensure that the crab3 environment has been sourced, then for compatibility reasons source the CMSSW environment again. We will use the example of generating a grid of test-statistic distributions for limits. $ source /cvmfs/cms.cern.ch/crab3/crab.sh; cmsenv $ combineTool.py -d htt_mt.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveToys --saveHybridResult -m 125 --job-mode crab3 --task-name grid-test --custom-crab custom_crab.py The option --custom-crab should point to a python file python containing a function of the form custom_crab(config) that will be used to modify the default crab configuration. You can use this to set the output site to your local grid site, or modify other options such as the voRole, or the site blacklist/whitelist. For example def custom_crab(config): print '>> Customising the crab config' config.Site.storageSite = 'T2_CH_CERN' config.Site.blacklist = ['SOME_SITE', 'SOME_OTHER_SITE'] Again it is possible to use the option --dry-run to see what the complete crab config will look like before actually submitted it. Once submitted the progress can be monitored using the standard crab commands. When all jobs are completed copy the output from your sites storage element to the local output folder. $ crab getoutput -d crab_grid-test # Now we have to un-tar the output files $ cd crab_grid-test/results/ $ for f in *.tar; do tar xf $f; done $ mv higgsCombine*.root ../../ $ cd ../../ These output files should be combined with hadd , after which we invoke combine as usual to calculate observed and expected limits from the merged grid as usual.","title":"Running the tool"},{"location":"part3/runningthetool/#how-to-run-the-tool","text":"The executable combine provided by the package allows to use the Higgs Combination Tool indicating by command line which is the method to use for limit combination and which are user's preferences to run it. To see the entire list of all available options ask for the help: combine --help The option -M allows to chose the method used. There are several groups of statistical methods: Asymptotic likelihood methods: AsymptoticLimits : limits calculated according to the asymptotic formulas in arxiv:1007.1727 Significance : simple profile likelihood approximation, for calculating significances. Bayesian methods: BayesianSimple : performing a classical numerical integration (for simple models only) MarkovChainMC : performing Markov Chain integration, for arbitrarily complex models. Frequentist or hybrid bayesian-frequentist methods: HybridNew : compute modified frequentist limits according to several possible prescriptions Fitting FitDiagnostics : performs maximum likelihood fits to extract the signal yield and provide diagnostic tools such as pre and post-fit models and correlations MultiDimFit : perform maximum likelihood fits in multiple parameters and likelihood scans Miscellaneous other modules that don't compute limits but use the same framework: GoodnessOfFit : perform a goodness of fit test for models including shape information using several GOF estimators ChannelConsistencyCheck : check how consistent are the individual channels of a combination are GenerateOnly : generate random or asimov toy datasets for use as input to other methods The command help is organized into five parts: Main options section indicates how to pass the datacard as input to the tool ( -d datacardName ) and how to choose the statistical method ( -M MethodName ) to compute a limit and level of verbosity for output -v Common statistics options include options common to different statistical methods such as -S , used to indicate wether to include or not systematics (default is 1, include them), --cl to specify the CL (default is 0.95) or -t to give the number of toy MC extractions required. Common input-output options . Is it possible to specify hypothesis point under analysis using -m or include specific string in output filename --name . Common miscellaneous options . Method specific options sections are dedicated to each method. By providing the Method name with the -M option, only the options for that specific method are shown in addition to the common options Those options reported above are just a sample of all available.The command --help provides documentation of all of them.","title":"How to run the tool"},{"location":"part3/runningthetool/#common-command-line-options","text":"There are a number of useful command line options which can be used to alter the model (or parameters of the model) at run These are the most commonly used, generic options, -H : run first another faster algorithm (e.g. the ProfileLikelihood described below) to get a hint of the limit, allowing the real algorithm to converge more quickly. We strongly recommend to use this option when using MarkovChainMC, HybridNew or FeldmanCousins calculators, unless you know in which range your limit lies and you set it manually (the default is [0, 20] ) --rMax , --rMin : manually restrict the range of signal strengths to consider. For Bayesian limits with MCMC, rMax a rule of thumb is that rMax should be 3-5 times the limit (a too small value of rMax will bias your limit towards low values, since you are restricting the integration range, while a too large value will bias you to higher limits) -S : if set to 1 (default), systematic uncertainties are taken into account; if set to 0, only statistical uncertainties are considered. If your model has no systematic uncertainties, this flag has no effect. In example reported above the option S has been not specified, and systematics have been included. --setParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. This option supports the use of regexp via by replacing name with rgx{some regular expression} --setParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration). This option supports the use of regexp via by replacing name with rgx{some regular expression} --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, AsymptoticLimits, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --parameters (-P) option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above). --freezeParameters name1[,name2,...] Will freeze the parameters with the given names to their set values. This option supports the use of regexp via by replacing name with rgx{some regular expression} similarly the option --floatParameters sets the parameter floating. groups of nuisances (constrained or otherwise), as defined in the datacard, can be frozen using --freezeNuisanceGroups . You can also specify to freeze nuisances which are not contained in a particular group using a ^ before the group name ( --freezeNuisanceGroups=^group_name will freeze everything except nuisance parameters in the group \"group_name\".) all constrained nuisance parameters (not flatParam or rateParam can be set floating using --floatAllNuisances . [warning] Note that the floating/freezing options have a priority ordering from lowest to highest as floatParameters < freezeParameters < freezeNuisanceGroups < floatAllNuisances . Options with higher priority will override those with lower priority. --trackParameters name1[,name2,...] will add a branch to the output tree for each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedParam_*name *. the exact behaviour depends on the method. For example, when using MultiDimFit with the --algo scan , the value of the parameter at each point in the scan will be saved while for FitDiagnostics , only the value at the end of the method will be saved.","title":"Common command line options"},{"location":"part3/runningthetool/#generic-minimizer-options","text":"Combine uses its own minimizer class which is used to steer Minuit (via RooMinimizer) named the CascadeMinimizer . This allows for sequential minimization which can help in case a particular setting/algo fails. Also, the CascadeMinimizer knows about extra features of Combine such as discrete nuisance parameters. All of the fits which are performed in several of the methods available use this minimizer. This means that the fits can be tuned using these common options, --cminPoiOnlyFit : First, perform a fit floating only the parameters of interest. This can be useful to find, roughly, where the global minimum is. --cminPreScan : Do a scan before first minimization --cminPreFit arg: If set to a value N > 0, the minimizer will perform a pre-fit with strategy (N-1) with frozen nuisance parameters. --cminApproxPreFitTolerance arg : If non-zero, do first a pre-fit with this tolerance (or 10 times the final tolerance, whichever is largest) --cminApproxPreFitStrategy arg : Strategy to use in the pre-fit. The default is strategy 0. --cminDefaultMinimizerType arg : Set the default minimizer Type. Default is Minuit2. --cminDefaultMinimizerAlgo arg : Set the default minimizer Algo. The default is Migrad --cminDefaultMinimizerTolerance arg : Set the default minimizer Tolerance, the default is 0.1 --cminDefaultMinimizerStrategy arg : Set the default minimizer Strategy between 0 (speed), 1 (balance - default ), 2 (robustness). The Minuit documentation for this is pretty sparse but in general, 0 means evaluate the function less often, while 2 will waste function calls to get precise answers. An important note is that Hesse (error/correlation estimation) will be run only if the strategy is 1 or 2. --cminFallbackAlgo arg : Provides a list of fallback algorithms if the default minimizer fails. You can provide multiple ones using the syntax is Type[,algo],strategy[:tolerance] : eg --cminFallbackAlgo Minuit2,Simplex,0:0.1 will fall back to the simplex algo of Minuit2 with strategy 0 and a tolerance 0.1, while --cminFallbackAlgo Minuit2,1 will use the default algo (migrad) of Minuit2 with strategy 1. --cminSetZeroPoint (0/1) : Set the reference of the NLL to 0 when minimizing, this can help faster convergence to the minimum if the NLL itself is large. The default is true (1), set to 0 to turn off. The allowed combinations of minimizer types and minimizer algos are as follows Minimizer Type Minimizer Algo Minuit Migrad , Simplex , Combined , Scan Minuit2 Migrad , Simplex , Combined , Scan GSLMultiMin ConjugateFR , ConjugatePR , BFGS , BFGS2 , SteepestDescent More of these options can be found in the Cascade Minimizer options section when running --help .","title":"Generic Minimizer Options"},{"location":"part3/runningthetool/#output-from-combine","text":"Most methods will print the results of the computation to the screen, however, in addition, combine will also produce a root file containing a tree called limit with these results. The name of this file will be of the format, higgsCombineTest.MethodName.mH$MASS.[word$WORD].root where $WORD is any user defined keyword from the datacard which has been set to a particular value. A few command line options of combine can be used to control this output: The option -n allows you to specify part of the name of the rootfile. e.g. if you do -n HWW the roofile will be called higgsCombineHWW.... instead of higgsCombineTest The option -m allows you to specify the higgs boson mass, which gets written in the filename and also in the tree (this simplifies the bookeeping because you can merge together multiple trees corresponding to different higgs masses using hadd and then use the tree to plot the value of the limit vs mass) (default is m=120) The option -s allows to specify the seed (eg -s 12345 ) used in toy generation. If this option is given, the name of the file will be extended by this seed, eg higgsCombineTest.AsymptoticLimits.mH120.12345.root The option --keyword-value allows you to specify the value of a keyword in the datacard such that $WORD (in the datacard) will be given the value of VALUE in the command --keyword-value WORD=VALUE , eg higgsCombineTest.AsymptoticLimits.mH120.WORDVALUE.12345.root The output file will contain a TDirectory named toys , which will be empty if no toys are generated (see below for details) and a TTree called limit with the following branches; Branch name Type Description limit Double_t Main result of combine run with method dependent meaning limitErr Double_t Estimated uncertainty on the result mh Double_t Value of MH , specified with -m option syst Int_t Whether or not systematics (constrained nuisances) are included (floating) as determined with -S 0/1 iToy Int_t Toy number identifier if running with -t iSeed Int_t Seed specified with -s t_cpu Float_t Estimated CPU time for algorithm t_real Float_t Estimated real time for algorithm quantileExpected Float_t Quantile identifier for methods which calculated expected (quantiles) and observed results (eg conversions from \\Delta\\ln L \\Delta\\ln L values) with method dependent meaning. Negative values are reserved for entries which do not related to quantiles of a calculation with the default being set to -1 (usually meaning the observed result). The value of any user defined keyword $WORD which is set using keyword-value described above will also be included as a branch with type string named WORD . The option can be repeated multiple times for multiple keywords. In some cases, the precise meanings of the branches will depend on the Method being used, which is included in this documentation.","title":"Output from combine"},{"location":"part3/runningthetool/#toy-data-generation","text":"By default, each of these methods will be run using the observed data as the input. In several cases (as detailed below), it might be useful to run the tool using toy datasets, including Asimov data. The option -t is used to specify to combine to first generate a toy dataset(s) which will be used in replacement of the real data. There are two versions of this, -t N with N > 0. Combine will generate N toy datasets from the model and re-run the method once per toy. The seed for the toy generation can be modified with the option -s (use -s -1 for a random seed). The output file will contain one entry in the tree for each of these toys. -t -1 will produce an Asimov dataset in which statistical fluctuations are suppressed. The procedure to generate this Asimov dataset depends on which type of analysis you are using, see below for details. The output file will contain the toys (as RooDataSets for the observables, including global observables) in the toys directory if the option --saveToys is provided. If you include this option, the limit TTree in the output will have an entry corresponding to the state of the POI used for the generation of the toy, with the value of quantileExpected set to -2 . You can add additional branches using the --trackParameters option as described in the common command line options section above. [warning] The default values of the nuisance parameters (or any parameter) are used to generate the toy. This means that if, for example, you are using parametric shapes and the parameters inside the workspace are set to arbitrary values, those arbitrary values will be used to generate the toy. This behaviour can be modified through the use of the option --setParameters x=value_x,y=value_y... which will set the values of the parameters ( x and y ) before toy generation. You can also load a snap-shot from a previous fit to set the nuisances to their post-fit values (see below).","title":"Toy data generation"},{"location":"part3/runningthetool/#asimov-datasets","text":"If you are using wither -t -1 or using AsymptoticLimits , combine will calculate results based on an Asimov dataset. For counting experiments, the Asimov data will just be set to the total number of expected events (given the values of the nuisance parameters and POIs of the model) For shape analyses with templates, the Asimov dataset will be constructed as a histogram using the same binning which is defined for your analysis. If your model uses parametric shapes (for example when you are using binned data, there are some options as to what Asimov dataset to produce. By default , combine will produce the Asimov data as a histogram using the binning which is associated to each observable (ie as set using RooRealVar::setBins ). If this binning doesn't exist, combine will guess a suitable binning - it is therefore best to use RooRealVar::setBins to associate a binning to each observable, even if your data is unbinned, if you intend to use Asimov datasets. You can also ask combine to use a Pseudo-Asimov dataset, which is created from many weighted unbinned events. Setting --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta with \\beta>0 \\beta>0 will trigger the internal logic of whether to produce a Pseudo-Asimov dataset. This logic is as follows; For each observable in your dataset, the number of bins, n_{b} n_{b} is determined either from the value of RooRealVar::getBins if it exists or assumed to be 100. If N_{b}=\\prod_{b}n_{b}>5000 N_{b}=\\prod_{b}n_{b}>5000 , the number of expected events N_{ev} N_{ev} is determined. Note if you are combining multiple channels, N_{ev} N_{ev} refers to the number of expected events in a single channel, the logic is separate for each channel. If N_{ev}/N_{b}<0.01 N_{ev}/N_{b}<0.01 then a Pseudo-Asimov dataset is created with the number of events equal to \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} . If N_{ev}/N_{b}\\geq 0.01 N_{ev}/N_{b}\\geq 0.01 , then a normal Asimov dataset is produced. If N_{b}\\leq 5000 N_{b}\\leq 5000 then a normal Asimov dataset will be produced The production of a Pseudo-Asimov dataset can be forced by using the option --X-rtd TMCSO_PseudoAsimov=X where X>0 will determine the number of weighted events for the Pseudo-Asimov dataset. You should try different values of X since larger values leads to more events in the Pseudo-Asimov dataset resulting in higher precision but in general the fit will be slower. Note that if yu You can turn off the internal logic by setting --X-rtd TMCSO_AdaptivePseudoAsimov=0 --X-rtd TMCSO_PseudoAsimov=0 thereby forcing histograms to be generated. [info] If you set --X-rtd TMCSO_PseudoAsimov=X with X>0 and also turn on --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta , with \\beta>0 \\beta>0 , the internal logic will be used but this time the default will be to generate Pseudo-Asimov datasets, rather than the normal Asimov ones.","title":"Asimov datasets"},{"location":"part3/runningthetool/#nuisance-parameter-generation","text":"The default method of dealing with systematics is to generate random values (around their nominal values, see above) for the nuisance parameters, according to their prior pdfs centred around their default values, before generating the data. The unconstrained nuisance parameters (eg flatParam or rateParam ) or those with flat priors are not randomised before the data generation. The following are options which define how the toys will be generated, --toysNoSystematics the nuisance parameters in each toy are not randomised when generating the toy datasets - i.e their nominal values are used to generate the data. Note that for methods which profile (fit) the nuisances, the parameters are still floating when evaluating the likelihood. --toysFrequentist the nuisance parameters in each toy are set to their nominal values which are obtained after fitting first to the data , with POIs fixed, before generating the data. For evaluating likelihoods, the constraint terms are instead randomised within their Gaussian constraint pdfs around the post-fit nuisance parameter values. If you are using toysFrequentist , be aware that the values set by --setParameters will be ignored for the toy generation as the post-fit values will instead be used (except for any parameter which is also a parameter of interest). You can override this behaviour and choose the nominal values for toy generation for any parameter by adding the option --bypassFrequentistFit which will skip the initial fit to data or by loading a snapshot (see below). [warning] The methods such as AsymptoticLimits and HybridNew --LHCmode LHC-limits , the \"nominal\" nuisance parameter values are taken from fits to the data and are therefore not \"blind\" to the observed data by default (following the fully frequentist paradigm). See the detailed documentation on these methods for avoiding this and running in a completely \"blind\" mode.","title":"Nuisance parameter generation"},{"location":"part3/runningthetool/#generate-only","text":"It is also possible to generate the toys first and then feed them to the Methods in combine. This can be done using -M GenerateOnly --saveToys . The toys can then be read and used with the other methods by specifying --toysFile=higgsCombineTest.GenerateOnly... and using the same options for the toy generation. [warning] Some Methods also use toys within the method itself (eg AsymptoticLimits and HybridNew ). For these, you should not specify the toy generation with -t or the options above and instead follow the specific instructions.","title":"Generate only"},{"location":"part3/runningthetool/#loading-snapshots","text":"Snapshots from workspaces can be loaded and used in order to generate toys using the option --snapshotName <name of snapshot> . This will first set the parameters to the values in the snapshot before any other parameter options are set and toys are generated. See the section on saving post-fit workspaces for creating workspaces with post-fit snapshots from MultiDimFit . Here are a few examples of calculations with toys from post-fit workspaces using a workspace with r, m_{H} r, m_{H} as parameters of interest Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0 , m=best fit MH , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected r uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0, m=128.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeParameters MH Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=0.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M AsymptoticLimits --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeParameters MH","title":"Loading snapshots"},{"location":"part3/runningthetool/#combinetool-for-job-submission","text":"For longer tasks which cannot be run locally, several methods in combine can be split to run on the LSF batch or the Grid . The splitting and submission is handled using the combineTool (see this getting started section to get the tool)","title":"combineTool for job submission"},{"location":"part3/runningthetool/#submission-to-the-lsf-batch","text":"The syntax for running on the batch with the tool is combineTool.py -M ALGO [options] --job-mode lxbatch --sub-opts=\"-q QUEUE\" --task-name NAME [--dry-run] with options being the usual list of combine options. The help option -h will give a list of both combine and combineTool sets of options. This can be used with several different methods from combine . The --dry-run option will show what will be run without actually doing so / submitting the jobs. For example, to generate toys (eg for use with limit setting) users running on lxplus at CERN the lxbatch mode can be used eg combineTool.py -d workspace.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveHybridResult -m 125 --job-mode lxbatch --task-name lxbatch-test --sub-opts='-q 1nd' The --singlePoint option is over-ridden so that this will produce a script for each value of the POI in the range 0.2 to 2.0 in steps of 0.05. You can merge multiple points into a script using --merge - e.g adding --merge 10 to the above command will mean that each job contains at most 10 of the values. The scripts are labelled by the --task-name option. These will be submitted directly to lxbatch adding any options in --sub-opts to the bsub call. The jobs will run and produce output in the current directory . Below is an example for splitting points in a multi-dimensional likelihood scan.","title":"Submission to the LSF Batch"},{"location":"part3/runningthetool/#splitting-jobs-for-a-multi-dimensional-likelihood-scan","text":"The option --split-points issues the command to split the jobs for MultiDimFit when using --algo grid . The following example will split the jobs such that there are 10 points in each of the jobs, which will be submitted to the 8nh queue. combineTool.py datacard.txt -M MultiDimFit --algo grid --points 50 --rMin 0 --rMax 1 --job-mode lxbatch --split-points 10 --sub-opts='-q 8nh' --task-name mytask -n mytask Remember, any usual options (such as redefining POIs or freezing parameters) are passed to combine and can be added to the command line for combineTool . [info] The option -n NAME should be included to avoid overwriting output files as the jobs will be run inside the directory from which the command is issued.","title":"Splitting jobs for a multi-dimensional likelihood scan"},{"location":"part3/runningthetool/#running-combine-jobs-on-the-grid","text":"For more CPU-intensive tasks, for example determining limits for complex models using toys, it is generally not feasible to compute all the results interactively. Instead, these jobs can be submitted to the Grid. In this example we will use the HybridNew method of combine to determine an upper limit for a sub-channel of the Run 1 SM H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau analysis. For full documentation, see the section on computing limits with toys . With this model it would take too long to find the limit in one go, so instead we create a set of jobs in which each one throws toys and builds up the test statistic distributions for a fixed value of the signal strength. These jobs can then be submitted to a batch system or to the Grid using crab3 . From the set of output distributions it is possible to extract the expected and observed limits. For this we will use combineTool.py First we need to build a workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ text2workspace.py data/tutorials/htt/125/htt_mt.txt -m 125 $ mv data/tutorials/htt/125/htt_mt.root ./ To get an idea of the range of signal strength values we will need to build test-statistic distributions for we will first use the AsymptoticLimits method of combine, $ combine -M Asymptotic htt_mt.root -m 125 << Combine >> [...] -- AsymptoticLimits (CLs) -- Observed Limit: r < 1.7384 Expected 2.5%: r < 0.4394 Expected 16.0%: r < 0.5971 Expected 50.0%: r < 0.8555 Expected 84.0%: r < 1.2340 Expected 97.5%: r < 1.7200 Based on this, a range of 0.2 to 2.0 should be suitable. We can use the same command for generating the distribution of test statistics with combineTool . The --singlePoint option is now enhanced to support expressions that generate a set of calls to combine with different values. The accepted syntax is of the form MIN:MAX:STEPSIZE , and multiple comma-separated expressions can be specified. The script also adds an option --dry-run which will not actually call combine but just prints out the commands that would be run, e.g, combineTool.py -M HybridNew -d htt_mt.root --LHCmode LHC-limits --singlePoint 0.2:2.0:0.2 -T 2000 -s -1 --saveToys --saveHybridResult -m 125 --dry-run ... [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.2 -n .Test.POINT.0.2 [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.4 -n .Test.POINT.0.4 [...] [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 2.0 -n .Test.POINT.2.0 When the --dry-run option is removed each command will be run in sequence.","title":"Running combine jobs on the Grid"},{"location":"part3/runningthetool/#grid-submission","text":"Submission to the grid with crab3 works in a similar way. Before doing so ensure that the crab3 environment has been sourced, then for compatibility reasons source the CMSSW environment again. We will use the example of generating a grid of test-statistic distributions for limits. $ source /cvmfs/cms.cern.ch/crab3/crab.sh; cmsenv $ combineTool.py -d htt_mt.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveToys --saveHybridResult -m 125 --job-mode crab3 --task-name grid-test --custom-crab custom_crab.py The option --custom-crab should point to a python file python containing a function of the form custom_crab(config) that will be used to modify the default crab configuration. You can use this to set the output site to your local grid site, or modify other options such as the voRole, or the site blacklist/whitelist. For example def custom_crab(config): print '>> Customising the crab config' config.Site.storageSite = 'T2_CH_CERN' config.Site.blacklist = ['SOME_SITE', 'SOME_OTHER_SITE'] Again it is possible to use the option --dry-run to see what the complete crab config will look like before actually submitted it. Once submitted the progress can be monitored using the standard crab commands. When all jobs are completed copy the output from your sites storage element to the local output folder. $ crab getoutput -d crab_grid-test # Now we have to un-tar the output files $ cd crab_grid-test/results/ $ for f in *.tar; do tar xf $f; done $ mv higgsCombine*.root ../../ $ cd ../../ These output files should be combined with hadd , after which we invoke combine as usual to calculate observed and expected limits from the merged grid as usual.","title":"Grid submission"},{"location":"part4/usefullinks/","text":"Useful links and further reading Tutorials and reading material There are several tutorials which have been run over the last few years with instructions and examples for running the combine tool. Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . 3rd tutorial 29th Nov 2017 4th tutorial 31st Oct 2018 - Latest for 81x-root606 branch. Worked examples from Higgs analyses using combine The CMS DAS at CERN 2014 The CMS DAS at DESY 2018 Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298 Procedure for the LHC Higgs boson search combination in summer 2011. This describes in more detail some of the methods used in Combine. Citations There is no document currently which can be cited for using the combine tool, however you can use the following publications for the procedures we use, Summer 2011 public ATLAS-CMS note for any Frequentist limit setting procedures with toys or Bayesian limits, constructing likelihoods, descriptions of nuisance parameter options (like log-normals ( lnN ) or gamma ( gmN ), and for definitions of test-statistics. CCGV paper if you use any of the asymptotic (eg with -M AsymptoticLimits or -M Significance approximations for limits/p-values. If you use the Barlow-Beeston approach to MC stat (bin-by-bin) uncertainties, please cite their paper Barlow-Beeston . You should also cite this note if you use the autoMCStats directive to produce a single parameter per bin. If you use shape uncertainties for template ( TH1 or RooDataHist ) based datacards, you can cite this note from J. Conway. If you are extracting uncertainties from LH scans - i.e using -2\\Delta Log{L}=1 -2\\Delta Log{L}=1 etc for the 1 \\sigma \\sigma intervals, you can cite either the ATLAS+CMS or CMS Higgs paper. There is also a long list of citation recommendations from the CMS Statistics Committee pages. Combine based packages SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester Contacts Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html CMS Statistics Committee You can find much more statistics theory and reccomendations on various statistical procedures in the CMS Statistics Committee Twiki Pages FAQ Why does combine have trouble with bins that have zero expected contents? If you're computing only upper limits, and your zero-prediction bins are all empty in data, then you can just set the background to a very small value instead of zero as anyway the computation is regular for background going to zero (e.g. a counting experiment with B\\leq1 B\\leq1 will have essentially the same expected limit and observed limit as one with B=0 B=0 ). If you're computing anything else, e.g. p-values, or if your zero-prediction bins are not empty in data, you're out of luck, and you should find a way to get a reasonable background prediction there (and set an uncertainty on it, as per the point above) How can an uncertainty be added to a zero quantity? You can put an uncertainty even on a zero event yield if you use a gamma distribution. That's in fact the more proper way of doing it if the prediction of zero comes from the limited size of your MC or data sample used to compute it. Why does changing the observation in data affect my expected limit? The expected limit (if using either the default behaviour of -M AsymptoticLimits or using the LHC-limits style limit setting with toys) uses the post-fit expectation of the background model to generate toys. This means that first the model is fit to the observed data before toy generation. See the sections on blind limits and toy generation to avoid this behavior. How can I deal with an interference term which involves a negative contribution? You will need to set up a specific PhysicsModel to deal with this, however you can see this section to implement such a model which can incorperate a negative contribution to the physics process How does combine work? That is not a question which can be answered without someone's head exploding so please try to formulate something specific. What does fit status XYZ mean? Combine reports the fit status in some routines (for example in the FitDiagnostics method). These are typically the status of the last call from Minuit. For details on the meanings of these status codes see the Minuit2Minimizer documentation page. Why does my fit not converge? There are several reasons why some fits may not converge. Often some indication can be obtained from the RooFitResult or status which you will see information from when using the --verbose X (with X>2 X>2 ) option. Sometimes however, it can be that the likelihood for your data is very unusual. You can get a rough idea about what the likelihood looks like as a function of your parameters (POIs and nuisances) using combineTool.py -M FastScan -w myworkspace.root (use --help for options). Why does the fit/fits take so long? The minimisation routines are common to many methods in combine. You can tune the fitting using the generic optimisation command line options described here . For example, setting the default minimizer strategy to 0 can greatly improve the speed since this avoids running Hesse. In calculations such as AsymptoticLimits , Hesse is not needed and hence this can be done, however, for FitDiagnostics the uncertainties and correlations are part of the output so using strategy 0 may not be particularly accurate. Why are the results for my counting experiment so slow or unstable? There is a known issue with counting experiments with large numbers of events which will cause unstable fits or even the fit to fail. You can avoid this by creating a \"fake\" shape datacard (see this section from the setting up the datacards page). The simplest way to do this is to run combineCards.py -S mycountingcard.txt > myshapecard.txt . You may still find that your parameter uncertainties are not correct when you have large numbers of events. This can be often fixed using the --robustHesse option. An example of this issue is detailed here . Why do some of my nuisance parameters have uncertainties > 1? When running -M FitDiagnostics you may find that the post-fit uncertainties of the nuisances are > 1 > 1 (or larger than their pre-fit values). If this is the case, you should first check if the same is true when adding the option --minos all which will invoke minos to scan the likelihood as a function of these parameters to determine the crossing at -2\\times\\Delta\\log\\mathcal{L}=1 -2\\times\\Delta\\log\\mathcal{L}=1 rather than relying on the estimate from Hesse. However, this is not guaranteed to succeed, in which case you can scan the likelihood yourself using MultiDimFit (see here ) and specifying the option --poi X where X is your nuisance parameter. How can I avoid using the data? For almost all methods, you can use toy data (or an Asimov dataset) in place of the real data for your results to be blind. You should be careful however as in some methods, such as -M AsymptoticLimits or -M HybridNew --LHCmode LHC-limits or any other method using the option --toysFrequentist , the data will be used to determine the most likely nuisance parameter values (to determine the so-called a-posteriori expectation). See the section on toy data generation for details on this. What if my nuisance parameters have correlations which are not 0 or 1? Combine is designed under the assumption that each source of nuisance parameter is uncorrelated with the other sources. If you have a case where some pair (or set) of nuisances have some known correlation structure, you can compute the eigenvectors of their correlation matrix and provide these diagonalised nuisances to combine. You can also model partial correlations , between different channels or data taking periods, of a given nuisance parameter using the combineTool as described in this page .","title":"Part4"},{"location":"part4/usefullinks/#useful-links-and-further-reading","text":"","title":"Useful links and further reading"},{"location":"part4/usefullinks/#tutorials-and-reading-material","text":"There are several tutorials which have been run over the last few years with instructions and examples for running the combine tool. Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . 3rd tutorial 29th Nov 2017 4th tutorial 31st Oct 2018 - Latest for 81x-root606 branch. Worked examples from Higgs analyses using combine The CMS DAS at CERN 2014 The CMS DAS at DESY 2018 Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298 Procedure for the LHC Higgs boson search combination in summer 2011. This describes in more detail some of the methods used in Combine.","title":"Tutorials and reading material"},{"location":"part4/usefullinks/#citations","text":"There is no document currently which can be cited for using the combine tool, however you can use the following publications for the procedures we use, Summer 2011 public ATLAS-CMS note for any Frequentist limit setting procedures with toys or Bayesian limits, constructing likelihoods, descriptions of nuisance parameter options (like log-normals ( lnN ) or gamma ( gmN ), and for definitions of test-statistics. CCGV paper if you use any of the asymptotic (eg with -M AsymptoticLimits or -M Significance approximations for limits/p-values. If you use the Barlow-Beeston approach to MC stat (bin-by-bin) uncertainties, please cite their paper Barlow-Beeston . You should also cite this note if you use the autoMCStats directive to produce a single parameter per bin. If you use shape uncertainties for template ( TH1 or RooDataHist ) based datacards, you can cite this note from J. Conway. If you are extracting uncertainties from LH scans - i.e using -2\\Delta Log{L}=1 -2\\Delta Log{L}=1 etc for the 1 \\sigma \\sigma intervals, you can cite either the ATLAS+CMS or CMS Higgs paper. There is also a long list of citation recommendations from the CMS Statistics Committee pages.","title":"Citations"},{"location":"part4/usefullinks/#combine-based-packages","text":"SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester","title":"Combine based packages"},{"location":"part4/usefullinks/#contacts","text":"Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html","title":"Contacts"},{"location":"part4/usefullinks/#cms-statistics-committee","text":"You can find much more statistics theory and reccomendations on various statistical procedures in the CMS Statistics Committee Twiki Pages","title":"CMS Statistics Committee"},{"location":"part4/usefullinks/#faq","text":"Why does combine have trouble with bins that have zero expected contents? If you're computing only upper limits, and your zero-prediction bins are all empty in data, then you can just set the background to a very small value instead of zero as anyway the computation is regular for background going to zero (e.g. a counting experiment with B\\leq1 B\\leq1 will have essentially the same expected limit and observed limit as one with B=0 B=0 ). If you're computing anything else, e.g. p-values, or if your zero-prediction bins are not empty in data, you're out of luck, and you should find a way to get a reasonable background prediction there (and set an uncertainty on it, as per the point above) How can an uncertainty be added to a zero quantity? You can put an uncertainty even on a zero event yield if you use a gamma distribution. That's in fact the more proper way of doing it if the prediction of zero comes from the limited size of your MC or data sample used to compute it. Why does changing the observation in data affect my expected limit? The expected limit (if using either the default behaviour of -M AsymptoticLimits or using the LHC-limits style limit setting with toys) uses the post-fit expectation of the background model to generate toys. This means that first the model is fit to the observed data before toy generation. See the sections on blind limits and toy generation to avoid this behavior. How can I deal with an interference term which involves a negative contribution? You will need to set up a specific PhysicsModel to deal with this, however you can see this section to implement such a model which can incorperate a negative contribution to the physics process How does combine work? That is not a question which can be answered without someone's head exploding so please try to formulate something specific. What does fit status XYZ mean? Combine reports the fit status in some routines (for example in the FitDiagnostics method). These are typically the status of the last call from Minuit. For details on the meanings of these status codes see the Minuit2Minimizer documentation page. Why does my fit not converge? There are several reasons why some fits may not converge. Often some indication can be obtained from the RooFitResult or status which you will see information from when using the --verbose X (with X>2 X>2 ) option. Sometimes however, it can be that the likelihood for your data is very unusual. You can get a rough idea about what the likelihood looks like as a function of your parameters (POIs and nuisances) using combineTool.py -M FastScan -w myworkspace.root (use --help for options). Why does the fit/fits take so long? The minimisation routines are common to many methods in combine. You can tune the fitting using the generic optimisation command line options described here . For example, setting the default minimizer strategy to 0 can greatly improve the speed since this avoids running Hesse. In calculations such as AsymptoticLimits , Hesse is not needed and hence this can be done, however, for FitDiagnostics the uncertainties and correlations are part of the output so using strategy 0 may not be particularly accurate. Why are the results for my counting experiment so slow or unstable? There is a known issue with counting experiments with large numbers of events which will cause unstable fits or even the fit to fail. You can avoid this by creating a \"fake\" shape datacard (see this section from the setting up the datacards page). The simplest way to do this is to run combineCards.py -S mycountingcard.txt > myshapecard.txt . You may still find that your parameter uncertainties are not correct when you have large numbers of events. This can be often fixed using the --robustHesse option. An example of this issue is detailed here . Why do some of my nuisance parameters have uncertainties > 1? When running -M FitDiagnostics you may find that the post-fit uncertainties of the nuisances are > 1 > 1 (or larger than their pre-fit values). If this is the case, you should first check if the same is true when adding the option --minos all which will invoke minos to scan the likelihood as a function of these parameters to determine the crossing at -2\\times\\Delta\\log\\mathcal{L}=1 -2\\times\\Delta\\log\\mathcal{L}=1 rather than relying on the estimate from Hesse. However, this is not guaranteed to succeed, in which case you can scan the likelihood yourself using MultiDimFit (see here ) and specifying the option --poi X where X is your nuisance parameter. How can I avoid using the data? For almost all methods, you can use toy data (or an Asimov dataset) in place of the real data for your results to be blind. You should be careful however as in some methods, such as -M AsymptoticLimits or -M HybridNew --LHCmode LHC-limits or any other method using the option --toysFrequentist , the data will be used to determine the most likely nuisance parameter values (to determine the so-called a-posteriori expectation). See the section on toy data generation for details on this. What if my nuisance parameters have correlations which are not 0 or 1? Combine is designed under the assumption that each source of nuisance parameter is uncorrelated with the other sources. If you have a case where some pair (or set) of nuisances have some known correlation structure, you can compute the eigenvectors of their correlation matrix and provide these diagonalised nuisances to combine. You can also model partial correlations , between different channels or data taking periods, of a given nuisance parameter using the combineTool as described in this page .","title":"FAQ"}]}